{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All functions for personality prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sophia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import bigrams, ngrams\n",
    "\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "from empath import Empath\n",
    "\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from lexicalrichness import LexicalRichness\n",
    "import textblob\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import datetime\n",
    "import random\n",
    "random.seed(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset with comments\n",
    "df = pd.read_csv('/home/sophia/ma_py/pandora_bigfive.csv')\n",
    "\n",
    "# Import dataset authors and delete not needed columns\n",
    "authors = pd.read_csv('/home/sophia/ma_py/author_profiles.csv')\n",
    "bigfive = authors[['author','agreeableness','openness','conscientiousness','extraversion','neuroticism']]\n",
    "bigfive = bigfive.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# change language to numeric representation\n",
    "def numeric_lang(df):\n",
    "    # change lang to numerical representation\n",
    "    language = df['lang'].values.tolist()\n",
    "    language = set(language)\n",
    "    language\n",
    "    df['language']= np.select([df.lang == 'en', df.lang == 'es', df.lang == 'nl'], \n",
    "                            [0, 1, 2], \n",
    "                            default=3)\n",
    "    # print(gramsdf['language'])\n",
    "    df = df.drop(columns=['lang'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# create time columns from UTC\n",
    "def create_timecolumns(df):\n",
    "    readable = []\n",
    "    weekday = []\n",
    "    month = []\n",
    "    year = []\n",
    "    for row in df['created_utc']:\n",
    "        item = datetime.datetime.fromtimestamp(row)\n",
    "        weekday_item = item.strftime('%A')\n",
    "        readable_item = datetime.datetime.fromtimestamp(row).isoformat()\n",
    "        month.append(str(readable_item[5:7]))\n",
    "        year.append(str(readable_item[0:4]))\n",
    "        readable.append(readable_item)\n",
    "        weekday.append(weekday_item.lower())\n",
    "    df['time'] = readable\n",
    "    df['weekday'] = weekday\n",
    "    df['month'] = month\n",
    "    df['year'] = year\n",
    "    return df\n",
    "\n",
    "# coun occurences in time columns to get time distribution\n",
    "def timecounter(lst, vocablst):\n",
    "    if vocablst == 'weekday':\n",
    "        vocab = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    elif vocablst == 'month':\n",
    "        vocab = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    elif vocablst == 'year':\n",
    "        vocab = ['2015', '2016', '2017', '2018', '2019']\n",
    "    else:\n",
    "        print(\"No valid input: vocab list\")\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", vocabulary=vocab)\n",
    "    vectors = vectorizer.fit_transform(lst)\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "# create a list of all subreddits in the dataset\n",
    "lst = df['subreddit'].tolist()\n",
    "lst = [item.lower() for item in lst]\n",
    "subredditset = set(lst)\n",
    "subredditlist = list(subredditset)\n",
    "\n",
    "# count occurences of subreddits \n",
    "def subredditcounter(lst, subredditlst):\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", vocabulary=subredditlist)\n",
    "    vectors = vectorizer.fit_transform(lst)\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "# aggregate dataset to get one row per author and create new columns for time and subreddit\n",
    "def create_authordf(df): \n",
    "    df = numeric_lang(df)\n",
    "    # body\n",
    "    df['complete_body'] = df.groupby(['author'])['body'].transform(lambda x : ' '. join(str(x)))\n",
    "    df['doc_body'] = df.groupby(['author'])['body'].transform(lambda x : 'ยง'. join(str(x)))\n",
    "    df['doc_body'] =  df['doc_body'].apply(lambda x: x.split(\"ยง\"))\n",
    "    \n",
    "    # language\n",
    "    df['lang'] = df['language'].apply(lambda x: str(x))\n",
    "    df['all_lang'] = df.groupby(['author'])['lang'].transform(lambda x : len(set(x)))\n",
    "    # created_utc\n",
    "    df['utc_lst'] = df['created_utc'].apply(lambda x: str(x))\n",
    "    df['all_utc'] = df.groupby(['author'])['utc_lst'].transform(lambda x : ' '. join(x))\n",
    "    df['all_utc'] = df['all_utc'].apply(lambda x: x.split())\n",
    "    # controversiality\n",
    "    df['mean_controversiality'] = df.groupby(['author']).agg({'controversiality': ['mean']})\n",
    "    df['mean_controversiality'] = df['mean_controversiality'].fillna(0)\n",
    "    # gilded\n",
    "    df['mean_gilded'] = df.groupby(['author']).agg({'gilded': ['mean']})\n",
    "    df['mean_gilded'] = df['mean_gilded'].fillna(0)\n",
    "    # number of subreddits\n",
    "    df['num_subreddits'] = df.groupby(['author'])['subreddit'].transform(lambda x : ' '. join(x))\n",
    "    df['num_subreddits'] = df['num_subreddits'].apply(lambda x: len(set(x.split())))\n",
    "    # number of comments per subreddit\n",
    "    df['subreddit'] = df['subreddit'].apply(lambda x: [x.lower()])\n",
    "    df['subreddit'] = df['subreddit'].apply(lambda x: ''.join(x))\n",
    "    df['subreddit_dist'] = df.groupby(['author'])['subreddit'].transform(lambda x : ' '. join(x))\n",
    "    subreddit_predist = subredditcounter(df['subreddit'], subredditlist)\n",
    "    subreddit_predist = subreddit_predist.tolist()\n",
    "    df['subreddit_dist'] = subreddit_predist\n",
    "    # time\n",
    "    df = create_timecolumns(df)\n",
    "    df['weekday_dist'] = df.groupby(['author'])['weekday'].transform(lambda x : ' '. join(x))\n",
    "    weekday = timecounter(df['weekday_dist'], 'weekday')\n",
    "    weekday = weekday.tolist()\n",
    "    df['weekday_dist'] = weekday\n",
    "    df['month_dist'] = df.groupby(['author'])['month'].transform(lambda x : ' '. join(x))\n",
    "    month = timecounter(df['month_dist'], 'month')\n",
    "    month = month.tolist()\n",
    "    df['month_dist'] = month\n",
    "    df['year_dist'] = df.groupby(['author'])['year'].transform(lambda x : ' '. join(x))\n",
    "    year = timecounter(df['year_dist'], 'year')\n",
    "    year = year.tolist()\n",
    "    df['year_dist'] = year\n",
    "    \n",
    "    newdf = df[['author', 'complete_body', 'doc_body', 'all_utc', 'mean_controversiality', \n",
    "                'mean_gilded', 'num_subreddits', 'subreddit_dist', 'weekday_dist', \n",
    "                'month_dist', 'year_dist', 'all_lang']]\n",
    "    newdf = newdf.sort_values(by='author')\n",
    "    newdf = newdf.drop_duplicates(subset=['author'])\n",
    "    return newdf\n",
    "\n",
    "# get one column for each feature in the distributions of time and subreddit\n",
    "weekday = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "month = ['january', 'february', 'march', 'april', 'may', 'june', 'juli', 'august', 'september', 'october', 'november', 'december']\n",
    "year = ['2015', '2016', '2017', '2018', '2019']\n",
    "\n",
    "def onecolumnperdatapoint(df, column, namelist):\n",
    "    for i in range(len(namelist)):\n",
    "#         df[namelist[i]] = df[column].apply(lambda x:[row[i] for row in x])\n",
    "        df[namelist[i]] = df[column].apply(lambda x:[x[i]])\n",
    "        df[namelist[i]] = [item[0] for item in df[namelist[i]]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for commentdf\n",
    "def create_commentdf(df):\n",
    "    pandora = create_authordf(df)\n",
    "    pandora = onecolumnperdatapoint(pandora, 'weekday_dist', weekday)\n",
    "    pandora = onecolumnperdatapoint(pandora, 'month_dist', month)\n",
    "    pandora = onecolumnperdatapoint(pandora, 'year_dist', year)\n",
    "    pandora = onecolumnperdatapoint(pandora, 'subreddit_dist', subredditlist)\n",
    "    pandora.drop(['weekday_dist', 'month_dist', 'year_dist', 'subreddit_dist'], axis=1, inplace=True)\n",
    "    return pandora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create commentdf\n",
    "pandora = create_commentdf(df)\n",
    "\n",
    "# merge commentdf and authordf\n",
    "pandoradf = pandora.merge(bigfive, how='left', on=['author'])\n",
    "pandoradf = pandoradf.sort_values(by='author')\n",
    "pandoradf = pandoradf[pandoradf['agreeableness'].notna()]\n",
    "pandoradf = pandoradf.reset_index()\n",
    "\n",
    "# create binary representation of personality traits\n",
    "def bigfive_cat(df):\n",
    "    # change big five to binary representation\n",
    "    df['agree'] = df['agreeableness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['openn'] = df['openness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['consc'] = df['conscientiousness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['extra'] = df['extraversion'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['neuro'] = df['neuroticism'].apply(lambda x: 0 if x<50 else 1)\n",
    "    return df\n",
    "\n",
    "pandoradf = bigfive_cat(pandoradf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# define stopwordlist to use\n",
    "def choose_stopwordlist(df, mode):\n",
    "    if mode == 'NLTK':\n",
    "        stopwordList = stopwords.words('english')\n",
    "    if mode == 'NLTK-neg':\n",
    "        stopwordList = stopwords.words('english')\n",
    "        stopwordList.remove('no')\n",
    "        stopwordList.remove('nor')\n",
    "        stopwordList.remove('not')\n",
    "    return stopwordList\n",
    "\n",
    "# remove decontractions\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# create sentence tokens\n",
    "def senttokenize(df):\n",
    "    sentbody = []\n",
    "    for row in df['doc_body']:\n",
    "        sentitem = []\n",
    "        for item in row:\n",
    "            sentences = sent_tokenize(item)\n",
    "            sentitem.append(sentences)\n",
    "        sentbody.append(sentitem)\n",
    "    df['senttokens'] = sentbody\n",
    "    return df\n",
    "\n",
    "# lower words and remove special characters\n",
    "def lower_special(df):\n",
    "    newrow = []\n",
    "    for row in tqdm(df['probody']):\n",
    "        newcomment = []\n",
    "        for comment in row:\n",
    "            text_pre = \"\"\n",
    "            for character in comment:\n",
    "                if character.isalnum() or character.isspace():\n",
    "                    character = character.lower()\n",
    "                    text_pre += character\n",
    "                else:\n",
    "                    text_pre += \" \"\n",
    "            newcomment.append(text_pre)\n",
    "        newrow.append(newcomment)   \n",
    "    df['probody'] = newrow\n",
    "    return df\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(df, stopwordList):\n",
    "    newprobody = []\n",
    "    for row in tqdm(df['probody']):\n",
    "        newrowprobody = []\n",
    "        for comment in row:\n",
    "            words = [word for word in comment.split() if (word not in stopwordList)]\n",
    "            newcomment = ' '.join(words)\n",
    "            newrowprobody.append(newcomment)\n",
    "        newprobody.append(newrowprobody)\n",
    "    df['probody'] = newprobody\n",
    "    return df\n",
    "\n",
    "# change numbers to words and tokenize words\n",
    "def num_tokenize(df):    \n",
    "    newbody_complete = []\n",
    "    newprobody_complete = []\n",
    "    # num2words\n",
    "    for row in tqdm(df['probody']):\n",
    "        newbody = []\n",
    "        newprobody = []\n",
    "        for sentence in row:\n",
    "            # string to list\n",
    "            inputtext = sentence.split()\n",
    "            numlist = []\n",
    "            for i in range(len(inputtext)):\n",
    "                if inputtext[i].isnumeric():\n",
    "                    numlist.append(i)\n",
    "            for number in numlist:\n",
    "                inputtext[number] = num2words(inputtext[number])\n",
    "\n",
    "            # list to string\n",
    "            inputtext = [word for word in inputtext if word.isalpha()]\n",
    "            celltext = ' '.join(inputtext)\n",
    "            newprobody.append(celltext)\n",
    "            # tokenize\n",
    "            words = word_tokenize(celltext)\n",
    "            newbody.append(words)\n",
    "        newbody_complete.append(newbody)\n",
    "        newprobody_complete.append(newprobody)\n",
    "    df['probody'] = newprobody_complete\n",
    "    df['tokens'] = newbody_complete\n",
    "    return df\n",
    "\n",
    "# Porter Stemmer\n",
    "def stemming(df):\n",
    "    ps = PorterStemmer()\n",
    "    for row in tqdm(df['tokens']):\n",
    "        for comment in row:\n",
    "            words = [ps.stem(word) for word in comment]\n",
    "            comment = ' '.join(words)\n",
    "    return df\n",
    "\n",
    "# bring columns of dataframe in correct order\n",
    "def ordering(df):\n",
    "    cols_tomove = ['index', 'author', 'complete_body', 'doc_body', 'probody', 'tokens', 'senttokens', 'agreeableness', 'openness', 'conscientiousness', 'extraversion', 'neuroticism', 'agree', 'openn', 'consc', 'extra', 'neuro']\n",
    "    orderdf  = df[cols_tomove + [col for col in df.columns if col not in cols_tomove]]\n",
    "    return orderdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper\n",
    "\n",
    "def preprocess(df):\n",
    "    # adjust some column representations\n",
    "    df = bigfive_cat(df)\n",
    "    # choose stopwordlist with or without negation\n",
    "    stopwordList = choose_stopwordlist(df, mode='NLTK-neg')\n",
    "    # decontract abbreviations (e.g., n't to not)\n",
    "    print(\"Decontract...\")\n",
    "    df['probody'] = df['doc_body'].apply(lambda x:([decontracted(x) for x in x]))\n",
    "    # create sentence tokens\n",
    "    print(\"Tokenize Sentences...\")\n",
    "    df = senttokenize(df)\n",
    "    # lower, remove stopwords, num2words, tokenize\n",
    "    print(\"Lower words and remove special characters...\")\n",
    "    df = lower_special(df)\n",
    "    print(\"Remove stopwords...\")\n",
    "    df = remove_stopwords(df, stopwordList)\n",
    "    print(\"Change numbers to words and tokenize words...\")\n",
    "    df = num_tokenize(df)\n",
    "    # porters stemmer\n",
    "    print(\"Porters Stemmer...\")\n",
    "    df = stemming(df)\n",
    "    print(\"Order df...\")\n",
    "    df = ordering(df)\n",
    "    print(\"Done!\")\n",
    "    return df\n",
    "\n",
    "# apply preprocessing\n",
    "predf = preprocess(pandoradf)\n",
    "\n",
    "predf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User features\n",
    "\n",
    "# Preprocessing for LDA\n",
    "def preprocess_lda(df):\n",
    "    neglst = [\"no\", \"not\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nowhere\", \"never\", \"nay\"]\n",
    "    inputlst = []\n",
    "    for row in tqdm(df['tokens']):\n",
    "        rowlst = []\n",
    "        for comment in row:\n",
    "            rowlst.append([word for word in comment if (word not in neglst)])\n",
    "        inputlst.append(rowlst)\n",
    "    return inputlst\n",
    "# LDA for topics\n",
    "def apply_lda(df, inputlst, number, name):\n",
    "    print(\"Start LDA...\")\n",
    "    lst = []\n",
    "    for row in tqdm(inputlst):\n",
    "        if len(row) < 2:\n",
    "            lst.append(-1)\n",
    "        else:\n",
    "            dictionary = corpora.Dictionary(row)\n",
    "            corpus = [dictionary.doc2bow(text) for text in row]\n",
    "            ldamodel = gensim.models.LdaMulticore(corpus, num_topics=number, id2word = dictionary, passes=20, workers=15)\n",
    "            result = ldamodel.print_topics(num_topics=1, num_words=1)\n",
    "            res = list(result)\n",
    "            topic = [item[0] for item in res]\n",
    "            lst.append(topic[0])\n",
    "    df[name] = lst\n",
    "    return df\n",
    "\n",
    "# Wrapper\n",
    "def extract_userfeatures(df):\n",
    "    print(\"Preprocessing for LDA...\")\n",
    "    inputlst = preprocess_lda(df)\n",
    "    print(\"LDA with fifty topics: \")\n",
    "    df = apply_lda(df, inputlst, 50, \"ldafifty\")\n",
    "    print(\"LDA with onehundred topics: \")\n",
    "    df = apply_lda(df, inputlst, 100, \"ldahundred\")\n",
    "    return df\n",
    "\n",
    "# create df with user features\n",
    "user_feat_df = extract_userfeatures(predf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linguistic features (functions)\n",
    "\n",
    "# other features that are not mentioned in the paper\n",
    "def create_features(df):\n",
    "    # Total number of characters (including space)\n",
    "    df['char_count'] = df['complete_body'].str.len()\n",
    "    # Total number of stopwords\n",
    "    stopwordList = stopwords.words('english')\n",
    "    df['stopwords'] = df['complete_body'].apply(lambda x: len([x for x in x.split() if x in stopwordList]))\n",
    "    # Total number of punctuation or special characters\n",
    "    df['total_punc'] = df['complete_body'].apply(lambda x: len([x for x in x.split() for j in x if j in string.punctuation]))\n",
    "    # Total number of numerics\n",
    "    df['total_num'] = df['complete_body'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    # Total number of uppercase words\n",
    "    df['total_uppercase'] = df['complete_body'].apply(lambda x: len([x for x in x.split() if x.isupper()]))    \n",
    "    return df\n",
    "\n",
    "# type token ratio\n",
    "def typetokenratio(df):\n",
    "    ratiolst = []\n",
    "    for comment in df['complete_body']:\n",
    "            lex = LexicalRichness(comment)\n",
    "            if lex.words == 0:\n",
    "                ratiolst.append(0)\n",
    "            else:\n",
    "                ratio = lex.ttr\n",
    "                ratiolst.append(ratio)\n",
    "    df['ttr'] = ratiolst\n",
    "    return df\n",
    "\n",
    "# words per sentence\n",
    "def wordcounter(df):\n",
    "    lengthscore = []\n",
    "    for row in df['senttokens']:\n",
    "        rowscore = []\n",
    "        for comment in row:\n",
    "            sentencescore = 0\n",
    "            for senttoken in comment:\n",
    "                length = len(senttoken.split())\n",
    "                sentencescore += length\n",
    "            sentencescore = sentencescore/len(comment)\n",
    "        lengthscore.append(sentencescore)\n",
    "        arr = np.array(lengthscore)\n",
    "    df['words_per_sent'] = lengthscore\n",
    "    return df\n",
    "\n",
    "# words longer than six characters\n",
    "def charcounter(df):\n",
    "    charscore = []\n",
    "    for row in df['tokens']:\n",
    "        for comment in row:\n",
    "            rowcharscore = 0\n",
    "            lencomment = len(comment)\n",
    "            if lencomment == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                number = 0\n",
    "                for token in comment:\n",
    "                    length = len(token)\n",
    "                    if length > 5:\n",
    "                        number+=1\n",
    "                score = number/lencomment\n",
    "            rowcharscore += score\n",
    "        rowcharscore = rowcharscore/len(row)\n",
    "        charscore.append(rowcharscore)\n",
    "    df['wordslongersix'] = charscore\n",
    "    return df\n",
    "\n",
    "# POS tagger\n",
    "def tagging(df):\n",
    "    past = [] #VPA\n",
    "    presence = [] #VPR\n",
    "    adverbs = [] #RB\n",
    "    prepositions = [] #PREP\n",
    "    pronouns = [] #PR\n",
    "    for comment in df['complete_body']:\n",
    "            text = comment.split()\n",
    "            tags = nltk.pos_tag(text)\n",
    "            counts = Counter(tag for word,tag in tags)\n",
    "            total = sum(counts.values())\n",
    "            pron = counts['PRP'] + counts['PRP$']\n",
    "            verbspr = counts['VB'] + counts['VBG'] + counts['VBP'] + counts['VBZ'] + counts['MD']\n",
    "            verbspa = counts['VBD'] + counts['VBN']\n",
    "            preps = counts['IN'] + counts['TO']\n",
    "            counts['PR'] = pron\n",
    "            counts['PREP'] = preps\n",
    "            counts['VPR'] = verbspr #present tense\n",
    "            counts['VPA'] = verbspa #past tense\n",
    "            if total == 0:\n",
    "                allcounts = dict((word, float(count)/1) for word,count in counts.items())\n",
    "            else:\n",
    "                allcounts = dict((word, float(count)/total) for word,count in counts.items())\n",
    "            try:\n",
    "                past.append(allcounts['VPA'])\n",
    "            except KeyError:\n",
    "                past.append(0)\n",
    "            try:\n",
    "                presence.append(allcounts['VPR'])\n",
    "            except KeyError:\n",
    "                presence.append(0)\n",
    "            try:\n",
    "                adverbs.append(allcounts['RB'])\n",
    "            except KeyError:\n",
    "                adverbs.append(0)\n",
    "            try:\n",
    "                prepositions.append(allcounts['PREP'])\n",
    "            except KeyError:\n",
    "                prepositions.append(0)\n",
    "            try:\n",
    "                pronouns.append(allcounts['PR'])\n",
    "            except KeyError:\n",
    "                pronouns.append(0)\n",
    "    df['pasttense'] = past\n",
    "    df['presencetense'] = presence\n",
    "    df['adverbs'] = adverbs\n",
    "    df['prepositions'] = prepositions\n",
    "    df['pronouns'] = pronouns\n",
    "    return df\n",
    "\n",
    "def ngrams(df, n_min, n_max, ngramtype):\n",
    "    # convert input from list to string\n",
    "    ngrams = []\n",
    "    inputtext = []\n",
    "    for row in df['tokens']:\n",
    "        for comment in row:\n",
    "            text = ' '.join(comment)\n",
    "        inputtext.append(text)\n",
    "    print(\"Length of inputtext: \", len(inputtext))\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n_min,n_max), analyzer=ngramtype)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    print(\"Get feature names...\")\n",
    "    names = vectorizer.get_feature_names()\n",
    "    print(\"Length of feature names: \", len(names))\n",
    "    print(\"Create df...\")\n",
    "    ngramdf = pd.DataFrame(denselist, columns=names)\n",
    "    ngramdf['author'] = df['author']\n",
    "    return ngramdf\n",
    "\n",
    "def merge_dfs(df1, df2, df3):\n",
    "    cwngramsdf = pd.merge(df1, df2, on='author', how='inner', suffixes= (None, \"_charngram\"))\n",
    "    gramsdf = pd.merge(df3, cwngramsdf, on='author', how='inner', suffixes= (None, \"_ngram\"))\n",
    "    return gramsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for linguistic features\n",
    "\n",
    "def extract_lin_features(df, create_ngrams):\n",
    "    print(\"Create additional features...\")\n",
    "    df = create_features(df)\n",
    "    print(\"Create ttr...\")\n",
    "    df = typetokenratio(df)\n",
    "    print(\"Count words per sentence...\")\n",
    "    df = wordcounter(df)\n",
    "    print(\"Count words with more than six letters...\")\n",
    "    df = charcounter(df)\n",
    "    print(\"POS-Tagger...\")\n",
    "    df = tagging(df)\n",
    "    print(\"number of rows df\", len(df))\n",
    "    \n",
    "    if create_ngrams == \"none\":\n",
    "        return df\n",
    "    \n",
    "    elif create_ngrams == \"all\":\n",
    "        print(\"Ngrams...\")\n",
    "        print(\"Create word ngrams...\")\n",
    "        wordngramsdf = ngrams(df, 1, 3, \"word\")\n",
    "        print(\"Create char ngrams...\")\n",
    "        charngramsdf = ngrams(df, 2, 3, \"char\")\n",
    "        print(\"Merge df...\")\n",
    "        gramsdf = merge_dfs(wordngramsdf, charngramsdf, df)\n",
    "        return gramsdf\n",
    "    \n",
    "    elif create_ngrams == \"word\":\n",
    "        wordngrams = ngrams(df, 1, 3, 'word')\n",
    "        wordngramsdf = pd.DataFrame(wordngrams)\n",
    "        gramsdf = pd.merge(df, wordngramsdf, on='author', how='inner', suffixes=(None, \"_ngram\"))\n",
    "        return gramsdf\n",
    "    \n",
    "# create dataframe with linguistic features\n",
    "\n",
    "# without ngrams\n",
    "# lin_feat_df = extract_lin_features(user_feat_df, \"none\")\n",
    "\n",
    "# with all ngrams\n",
    "lin_ngrams_df = extract_lin_features(user_feat_df, \"all\")\n",
    "\n",
    "# wordngrams only\n",
    "# lin_wordngrams_df = extract_lin_features(user_feat_df, \"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordlists (functions)\n",
    "\n",
    "# Empath\n",
    "# create new categories with empath\n",
    "def new_cat():\n",
    "    empath = Empath()\n",
    "    social = empath.create_category(\"social\",[\"mate\",\"talk\",\"they\"])\n",
    "    humans = empath.create_category(\"humans\",[\"adult\",\"baby\",\"boy\"])\n",
    "    cognitive = empath.create_category(\"cognitive\",[\"cause\",\"know\",\"ought\"])\n",
    "    insight = empath.create_category(\"insight\",[\"think\",\"know\",\"consider\"])\n",
    "    causation = empath.create_category(\"causation\",[\"because\",\"effect\",\"hence\"])\n",
    "    discrepancy = empath.create_category(\"discrepancy\",[\"should\",\"would\",\"could\"])\n",
    "    tentative = empath.create_category(\"tentative\",[\"maybe\",\"perhaps\",\"guess\"])\n",
    "    certainty = empath.create_category(\"certainty\",[\"always\",\"never\", \"proof\"])\n",
    "    inhibition = empath.create_category(\"inhibition\",[\"block\",\"constrain\",\"stop\"])\n",
    "    inclusive = empath.create_category(\"inclusive\",[\"and\",\"with\",\"include\"])\n",
    "    exclusive = empath.create_category(\"exclusive\",[\"but\",\"without\",\"exclude\"])\n",
    "    perceptual = empath.create_category(\"perceptual\",[\"observing\",\"hear\",\"feeling\"])\n",
    "    see = empath.create_category(\"see\",[\"view\",\"saw\",\"seen\"])\n",
    "    feel = empath.create_category(\"feel\",[\"feels\",\"touch\",\"feeling\"])\n",
    "    biological = empath.create_category(\"biological\",[\"eat\",\"blood\",\"pain\"])\n",
    "    relativity = empath.create_category(\"relativity\",[\"area\",\"bend\",\"go\"])\n",
    "    space = empath.create_category(\"space\",[\"down\",\"in\",\"thin\"])\n",
    "    time = empath.create_category(\"time\",[\"end\",\"until\",\"season\"])\n",
    "    agreement = empath.create_category(\"agreement\", [\"agree\", \"ok\", \"yes\"])\n",
    "    fillers = empath.create_category(\"fillers\", [\"like\", \"Imean\", \"yaknow\"])\n",
    "    nonfluencies = empath.create_category(\"nonfluencies\", [\"umm\", \"hm\", \"er\"])\n",
    "    conjunctions = empath.create_category(\"conjunctions\", [\"and\", \"but\", \"whereas\"])\n",
    "    quantifiers = empath.create_category(\"quantifiers\", [\"few\", \"many\", \"much\"])\n",
    "    numbers = empath.create_category(\"numbers\", [\"two\", \"fourteen\", \"thousand\"])\n",
    "\n",
    "def apply_empath(df):\n",
    "    empath = Empath()\n",
    "    print(\"Create new empath categories...\")\n",
    "    new_cat()\n",
    "    print(\"Apply empath...\")\n",
    "    empathvalues = []\n",
    "    empathcategories = [\"swearing_terms\", \"social\", \"family\", \"friends\", \"humans\", \"emotional\", \"positive_emotion\", \"negative_emotion\", \"fear\", \"anger\", \"sadness\", \"cognitive\", \"insight\", \"causation\", \"discrepancy\", \"tentative\", \"certainty\", \"inhibition\", \"inclusive\", \"exclusive\", \"perceptual\", \"see\", \"hear\", \"feel\", \"biological\", \"body\", \"health\", \"sexual\", \"eat\", \"relativity\", \"space\", \"time\", \"work\", \"achievement\", \"leisure\", \"home\", \"money\", \"religion\", \"death\" ,\"agreement\", \"fillers\", \"nonfluencies\"]\n",
    "    for sentence in tqdm(df['complete_body']):\n",
    "        empathvalues.append(empath.analyze(sentence, categories=empathcategories, normalize=True))\n",
    "    empathdf = pd.DataFrame(empathvalues)\n",
    "    empathdf['author'] = df['author']\n",
    "\n",
    "    newdf = pd.merge(df, empathdf, on='author', how='inner', suffixes=(None, \"_wordlist\"))\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data for other wordlists\n",
    "concretenessdf = pd.read_csv('/home/sophia/ma_py/psych_lists/concreteness.csv')\n",
    "cdf = concretenessdf[['Conc.M']]\n",
    "cmatrix = cdf.to_numpy()\n",
    "concrete = concretenessdf['Word'].values.tolist()\n",
    "\n",
    "happinessdf = pd.read_csv('/home/sophia/ma_py/psych_lists/happiness_ratings.csv')\n",
    "hdf = happinessdf[['happiness_average']]\n",
    "hmatrix = hdf.to_numpy()\n",
    "happiness = happinessdf['word'].values.tolist()\n",
    "\n",
    "cursedf = pd.read_csv('/home/sophia/ma_py/psych_lists/mean_good_curse.csv')\n",
    "cudf = cursedf[['mean_good_curse']]\n",
    "cumatrix = cudf.to_numpy()\n",
    "curse = cursedf['word'].values.tolist()\n",
    "\n",
    "sensorydf = pd.read_csv('/home/sophia/ma_py/psych_lists/sensory_experience_ratings.csv')\n",
    "serdf = sensorydf[['Average SER']]\n",
    "sermatrix = serdf.to_numpy()\n",
    "ser = sensorydf['Word'].values.tolist()\n",
    "\n",
    "alldf = pd.read_csv('/home/sophia/ma_py/psych_lists/sensory_ratings_all.csv')\n",
    "newalldf = alldf[['Emotion', 'Polarity', 'Social', 'Moral', 'MotionSelf', 'Thought', 'Color', 'TasteSmell', 'Tactile', 'VisualForm', 'Auditory', 'Space', 'Quantity', 'Time', 'CNC', 'IMG', 'FAM']]\n",
    "allmatrix = newalldf.to_numpy()\n",
    "allsens = alldf['Word'].values.tolist()\n",
    "\n",
    "valarodomdf = pd.read_csv('/home/sophia/ma_py/psych_lists/valence_arousal_dominence.csv')\n",
    "vaddf = valarodomdf[['V.Mean.Sum', 'A.Mean.Sum', 'D.Mean.Sum']]\n",
    "vadmatrix = vaddf.to_numpy()\n",
    "vad = valarodomdf['Word'].values.tolist()\n",
    "\n",
    "mrcdf = pd.read_csv('/home/sophia/ma_py/psych_lists/mrclists_c_p.csv', sep='\\t', names=['word', 'cmean', 'pmean'])\n",
    "cpdf = mrcdf[['cmean', 'pmean']]\n",
    "cpmatrix = cpdf.to_numpy()\n",
    "mrc = mrcdf['word'].values.tolist()\n",
    "\n",
    "# function for other wordlists\n",
    "\n",
    "def counter(df, vocab):\n",
    "    inputtext = []\n",
    "    for row in df['complete_body']:\n",
    "        text = ' '.join(row)\n",
    "        inputtext.append(text)\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,1), vocabulary = vocab)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "def multiply(matrix, ratings):\n",
    "    # matrix multiplication \n",
    "    result = np.matmul(matrix, ratings)\n",
    "    # divide each score with the number of words in the list to normalize\n",
    "    result = result/(len(ratings))\n",
    "    return result\n",
    "\n",
    "def aggregator(df, vocab, ratings, name):\n",
    "    count = counter(df, vocab)\n",
    "    result = multiply(count, ratings)\n",
    "    num_rows, num_cols = result.shape\n",
    "    \n",
    "    if num_cols ==1:\n",
    "        df[name] = result\n",
    "    else:\n",
    "        resultdf = pd.DataFrame(result)\n",
    "        for i in range(len(name)):\n",
    "            # first i is zero\n",
    "            column = name[i]\n",
    "            df[column] = resultdf[i]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordlists created manually\n",
    "\n",
    "negations = [\"no\", \"not\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nowhere\", \"never\", \"nay\"]\n",
    "articles = [\"a\", \"an\", \"the\"]\n",
    "future = [\"will\", \"gonna\"]\n",
    "\n",
    "def list_counter(df, vocab, name):\n",
    "    inputtext = []\n",
    "    total = []\n",
    "    for row in df['complete_body']:\n",
    "        total.append(len(row))\n",
    "        text = ' '.join(row)\n",
    "        inputtext.append(text)\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,1), vocabulary = vocab)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    v = vectors.toarray()\n",
    "    averagev = v.sum(axis=1)\n",
    "    totalvector =  np.array(total)\n",
    "    score = np.divide(averagev, totalvector)\n",
    "    df[name] = score\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for wordlists\n",
    "\n",
    "def extract_wordlist_features(df):\n",
    "    print(\"Empath...\")\n",
    "    empdf = apply_empath(df)\n",
    "    # create scores for each word list and add them to df\n",
    "    print(\"Count Wordlist Concreteness: \\n\")\n",
    "    psychdf = aggregator(empdf, concrete, cmatrix, \"concreteness\")\n",
    "    print(\"Count Wordlist Happiness: \\n\")\n",
    "    psychdf = aggregator(empdf, happiness, hmatrix, \"happiness\")\n",
    "    print(\"Count Wordlist Good_Curse: \\n\")\n",
    "    psychdf = aggregator(empdf, curse, cumatrix, \"good_curse\")\n",
    "    print(\"Count 17 further wordlists: \\n\")\n",
    "    psychdf = aggregator(empdf, allsens, allmatrix, ['emotion', 'polarity', 'social', 'moral', 'motionself', 'thought', 'color', 'tastesmell', 'tactile', 'visualform', 'auditory', 'space', 'quantity', 'time', 'CNC', 'IMG', 'FAM'])\n",
    "    print(\"Count Wordlist SER: \\n\")\n",
    "    psychdf = aggregator(empdf, ser, sermatrix, \"SER\")\n",
    "    print(\"Count Wordlists Valence, Arousal, Dominance: \\n\")\n",
    "    psychdf = aggregator(empdf, vad, vadmatrix, ['valence', 'arousal', 'dominance'])\n",
    "    print(\"Count Wordlist Negation: \\n\")\n",
    "    psychdf = list_counter(empdf, negations, \"negations\")\n",
    "    print(\"Count Wordlist Articles: \\n\")\n",
    "    psychdf = list_counter(empdf, articles, \"articles\")\n",
    "    print(\"Count Wordlist Future: \\n\")\n",
    "    psychdf = list_counter(empdf, future, \"future\")\n",
    "    print(\"Count Wordlists from MRC (2): \\n\")\n",
    "    psychdf = aggregator(empdf, mrc, cpmatrix, [\"mrc_cmean\", \"mrc_pmean\"])\n",
    "    \n",
    "    return psychdf\n",
    "\n",
    "psychdf = extract_wordlist_features(lin_ngrams_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "# histogram of distribution of traits in dataset\n",
    "def all_hist_true(df):\n",
    "    plt.figure(figsize = (16, 8))\n",
    "#     plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(df['agreeableness'], bins = 20)\n",
    "    plt.title('Agreeableness')\n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.hist(df['openness'], bins = 20)\n",
    "    plt.title('Openness')\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.hist(df['conscientiousness'], bins = 20)\n",
    "    plt.title('Conscientiousness')\n",
    "    \n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.hist(df['extraversion'], bins = 20)\n",
    "    plt.title('Extraversion')\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.hist(df['neuroticism'], bins = 20)\n",
    "    plt.title('Neuroticism')\n",
    "    \n",
    "    plt.suptitle(\"Histograms of the true trait values\")\n",
    "    plt.subplots_adjust(left=0.1, \n",
    "                    bottom=0.1,  \n",
    "                    right=0.9,  \n",
    "                    top=0.9,  \n",
    "                    wspace=0.4,  \n",
    "                    hspace=0.4) \n",
    "    plt.show()\n",
    "\n",
    "#split dataset in features and target variable depending on which trait to focus on\n",
    "def trait(df, trait_name, startnumber):\n",
    "    featurelist = df.columns.tolist()\n",
    "    feature_cols = featurelist[startnumber:]\n",
    "    x = df[feature_cols] \n",
    "    \n",
    "    if trait_name == 'agree':\n",
    "        y = df.agree\n",
    "    elif trait_name == 'openn':\n",
    "        y = df.openn\n",
    "    elif trait_name == 'consc':\n",
    "        y = df.consc\n",
    "    elif trait_name == 'extra':\n",
    "        y = df.extra\n",
    "    elif trait_name == 'neuro':\n",
    "        y = df.neuro       \n",
    "    return x,y \n",
    "\n",
    "# create pipeline\n",
    "def create_pipeline(x_train, y_train ,classifier):\n",
    "    if classifier == \"log\":\n",
    "        pipeline = Pipeline([\n",
    "          ('variance_threshold', VarianceThreshold()),\n",
    "          ('feature_selection',  SelectKBest(f_classif, k=30)),\n",
    "          ('scaler', StandardScaler()),\n",
    "          ('classification',LogisticRegression(n_jobs=-1))\n",
    "        ])\n",
    "        \n",
    "    pipeline.fit(x_train, y_train)\n",
    "    return pipeline\n",
    "\n",
    "def get_names(x, pipeline):\n",
    "    features = pipeline.named_steps['feature_selection']\n",
    "    names = x.columns[features.get_support(indices=True)]\n",
    "    return names\n",
    "\n",
    "def get_pvalues(pipeline, x):\n",
    "    features = pipeline.named_steps['feature_selection']\n",
    "    pvalues = features.pvalues_\n",
    "    dfpvalues = pd.DataFrame(features.pvalues_)\n",
    "    dfscores = pd.DataFrame(features.scores_)\n",
    "    dfcolumns = pd.DataFrame(x.columns)\n",
    "    # concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores, dfpvalues],axis=1)\n",
    "    featureScores.columns = ['Specs','Score', 'P-Value']\n",
    "    # plot\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.hist(pvalues)\n",
    "    plt.show()\n",
    "    return featureScores\n",
    "\n",
    "def scores(y_test, y_pred, presentationtype):\n",
    "    if presentationtype == \"scores\":\n",
    "        accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "        precision=metrics.precision_score(y_test, y_pred)\n",
    "        recall=metrics.recall_score(y_test, y_pred)\n",
    "        f_one=metrics.f1_score(y_test, y_pred)\n",
    "        return accuracy, precision, recall, f_one\n",
    "    if presentationtype == \"report\":\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        return report\n",
    "    \n",
    "def score_plot(logreg, y_test, x_test):\n",
    "    lr_probs = logreg.predict_proba(x_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # predict class values\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "    # plot the precision-recall curves\n",
    "    no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return lr_precision, lr_recall\n",
    "\n",
    "def create_cnfmatrix(y_test, y_pred, plotting=True):\n",
    "    cnfpipe_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n",
    "    sumpositive = tp + fn\n",
    "    sumnegative = fp + tn\n",
    "    sumcorrect = tp + tn\n",
    "    sumwrong = fp + fn\n",
    "    sumall = tn+fp+fn+tp\n",
    "    print(\"TN, FP, FN, TP: \", tn, fp, fn, tp, \"\\nSum: \", sumall, \"\\nSum correct predictions: \", \n",
    "          sumcorrect, \"Percent: \", sumcorrect/sumall, \"\\nSum wrong predictions: \", sumwrong, \"\\tPercent: \",\n",
    "          sumwrong/sumall, \"\\nSum actual positives: \", sumpositive, \"\\tPercent: \", sumpositive/sumall,\n",
    "          \"\\nSum actual negatives: \", sumnegative, \"\\tPercent: \", sumnegative/sumall)\n",
    "    \n",
    "    if plotting:\n",
    "        %matplotlib inline\n",
    "        class_names=[0,1] # name  of classes\n",
    "        fig, ax = plt.subplots()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "        # create heatmap\n",
    "        sns.heatmap(pd.DataFrame(cnfpipe_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "        ax.xaxis.set_label_position(\"bottom\")\n",
    "        plt.tight_layout()\n",
    "        plt.title('Confusion matrix', y=1.1)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for classifier\n",
    "\n",
    "def classify(df, trait_name, startnumber, plotting=True):\n",
    "    print(\"Trait to predict: \", trait_name)\n",
    "    x,y = trait(df, trait_name, startnumber)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "    print(\"Number of authors in y_train: \", len(y_train))\n",
    "    print(\"Number of authors in y_test: \", len(y_test))\n",
    "    logpipe = create_pipeline(x_train, y_train, 'log')\n",
    "    y_pred=logpipe.predict(x_test)\n",
    "    print(\"Number of authors in y_pred: \", len(y_pred))\n",
    "    names = get_names(x, logpipe)\n",
    "    print(\"Names of the top\", len(names), \"features: \\n\", names, \"\\n\")\n",
    "    pvalues = get_pvalues(logpipe, x)\n",
    "    print(\"\\nP-Values: \")\n",
    "    print(pvalues.nsmallest(30,'P-Value'))\n",
    "    print(\"\\n\")\n",
    "    cnfmatrix = create_cnfmatrix(y_test, y_pred, plotting=True) \n",
    "#     accuracy, precision, recall, f_one = scores(y_test, y_pred, \"scores\")\n",
    "#     print(\"Scores:\\nAccuracy:\",accuracy, \"\\nPrecision:\",precision, \"\\nRecall:\",recall, \"\\nF1 score:\",f_one)\n",
    "    report = scores(y_test, y_pred, \"report\")\n",
    "    print(\"Classification report: \\n\", report)\n",
    "    lr_precision, lr_recall = score_plot(logpipe, y_test, x_test)\n",
    "    print(\"\\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psychdf.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 18\n",
    "print (\"Number of authors: \", len(psychdf))\n",
    "\n",
    "# personality prediction on test set\n",
    "all_hist_true(psychdf)\n",
    "classify(psychdf, \"agree\", start, plotting=True)\n",
    "classify(psychdf, \"openn\", start, plotting=True)\n",
    "classify(psychdf, \"consc\", start, plotting=True)\n",
    "classify(psychdf, \"extra\", start, plotting=True)\n",
    "classify(psychdf, \"neuro\", start, plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for the train set\n",
    "\n",
    "def classify_trainset(df, trait_name, startnumber, plotting=True):\n",
    "    print(\"Trait to predict: \", trait_name)\n",
    "    x,y = trait(df, trait_name, startnumber)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "    logpipe = create_pipeline(x_train, y_train, 'log')\n",
    "    y_pred=logpipe.predict(x_train)\n",
    "    print(\"Number of authors in y_pred: \", len(y_pred))\n",
    "    names = get_names(x, logpipe)\n",
    "    print(\"Names of the top\", len(names), \"features: \\n\", names, \"\\n\")\n",
    "    pvalues = get_pvalues(logpipe, x)\n",
    "#     print(\"p-values of\", len(pvalues), \"features: \\n\", pvalues, \"\\n\")\n",
    "    print(\"\\nP-Values: \")\n",
    "    print(pvalues.nsmallest(30,'P-Value'))\n",
    "    print(\"\\n\")\n",
    "    cnfmatrix = create_cnfmatrix(y_train, y_pred, plotting=True) \n",
    "#     accuracy, precision, recall, f_one = scores(y_test, y_pred, \"scores\")\n",
    "#     print(\"Scores:\\nAccuracy:\",accuracy, \"\\nPrecision:\",precision, \"\\nRecall:\",recall, \"\\nF1 score:\",f_one)\n",
    "    report = scores(y_train, y_pred, \"report\")\n",
    "    print(\"Classification report: \\n\", report)\n",
    "    lr_precision, lr_recall = score_plot(logpipe, y_train, x_train)\n",
    "#     print(\"Scores:\\nLR_Precision:\",lr_precision, \"\\nLR_Recall:\",lr_recall)\n",
    "    plt.show()\n",
    "    print(\"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_trainset(psychdf, \"agree\", start, plotting=True)\n",
    "classify_trainset(psychdf, \"openn\", start, plotting=True)\n",
    "classify_trainset(psychdf, \"consc\", start, plotting=True)\n",
    "classify_trainset(psychdf, \"extra\", start, plotting=True)\n",
    "classify_trainset(psychdf, \"neuro\", start, plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
