{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All functions for personality prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sophia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import bigrams, ngrams\n",
    "\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "from empath import Empath\n",
    "\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from lexicalrichness import LexicalRichness\n",
    "import textblob\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import datetime\n",
    "import random\n",
    "random.seed(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset with comments\n",
    "df = pd.read_csv('/home/sophia/ma_py/pandora_bigfive.csv')\n",
    "\n",
    "# Import dataset authors and delete not needed columns\n",
    "authors = pd.read_csv('/home/sophia/ma_py/author_profiles.csv')\n",
    "bigfive = authors[['author','agreeableness','openness','conscientiousness','extraversion','neuroticism']]\n",
    "bigfive = bigfive[bigfive['agreeableness'].notna()]\n",
    "del authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# change language to numeric representation\n",
    "def numeric_lang(df):\n",
    "    # change lang to numerical representation\n",
    "    language = df['lang'].values.tolist()\n",
    "    language = set(language)\n",
    "    language\n",
    "    df['language']= np.select([df.lang == 'en', df.lang == 'es', df.lang == 'nl'], \n",
    "                            [0, 1, 2], \n",
    "                            default=3)\n",
    "    # print(gramsdf['language'])\n",
    "    df = df.drop(columns=['lang'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# create time columns from UTC\n",
    "def create_timecolumns(df):\n",
    "    readable = []\n",
    "    weekday = []\n",
    "    month = []\n",
    "    year = []\n",
    "    for row in tqdm(df['created_utc']):\n",
    "        item = datetime.datetime.fromtimestamp(row)\n",
    "        weekday_item = item.strftime('%A')\n",
    "        readable_item = datetime.datetime.fromtimestamp(row).isoformat()\n",
    "        month.append(str(readable_item[5:7]))\n",
    "        year.append(str(readable_item[0:4]))\n",
    "        readable.append(readable_item)\n",
    "        weekday.append(weekday_item.lower())\n",
    "    df['time'] = readable\n",
    "    df['weekday'] = weekday\n",
    "    df['month'] = month\n",
    "    df['year'] = year\n",
    "    return df\n",
    "\n",
    "# count occurences in time columns to get time distribution\n",
    "def timecounter(lst, vocablst):\n",
    "    if vocablst == 'weekday':\n",
    "        vocab = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    elif vocablst == 'month':\n",
    "        vocab = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    elif vocablst == 'year':\n",
    "        vocab = ['2015', '2016', '2017', '2018', '2019']\n",
    "    else:\n",
    "        print(\"No valid input: vocab list\")\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", vocabulary=vocab)\n",
    "    vectors = vectorizer.fit_transform(lst)\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "# create a list of all subreddits in the dataset\n",
    "lst = df['subreddit'].tolist()\n",
    "lst = [item.lower() for item in lst]\n",
    "subredditset = set(lst)\n",
    "subredditlist = list(subredditset)\n",
    "\n",
    "# count occurences of subreddits \n",
    "def subredditcounter(lst, subredditlst):\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", vocabulary=subredditlist)\n",
    "    vectors = vectorizer.fit_transform(lst)\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "# aggregate dataset to get one row per author and create new columns for time and subreddit\n",
    "def create_groupdf(df): \n",
    "    print(\"\\tCreate numeric language representation...\")\n",
    "    df = numeric_lang(df)\n",
    "    print(\"\\tCreate time columns...\")\n",
    "    df = create_timecolumns(df)\n",
    "    # create dictionary for aggregation function\n",
    "    d = {'lang': ['nunique'] , 'controversiality': ['mean'], 'gilded': ['mean'], \n",
    "         'body': (' '. join), 'doc_body': (lambda x : list(x)),\n",
    "         'utc': (lambda x : list(x)), 'subreddit': (' '. join), 'num_subreddit': ['nunique'],\n",
    "         'weekday': (' '. join), 'month': (' '. join), 'year': (' '. join)}\n",
    "    # '§'. join(x)\n",
    " \n",
    "    # new ungrouped columns\n",
    "    print(\"\\tCreate new ungrouped columns...\")\n",
    "    df['body'] = df['body'].apply(lambda x: str(x))\n",
    "    df['doc_body'] = df['body']\n",
    "    df['num_subreddit'] = df['subreddit']\n",
    "    df['lang'] = df['language'].apply(lambda x: str(x))\n",
    "    df['utc'] = df['created_utc'].apply(lambda x: str(x))\n",
    "#     df['subreddit'] = df['subreddit'].apply(lambda x: [x.lower()])\n",
    "    df['subreddit'] = df['subreddit'].apply(lambda x: ''.join(x.lower()))\n",
    "    \n",
    "    # create df groupd by author + transform\n",
    "    print(\"\\tGroup df by author...\")\n",
    "    groupdf = df.groupby(['author']).agg(d)\n",
    "    groupdf = groupdf.reset_index()\n",
    "    groupdf.columns = groupdf.columns.droplevel(1)\n",
    "    return groupdf\n",
    "    \n",
    "def create_new_columns(df):    \n",
    "    # body\n",
    "#     print(\"\\tCreate doc_body...\")\n",
    "# #     df['doc_body'] =  df['doc_body'].apply(lambda x: [x.split(\"§\") for x in x])\n",
    "#     # created_utc\n",
    "#     print(\"\\tCreate utc list...\")\n",
    "#     df['all_utc'] = df['utc_lst'].apply(lambda x: x.split())\n",
    "    # controversiality\n",
    "    print(\"\\tCreate controversiality column...\")\n",
    "    df['controversiality'] = df['controversiality'].fillna(0)\n",
    "    # gilded\n",
    "    print(\"\\tCreate mean_gilded...\")\n",
    "    df['gilded'] = df['gilded'].fillna(0)\n",
    "    # number of comments per subreddit\n",
    "    print(\"\\tCreate subreddit_dist...\")\n",
    "    subreddit_predist = subredditcounter(df['subreddit'], subredditlist)\n",
    "    subreddit_predist = subreddit_predist.tolist()\n",
    "    df['subreddit_dist'] = subreddit_predist\n",
    "    # time\n",
    "    print(\"\\tCreate weekday_dist...\")\n",
    "    weekday = timecounter(df['weekday'], 'weekday')\n",
    "    weekday = weekday.tolist()\n",
    "    df['weekday_dist'] = weekday\n",
    "    print(\"\\tCreate month_dist...\")\n",
    "    month = timecounter(df['month'], 'month')\n",
    "    month = month.tolist()\n",
    "    df['month_dist'] = month\n",
    "    print(\"\\tCreate year_dist...\")\n",
    "    year = timecounter(df['year'], 'year')\n",
    "    year = year.tolist()\n",
    "    df['year_dist'] = year\n",
    "    \n",
    "    print(\"\\tCreate new aggregated df...\")\n",
    "    newdf = df[['author', 'body', 'doc_body', 'utc', 'controversiality', \n",
    "                'gilded', 'num_subreddit', 'subreddit_dist', 'weekday_dist', \n",
    "                'month_dist', 'year_dist', 'lang']]\n",
    "    print(\"\\tSort new aggregated df...\")\n",
    "    newdf = newdf.sort_values(by='author')\n",
    "    print(\"\\tDrop duplicates in new aggregated df...\")\n",
    "    newdf = newdf.drop_duplicates(subset=['author'])\n",
    "    return newdf\n",
    "\n",
    "# get one column for each feature in the distributions of time and subreddit\n",
    "weekday = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "month = ['january', 'february', 'march', 'april', 'may', 'june', 'juli', 'august', 'september', 'october', 'november', 'december']\n",
    "year = ['2015', '2016', '2017', '2018', '2019']\n",
    "\n",
    "def onecolumnperdatapoint(df, column, namelist):\n",
    "    for i in tqdm(range(len(namelist))):\n",
    "        df[namelist[i]] = df[column].apply(lambda x:[x[i]])\n",
    "        df[namelist[i]] = [item[0] for item in df[namelist[i]]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create comment df (name: pandora)...\n",
      "Create new df grouped by author...\n",
      "\tCreate numeric language representation...\n",
      "\tCreate time columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116cda8795d1449e88e1f9672d832588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3103208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCreate new ungrouped columns...\n",
      "\tGroup df by author...\n",
      "Create new columns with features...\n",
      "\tCreate controversiality column...\n",
      "\tCreate mean_gilded...\n",
      "\tCreate subreddit_dist...\n",
      "\tCreate weekday_dist...\n",
      "\tCreate month_dist...\n",
      "\tCreate year_dist...\n",
      "\tCreate new aggregated df...\n",
      "\tSort new aggregated df...\n",
      "\tDrop duplicates in new aggregated df...\n",
      "Distribute the weekday_dist to several columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783b1da594a44c8a98434947153f0ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribute the month_dist to several columns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f04dea72fda4b8193032cdafa598b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribute the year_dist to several columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc2ecd09ca04562b23b87617686cb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribute the subreddit_dist to several columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bd3b5a9c3c4be3b45e63a81c4ee1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16063 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop dist columns...\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "# Wrapper for commentdf\n",
    "def create_commentdf(df):\n",
    "    print(\"Create new df grouped by author...\")\n",
    "    groupdf = create_groupdf(df)\n",
    "    print(\"Create new columns with features...\")\n",
    "    pandora = create_new_columns(groupdf)\n",
    "    print(\"Distribute the weekday_dist to several columns...\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'weekday_dist', weekday)\n",
    "    print(\"Distribute the month_dist to several columns\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'month_dist', month)\n",
    "    print(\"Distribute the year_dist to several columns...\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'year_dist', year)\n",
    "    print(\"Distribute the subreddit_dist to several columns...\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'subreddit_dist', subredditlist)\n",
    "    print(\"Drop dist columns...\")\n",
    "    pandora.drop(['weekday_dist', 'month_dist', 'year_dist', 'subreddit_dist'], axis=1, inplace=True)\n",
    "    return pandora\n",
    "\n",
    "# create commentdf\n",
    "print(\"Create comment df (name: pandora)...\")\n",
    "pandora = create_commentdf(df)\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort pandora df...\n",
      "Sort big five df...\n",
      "Set pandora index...\n",
      "Set bigfive index...\n",
      "Join commentdf and authordf\n",
      "Create binary representations for each personality trait\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>doc_body</th>\n",
       "      <th>utc</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>num_subreddit</th>\n",
       "      <th>lang</th>\n",
       "      <th>monday</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>wednesday</th>\n",
       "      <th>...</th>\n",
       "      <th>tentaclenuke</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agree</th>\n",
       "      <th>openn</th>\n",
       "      <th>consc</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-Areopagan-</th>\n",
       "      <td>Your first and second question is the same que...</td>\n",
       "      <td>[Your first and second question is the same qu...</td>\n",
       "      <td>[1513882848, 1513744846, 1522253427, 151370438...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-BigSexy-</th>\n",
       "      <td>I've been asked to cum everywhere with my ex j...</td>\n",
       "      <td>[I've been asked to cum everywhere with my ex ...</td>\n",
       "      <td>[1507650565, 1516397088, 1502590403, 151682490...</td>\n",
       "      <td>0.020737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>444</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-BlitzN9ne</th>\n",
       "      <td>I'm currently in the middle of making a Payday...</td>\n",
       "      <td>[I'm currently in the middle of making a Payda...</td>\n",
       "      <td>[1422166355, 1423504286, 1449881503, 145521567...</td>\n",
       "      <td>0.014159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>393</td>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-CrestiaBell</th>\n",
       "      <td>First and foremost I extend my condolences to ...</td>\n",
       "      <td>[First and foremost I extend my condolences to...</td>\n",
       "      <td>[1462304635, 1528773104, 1513663029, 148131600...</td>\n",
       "      <td>0.017687</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>1258</td>\n",
       "      <td>0</td>\n",
       "      <td>1210</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-dyad-</th>\n",
       "      <td>I failed both...I'm great at reading people ir...</td>\n",
       "      <td>[I failed both...I'm great at reading people i...</td>\n",
       "      <td>[1475875524, 1473096864, 1505168466, 150318014...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zugzwang_03</th>\n",
       "      <td>You know that giggly group of women going to t...</td>\n",
       "      <td>[You know that giggly group of women going to ...</td>\n",
       "      <td>[1466099531, 1469625145, 1455352713, 150837533...</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>146</td>\n",
       "      <td>3</td>\n",
       "      <td>2409</td>\n",
       "      <td>0</td>\n",
       "      <td>2317</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuluthrone</th>\n",
       "      <td>I saw some speculate that the \"download\" would...</td>\n",
       "      <td>[I saw some speculate that the \"download\" woul...</td>\n",
       "      <td>[1438979642, 1451195516, 1468833505, 151331960...</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zwelg</th>\n",
       "      <td>I am actually pretty pleased about my score:Ag...</td>\n",
       "      <td>[I am actually pretty pleased about my score:A...</td>\n",
       "      <td>[1508185843]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zymmaster</th>\n",
       "      <td>Respectfully disagree. Offense had plenty of i...</td>\n",
       "      <td>[Respectfully disagree. Offense had plenty of ...</td>\n",
       "      <td>[1455228093, 1476665332, 1468599441, 146004506...</td>\n",
       "      <td>0.010444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>394</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyzee</th>\n",
       "      <td>Tarzaned can't be compared in this situation. ...</td>\n",
       "      <td>[Tarzaned can't be compared in this situation....</td>\n",
       "      <td>[1494445453, 1487154474, 1521448885, 142322120...</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1606 rows × 16099 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           body  \\\n",
       "author                                                            \n",
       "-Areopagan-   Your first and second question is the same que...   \n",
       "-BigSexy-     I've been asked to cum everywhere with my ex j...   \n",
       "-BlitzN9ne    I'm currently in the middle of making a Payday...   \n",
       "-CrestiaBell  First and foremost I extend my condolences to ...   \n",
       "-dyad-        I failed both...I'm great at reading people ir...   \n",
       "...                                                         ...   \n",
       "zugzwang_03   You know that giggly group of women going to t...   \n",
       "zuluthrone    I saw some speculate that the \"download\" would...   \n",
       "zwelg         I am actually pretty pleased about my score:Ag...   \n",
       "zymmaster     Respectfully disagree. Offense had plenty of i...   \n",
       "zyzee         Tarzaned can't be compared in this situation. ...   \n",
       "\n",
       "                                                       doc_body  \\\n",
       "author                                                            \n",
       "-Areopagan-   [Your first and second question is the same qu...   \n",
       "-BigSexy-     [I've been asked to cum everywhere with my ex ...   \n",
       "-BlitzN9ne    [I'm currently in the middle of making a Payda...   \n",
       "-CrestiaBell  [First and foremost I extend my condolences to...   \n",
       "-dyad-        [I failed both...I'm great at reading people i...   \n",
       "...                                                         ...   \n",
       "zugzwang_03   [You know that giggly group of women going to ...   \n",
       "zuluthrone    [I saw some speculate that the \"download\" woul...   \n",
       "zwelg         [I am actually pretty pleased about my score:A...   \n",
       "zymmaster     [Respectfully disagree. Offense had plenty of ...   \n",
       "zyzee         [Tarzaned can't be compared in this situation....   \n",
       "\n",
       "                                                            utc  \\\n",
       "author                                                            \n",
       "-Areopagan-   [1513882848, 1513744846, 1522253427, 151370438...   \n",
       "-BigSexy-     [1507650565, 1516397088, 1502590403, 151682490...   \n",
       "-BlitzN9ne    [1422166355, 1423504286, 1449881503, 145521567...   \n",
       "-CrestiaBell  [1462304635, 1528773104, 1513663029, 148131600...   \n",
       "-dyad-        [1475875524, 1473096864, 1505168466, 150318014...   \n",
       "...                                                         ...   \n",
       "zugzwang_03   [1466099531, 1469625145, 1455352713, 150837533...   \n",
       "zuluthrone    [1438979642, 1451195516, 1468833505, 151331960...   \n",
       "zwelg                                              [1508185843]   \n",
       "zymmaster     [1455228093, 1476665332, 1468599441, 146004506...   \n",
       "zyzee         [1494445453, 1487154474, 1521448885, 142322120...   \n",
       "\n",
       "              controversiality    gilded  num_subreddit  lang  monday  \\\n",
       "author                                                                  \n",
       "-Areopagan-           0.000000  0.000000              1     1       0   \n",
       "-BigSexy-             0.020737  0.000000            147     4     359   \n",
       "-BlitzN9ne            0.014159  0.000000            116     4     393   \n",
       "-CrestiaBell          0.017687  0.000866            149     4    1258   \n",
       "-dyad-                0.000000  0.000000              5     2      25   \n",
       "...                        ...       ...            ...   ...     ...   \n",
       "zugzwang_03           0.011709  0.000291            146     3    2409   \n",
       "zuluthrone            0.018458  0.000000             46     3     119   \n",
       "zwelg                 0.000000  0.000000              1     1       1   \n",
       "zymmaster             0.010444  0.000000             99     3     300   \n",
       "zyzee                 0.010989  0.000000             13     3      11   \n",
       "\n",
       "              tuesday  wednesday  ...  tentaclenuke  agreeableness  openness  \\\n",
       "author                            ...                                          \n",
       "-Areopagan-         0          2  ...             0            0.0      99.0   \n",
       "-BigSexy-           0        444  ...             0           39.0      92.0   \n",
       "-BlitzN9ne          0        343  ...             0           50.0      85.0   \n",
       "-CrestiaBell        0       1210  ...             0           50.0      85.0   \n",
       "-dyad-              0         40  ...             0           60.0      67.0   \n",
       "...               ...        ...  ...           ...            ...       ...   \n",
       "zugzwang_03         0       2317  ...             0           10.0      41.0   \n",
       "zuluthrone          0        142  ...             0           17.0      96.0   \n",
       "zwelg               0          0  ...             0           39.0      89.0   \n",
       "zymmaster           0        394  ...             0           28.0      47.0   \n",
       "zyzee               0         19  ...             0           88.0      78.0   \n",
       "\n",
       "              conscientiousness  extraversion  neuroticism  agree  openn  \\\n",
       "author                                                                     \n",
       "-Areopagan-                96.0          60.0          1.0      0      1   \n",
       "-BigSexy-                   1.0          18.0          4.0      0      1   \n",
       "-BlitzN9ne                 15.0          50.0         30.0      1      1   \n",
       "-CrestiaBell               50.0          85.0         50.0      1      1   \n",
       "-dyad-                     45.0          10.0         47.0      1      1   \n",
       "...                         ...           ...          ...    ...    ...   \n",
       "zugzwang_03                86.0          83.0         18.0      0      0   \n",
       "zuluthrone                 28.0          95.0         34.0      0      1   \n",
       "zwelg                      91.0          80.0          3.0      0      1   \n",
       "zymmaster                  62.0          21.0         49.0      0      0   \n",
       "zyzee                      31.0          75.0         10.0      1      1   \n",
       "\n",
       "              consc  extra  \n",
       "author                      \n",
       "-Areopagan-       1      1  \n",
       "-BigSexy-         0      0  \n",
       "-BlitzN9ne        0      1  \n",
       "-CrestiaBell      1      1  \n",
       "-dyad-            0      0  \n",
       "...             ...    ...  \n",
       "zugzwang_03       1      1  \n",
       "zuluthrone        0      1  \n",
       "zwelg             1      1  \n",
       "zymmaster         1      0  \n",
       "zyzee             0      1  \n",
       "\n",
       "[1606 rows x 16099 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge commentdf and authordf\n",
    "print(\"Sort pandora df...\")\n",
    "pandora= pandora.sort_values(by='author')\n",
    "print(\"Sort big five df...\")\n",
    "bigfive= bigfive.sort_values(by='author')\n",
    "if pandora.index.name != 'author':\n",
    "    print(\"Set pandora index...\")\n",
    "    pandora = pandora.set_index('author')\n",
    "if bigfive.index.name != 'author':\n",
    "    print(\"Set bigfive index...\")\n",
    "    bigfive = bigfive.set_index('author')\n",
    "print(\"Join commentdf and authordf\")\n",
    "pandoradf = pandora.join(bigfive)\n",
    "# pandoradf = pandoradf.reset_index()\n",
    "\n",
    "# create binary representation of personality traits\n",
    "def bigfive_cat(df):\n",
    "    # change big five to binary representation\n",
    "    df['agree'] = df['agreeableness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['openn'] = df['openness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['consc'] = df['conscientiousness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['extra'] = df['extraversion'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['neuro'] = df['neuroticism'].apply(lambda x: 0 if x<50 else 1)\n",
    "    return df\n",
    "\n",
    "print(\"Create binary representations for each personality trait\")\n",
    "pandoradf = bigfive_cat(pandoradf)\n",
    "pandoradf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# define stopwordlist to use\n",
    "def choose_stopwordlist(df, mode):\n",
    "    if mode == 'NLTK':\n",
    "        stopwordList = stopwords.words('english')\n",
    "    if mode == 'NLTK-neg':\n",
    "        stopwordList = stopwords.words('english')\n",
    "        stopwordList.remove('no')\n",
    "        stopwordList.remove('nor')\n",
    "        stopwordList.remove('not')\n",
    "    return stopwordList\n",
    "\n",
    "# remove decontractions\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# create sentence tokens\n",
    "def senttokenize(df):\n",
    "    sentbody = []\n",
    "    for row in tqdm(df['doc_body']):\n",
    "        sentitem = []\n",
    "        for item in row:\n",
    "            sentences = sent_tokenize(item)\n",
    "            sentitem.append(sentences)\n",
    "        sentbody.append(sentitem)\n",
    "    df['senttokens'] = sentbody\n",
    "    return df\n",
    "\n",
    "# lower words and remove special characters\n",
    "def lower_special(df):\n",
    "    newrow = []\n",
    "    for row in tqdm(df['probody']):\n",
    "        newcomment = []\n",
    "        for comment in row:\n",
    "            text_pre = \"\"\n",
    "            for character in comment:\n",
    "                if character.isalnum() or character.isspace():\n",
    "                    character = character.lower()\n",
    "                    text_pre += character\n",
    "                else:\n",
    "                    text_pre += \" \"\n",
    "            newcomment.append(text_pre)\n",
    "        newrow.append(newcomment)   \n",
    "    df['probody'] = newrow\n",
    "    return df\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(df, stopwordList):\n",
    "    newprobody = []\n",
    "    for row in tqdm(df['probody']):\n",
    "        newrowprobody = []\n",
    "        for comment in row:\n",
    "            words = [word for word in comment.split() if (word not in stopwordList)]\n",
    "            newcomment = ' '.join(words)\n",
    "            newrowprobody.append(newcomment)\n",
    "        newprobody.append(newrowprobody)\n",
    "    df['probody'] = newprobody\n",
    "    return df\n",
    "\n",
    "# change numbers to words and tokenize words\n",
    "\n",
    "import decimal\n",
    "def num_tokenize(df):    \n",
    "    newbody_complete = []\n",
    "    newprobody_complete = []\n",
    "    # num2words\n",
    "    for row in tqdm(df['probody']):\n",
    "        newbody = []\n",
    "        newprobody = []\n",
    "        for sentence in row:\n",
    "            # string to list\n",
    "            inputtext = sentence.split()\n",
    "            numlist = []\n",
    "            for i in range(len(inputtext)):\n",
    "                if inputtext[i].isnumeric():\n",
    "                    numlist.append(i)\n",
    "            for number in numlist:\n",
    "                # deleted: fractions, superscripts, extremely large numbers, 卌卌, 一\n",
    "                try:\n",
    "                    inputtext[number] = num2words(inputtext[number])\n",
    "                except decimal.InvalidOperation:\n",
    "                    inputtext[number] = \" \"\n",
    "                except OverflowError:\n",
    "                    inputtext[number] = \" \"\n",
    "\n",
    "            # list to string\n",
    "            inputtext = [word for word in inputtext if word.isalpha()]\n",
    "            celltext = ' '.join(inputtext)\n",
    "            newprobody.append(celltext)\n",
    "            # tokenize\n",
    "            words = word_tokenize(celltext)\n",
    "            newbody.append(words)\n",
    "        newbody_complete.append(newbody)\n",
    "        newprobody_complete.append(newprobody)\n",
    "    df['probody'] = newprobody_complete\n",
    "    df['tokens'] = newbody_complete\n",
    "    return df\n",
    "\n",
    "# Porter Stemmer\n",
    "def stemming(df):\n",
    "    ps = PorterStemmer()\n",
    "    for row in tqdm(df['tokens']):\n",
    "        for comment in row:\n",
    "            words = [ps.stem(word) for word in comment]\n",
    "            comment = ' '.join(words)\n",
    "    return df\n",
    "\n",
    "# bring columns of dataframe in correct order\n",
    "def ordering(df):\n",
    "    cols_tomove = ['body', 'doc_body', 'probody', 'tokens', 'senttokens', 'agreeableness', 'openness', 'conscientiousness', 'extraversion', 'neuroticism', 'agree', 'openn', 'consc', 'extra', 'neuro']\n",
    "    orderdf  = df[cols_tomove + [col for col in df.columns if col not in cols_tomove]]\n",
    "    return orderdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decontract...\n",
      "Tokenize Sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4b72b5459e4c3e84b35b197afff883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower words and remove special characters...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfefba241dd467884b25202bc35f2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3810a7d61a1d40debb20ea08a3e4e877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change numbers to words and tokenize words...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1984080c8f794eecb3a80da5f5eb69e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidOperation",
     "evalue": "[<class 'decimal.ConversionSyntax'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidOperation\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6048fe80f76a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# apply preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mpredf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandoradf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mpredf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-6048fe80f76a>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwordList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Change numbers to words and tokenize words...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# porters stemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Porters Stemmer...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-cfdf7253665a>\u001b[0m in \u001b[0;36mnum_tokenize\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mnumlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0minputtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum2words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# list to string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/num2words/__init__.py\u001b[0m in \u001b[0;36mnum2words\u001b[0;34m(number, ordinal, lang, to, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# backwards compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/num2words/base.py\u001b[0m in \u001b[0;36mstr_to_number\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstr_to_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDecimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_cardinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidOperation\u001b[0m: [<class 'decimal.ConversionSyntax'>]"
     ]
    }
   ],
   "source": [
    "# Wrapper\n",
    "\n",
    "def preprocess(df):\n",
    "    # adjust some column representations\n",
    "    df = bigfive_cat(df)\n",
    "    # choose stopwordlist with or without negation\n",
    "    stopwordList = choose_stopwordlist(df, mode='NLTK-neg')\n",
    "    # decontract abbreviations (e.g., n't to not)\n",
    "    print(\"Decontract...\")\n",
    "    df['probody'] = df['doc_body'].apply(lambda x:([decontracted(x) for x in x]))\n",
    "    # create sentence tokens\n",
    "    print(\"Tokenize Sentences...\")\n",
    "    df = senttokenize(df)\n",
    "    # lower, remove stopwords, num2words, tokenize\n",
    "    print(\"Lower words and remove special characters...\")\n",
    "    df = lower_special(df)\n",
    "    print(\"Remove stopwords...\")\n",
    "    df = remove_stopwords(df, stopwordList)\n",
    "    print(\"Change numbers to words and tokenize words...\")\n",
    "    df = num_tokenize(df)\n",
    "    # porters stemmer\n",
    "    print(\"Porters Stemmer...\")\n",
    "    df = stemming(df)\n",
    "    print(\"Order df...\")\n",
    "    df = ordering(df)\n",
    "    print(\"Done!\")\n",
    "    return df\n",
    "\n",
    "# apply preprocessing\n",
    "predf = preprocess(pandoradf)\n",
    "predf.to_pickle(\"preprocessed_df_allcomments.pkl\")\n",
    "predf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User features\n",
    "\n",
    "# Preprocessing for LDA\n",
    "def preprocess_lda(df):\n",
    "    neglst = [\"no\", \"not\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nowhere\", \"never\", \"nay\"]\n",
    "    inputlst = []\n",
    "    for row in tqdm(df['tokens']):\n",
    "        rowlst = []\n",
    "        for comment in row:\n",
    "            rowlst.append([word for word in comment if (word not in neglst)])\n",
    "        inputlst.append(rowlst)\n",
    "    return inputlst\n",
    "# LDA for topics\n",
    "def apply_lda(df, inputlst, number, name):\n",
    "    print(\"Start LDA...\")\n",
    "    lst = []\n",
    "    for row in tqdm(inputlst):\n",
    "        if len(row) < 2:\n",
    "            lst.append(-1)\n",
    "        else:\n",
    "            dictionary = corpora.Dictionary(row)\n",
    "            corpus = [dictionary.doc2bow(text) for text in row]\n",
    "            ldamodel = gensim.models.LdaMulticore(corpus, num_topics=number, id2word = dictionary, passes=20, workers=15)\n",
    "            result = ldamodel.print_topics(num_topics=1, num_words=1)\n",
    "            res = list(result)\n",
    "            topic = [item[0] for item in res]\n",
    "            lst.append(topic[0])\n",
    "    df[name] = lst\n",
    "    return df\n",
    "\n",
    "# Wrapper\n",
    "def extract_userfeatures(df):\n",
    "    print(\"Preprocessing for LDA...\")\n",
    "    inputlst = preprocess_lda(df)\n",
    "    print(\"LDA with fifty topics: \")\n",
    "    df = apply_lda(df, inputlst, 50, \"ldafifty\")\n",
    "    print(\"LDA with onehundred topics: \")\n",
    "    df = apply_lda(df, inputlst, 100, \"ldahundred\")\n",
    "    return df\n",
    "\n",
    "# create df with user features\n",
    "# user_feat_df = extract_userfeatures(predf)\n",
    "# user_feat_df.to_pickle(\"user_feat_df_allcomments.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linguistic features (functions)\n",
    "\n",
    "# other features that are not mentioned in the paper\n",
    "def create_features(df):\n",
    "    # Total number of characters (including space)\n",
    "    df['char_count'] = df['complete_body'].str.len()\n",
    "    # Total number of stopwords\n",
    "    stopwordList = stopwords.words('english')\n",
    "    df['stopwords'] = df['complete_body'].apply(lambda x: len([x for x in x.split() if x in stopwordList]))\n",
    "    # Total number of punctuation or special characters\n",
    "    df['total_punc'] = df['complete_body'].apply(lambda x: len([x for x in x.split() for j in x if j in string.punctuation]))\n",
    "    # Total number of numerics\n",
    "    df['total_num'] = df['complete_body'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    # Total number of uppercase words\n",
    "    df['total_uppercase'] = df['complete_body'].apply(lambda x: len([x for x in x.split() if x.isupper()]))    \n",
    "    return df\n",
    "\n",
    "# type token ratio\n",
    "def typetokenratio(df):\n",
    "    ratiolst = []\n",
    "    for comment in df['complete_body']:\n",
    "            lex = LexicalRichness(comment)\n",
    "            if lex.words == 0:\n",
    "                ratiolst.append(0)\n",
    "            else:\n",
    "                ratio = lex.ttr\n",
    "                ratiolst.append(ratio)\n",
    "    df['ttr'] = ratiolst\n",
    "    return df\n",
    "\n",
    "# words per sentence\n",
    "def wordcounter(df):\n",
    "    lengthscore = []\n",
    "    for row in df['senttokens']:\n",
    "        rowscore = []\n",
    "        for comment in row:\n",
    "            sentencescore = 0\n",
    "            for senttoken in comment:\n",
    "                length = len(senttoken.split())\n",
    "                sentencescore += length\n",
    "            sentencescore = sentencescore/len(comment)\n",
    "        lengthscore.append(sentencescore)\n",
    "        arr = np.array(lengthscore)\n",
    "    df['words_per_sent'] = lengthscore\n",
    "    return df\n",
    "\n",
    "# words longer than six characters\n",
    "def charcounter(df):\n",
    "    charscore = []\n",
    "    for row in df['tokens']:\n",
    "        for comment in row:\n",
    "            rowcharscore = 0\n",
    "            lencomment = len(comment)\n",
    "            if lencomment == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                number = 0\n",
    "                for token in comment:\n",
    "                    length = len(token)\n",
    "                    if length > 5:\n",
    "                        number+=1\n",
    "                score = number/lencomment\n",
    "            rowcharscore += score\n",
    "        rowcharscore = rowcharscore/len(row)\n",
    "        charscore.append(rowcharscore)\n",
    "    df['wordslongersix'] = charscore\n",
    "    return df\n",
    "\n",
    "# POS tagger\n",
    "def tagging(df):\n",
    "    past = [] #VPA\n",
    "    presence = [] #VPR\n",
    "    adverbs = [] #RB\n",
    "    prepositions = [] #PREP\n",
    "    pronouns = [] #PR\n",
    "    for comment in df['complete_body']:\n",
    "            text = comment.split()\n",
    "            tags = nltk.pos_tag(text)\n",
    "            counts = Counter(tag for word,tag in tags)\n",
    "            total = sum(counts.values())\n",
    "            pron = counts['PRP'] + counts['PRP$']\n",
    "            verbspr = counts['VB'] + counts['VBG'] + counts['VBP'] + counts['VBZ'] + counts['MD']\n",
    "            verbspa = counts['VBD'] + counts['VBN']\n",
    "            preps = counts['IN'] + counts['TO']\n",
    "            counts['PR'] = pron\n",
    "            counts['PREP'] = preps\n",
    "            counts['VPR'] = verbspr #present tense\n",
    "            counts['VPA'] = verbspa #past tense\n",
    "            if total == 0:\n",
    "                allcounts = dict((word, float(count)/1) for word,count in counts.items())\n",
    "            else:\n",
    "                allcounts = dict((word, float(count)/total) for word,count in counts.items())\n",
    "            try:\n",
    "                past.append(allcounts['VPA'])\n",
    "            except KeyError:\n",
    "                past.append(0)\n",
    "            try:\n",
    "                presence.append(allcounts['VPR'])\n",
    "            except KeyError:\n",
    "                presence.append(0)\n",
    "            try:\n",
    "                adverbs.append(allcounts['RB'])\n",
    "            except KeyError:\n",
    "                adverbs.append(0)\n",
    "            try:\n",
    "                prepositions.append(allcounts['PREP'])\n",
    "            except KeyError:\n",
    "                prepositions.append(0)\n",
    "            try:\n",
    "                pronouns.append(allcounts['PR'])\n",
    "            except KeyError:\n",
    "                pronouns.append(0)\n",
    "    df['pasttense'] = past\n",
    "    df['presencetense'] = presence\n",
    "    df['adverbs'] = adverbs\n",
    "    df['prepositions'] = prepositions\n",
    "    df['pronouns'] = pronouns\n",
    "    return df\n",
    "\n",
    "def ngrams(df, n_min, n_max, ngramtype):\n",
    "    # convert input from list to string\n",
    "    ngrams = []\n",
    "    inputtext = []\n",
    "    for row in df['tokens']:\n",
    "        for comment in row:\n",
    "            text = ' '.join(comment)\n",
    "        inputtext.append(text)\n",
    "    print(\"Length of inputtext: \", len(inputtext))\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n_min,n_max), analyzer=ngramtype)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    print(\"Get feature names...\")\n",
    "    names = vectorizer.get_feature_names()\n",
    "    print(\"Length of feature names: \", len(names))\n",
    "    print(\"Create df...\")\n",
    "    ngramdf = pd.DataFrame(denselist, columns=names)\n",
    "#     ngramdf['author'] = df['author']\n",
    "    return ngramdf\n",
    "\n",
    "def merge_dfs(df1, df2, df3):\n",
    "#     cwngramsdf = pd.merge(df1, df2, on='author', how='inner', suffixes= (None, \"_charngram\"))\n",
    "#     gramsdf = pd.merge(df3, cwngramsdf, on='author', how='inner', suffixes= (None, \"_ngram\"))\n",
    "    ngramsdf = df1.join(df2)\n",
    "    newdf = df3.join(ngramsdf)\n",
    "    return gramsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for linguistic features\n",
    "\n",
    "def extract_lin_features(df, create_ngrams):\n",
    "    print(\"Create additional features...\")\n",
    "    df = create_features(df)\n",
    "    print(\"Create ttr...\")\n",
    "    df = typetokenratio(df)\n",
    "    print(\"Count words per sentence...\")\n",
    "    df = wordcounter(df)\n",
    "    print(\"Count words with more than six letters...\")\n",
    "    df = charcounter(df)\n",
    "    print(\"POS-Tagger...\")\n",
    "    df = tagging(df)\n",
    "    print(\"number of rows df\", len(df))\n",
    "    \n",
    "    if create_ngrams == \"none\":\n",
    "        return df\n",
    "    \n",
    "    elif create_ngrams == \"all\":\n",
    "        print(\"Ngrams...\")\n",
    "        print(\"Create word ngrams...\")\n",
    "        wordngramsdf = ngrams(df, 1, 3, \"word\")\n",
    "        print(\"Create char ngrams...\")\n",
    "        charngramsdf = ngrams(df, 2, 3, \"char\")\n",
    "        print(\"Merge df...\")\n",
    "        gramsdf = merge_dfs(wordngramsdf, charngramsdf, df)\n",
    "        return gramsdf\n",
    "    \n",
    "    elif create_ngrams == \"word\":\n",
    "        wordngrams = ngrams(df, 1, 3, 'word')\n",
    "        wordngramsdf = pd.DataFrame(wordngrams)\n",
    "#         gramsdf = pd.merge(df, wordngramsdf, on='author', how='inner', suffixes=(None, \"_ngram\"))\n",
    "        gramsdf = df.join(wordngramsdf)\n",
    "        return gramsdf\n",
    "    \n",
    "# create dataframe with linguistic features\n",
    "\n",
    "# without ngrams\n",
    "# lin_feat_df = extract_lin_features(user_feat_df, \"none\")\n",
    "\n",
    "# with all ngrams\n",
    "# lin_ngrams_df = extract_lin_features(user_feat_df, \"all\")\n",
    "\n",
    "lin_ngrams_df = extract_lin_features(predf, \"all\")\n",
    "lin_ngrams_df.to_pickle(\"lin_feat_df_withoutuserfeat_allcomments.pkl\")\n",
    "\n",
    "# wordngrams only\n",
    "# lin_wordngrams_df = extract_lin_features(user_feat_df, \"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordlists (functions)\n",
    "\n",
    "# Empath\n",
    "# create new categories with empath\n",
    "def new_cat():\n",
    "    empath = Empath()\n",
    "    social = empath.create_category(\"social\",[\"mate\",\"talk\",\"they\"])\n",
    "    humans = empath.create_category(\"humans\",[\"adult\",\"baby\",\"boy\"])\n",
    "    cognitive = empath.create_category(\"cognitive\",[\"cause\",\"know\",\"ought\"])\n",
    "    insight = empath.create_category(\"insight\",[\"think\",\"know\",\"consider\"])\n",
    "    causation = empath.create_category(\"causation\",[\"because\",\"effect\",\"hence\"])\n",
    "    discrepancy = empath.create_category(\"discrepancy\",[\"should\",\"would\",\"could\"])\n",
    "    tentative = empath.create_category(\"tentative\",[\"maybe\",\"perhaps\",\"guess\"])\n",
    "    certainty = empath.create_category(\"certainty\",[\"always\",\"never\", \"proof\"])\n",
    "    inhibition = empath.create_category(\"inhibition\",[\"block\",\"constrain\",\"stop\"])\n",
    "    inclusive = empath.create_category(\"inclusive\",[\"and\",\"with\",\"include\"])\n",
    "    exclusive = empath.create_category(\"exclusive\",[\"but\",\"without\",\"exclude\"])\n",
    "    perceptual = empath.create_category(\"perceptual\",[\"observing\",\"hear\",\"feeling\"])\n",
    "    see = empath.create_category(\"see\",[\"view\",\"saw\",\"seen\"])\n",
    "    feel = empath.create_category(\"feel\",[\"feels\",\"touch\",\"feeling\"])\n",
    "    biological = empath.create_category(\"biological\",[\"eat\",\"blood\",\"pain\"])\n",
    "    relativity = empath.create_category(\"relativity\",[\"area\",\"bend\",\"go\"])\n",
    "    space = empath.create_category(\"space\",[\"down\",\"in\",\"thin\"])\n",
    "    time = empath.create_category(\"time\",[\"end\",\"until\",\"season\"])\n",
    "    agreement = empath.create_category(\"agreement\", [\"agree\", \"ok\", \"yes\"])\n",
    "    fillers = empath.create_category(\"fillers\", [\"like\", \"Imean\", \"yaknow\"])\n",
    "    nonfluencies = empath.create_category(\"nonfluencies\", [\"umm\", \"hm\", \"er\"])\n",
    "    conjunctions = empath.create_category(\"conjunctions\", [\"and\", \"but\", \"whereas\"])\n",
    "    quantifiers = empath.create_category(\"quantifiers\", [\"few\", \"many\", \"much\"])\n",
    "    numbers = empath.create_category(\"numbers\", [\"two\", \"fourteen\", \"thousand\"])\n",
    "\n",
    "def apply_empath(df):\n",
    "    empath = Empath()\n",
    "    print(\"Create new empath categories...\")\n",
    "    new_cat()\n",
    "    print(\"Apply empath...\")\n",
    "    empathvalues = []\n",
    "    empathcategories = [\"swearing_terms\", \"social\", \"family\", \"friends\", \"humans\", \"emotional\", \"positive_emotion\", \"negative_emotion\", \"fear\", \"anger\", \"sadness\", \"cognitive\", \"insight\", \"causation\", \"discrepancy\", \"tentative\", \"certainty\", \"inhibition\", \"inclusive\", \"exclusive\", \"perceptual\", \"see\", \"hear\", \"feel\", \"biological\", \"body\", \"health\", \"sexual\", \"eat\", \"relativity\", \"space\", \"time\", \"work\", \"achievement\", \"leisure\", \"home\", \"money\", \"religion\", \"death\" ,\"agreement\", \"fillers\", \"nonfluencies\"]\n",
    "    for sentence in tqdm(df['complete_body']):\n",
    "        empathvalues.append(empath.analyze(sentence, categories=empathcategories, normalize=True))\n",
    "    empathdf = pd.DataFrame(empathvalues)\n",
    "#     empathdf['author'] = df['author']\n",
    "\n",
    "#     newdf = pd.merge(df, empathdf, on='author', how='inner', suffixes=(None, \"_wordlist\"))\n",
    "    newdf = df.join(empathdf)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data for other wordlists\n",
    "concretenessdf = pd.read_csv('/home/sophia/ma_py/psych_lists/concreteness.csv')\n",
    "cdf = concretenessdf[['Conc.M']]\n",
    "cmatrix = cdf.to_numpy()\n",
    "concrete = concretenessdf['Word'].values.tolist()\n",
    "\n",
    "happinessdf = pd.read_csv('/home/sophia/ma_py/psych_lists/happiness_ratings.csv')\n",
    "hdf = happinessdf[['happiness_average']]\n",
    "hmatrix = hdf.to_numpy()\n",
    "happiness = happinessdf['word'].values.tolist()\n",
    "\n",
    "cursedf = pd.read_csv('/home/sophia/ma_py/psych_lists/mean_good_curse.csv')\n",
    "cudf = cursedf[['mean_good_curse']]\n",
    "cumatrix = cudf.to_numpy()\n",
    "curse = cursedf['word'].values.tolist()\n",
    "\n",
    "sensorydf = pd.read_csv('/home/sophia/ma_py/psych_lists/sensory_experience_ratings.csv')\n",
    "serdf = sensorydf[['Average SER']]\n",
    "sermatrix = serdf.to_numpy()\n",
    "ser = sensorydf['Word'].values.tolist()\n",
    "\n",
    "alldf = pd.read_csv('/home/sophia/ma_py/psych_lists/sensory_ratings_all.csv')\n",
    "newalldf = alldf[['Emotion', 'Polarity', 'Social', 'Moral', 'MotionSelf', 'Thought', 'Color', 'TasteSmell', 'Tactile', 'VisualForm', 'Auditory', 'Space', 'Quantity', 'Time', 'CNC', 'IMG', 'FAM']]\n",
    "allmatrix = newalldf.to_numpy()\n",
    "allsens = alldf['Word'].values.tolist()\n",
    "\n",
    "valarodomdf = pd.read_csv('/home/sophia/ma_py/psych_lists/valence_arousal_dominence.csv')\n",
    "vaddf = valarodomdf[['V.Mean.Sum', 'A.Mean.Sum', 'D.Mean.Sum']]\n",
    "vadmatrix = vaddf.to_numpy()\n",
    "vad = valarodomdf['Word'].values.tolist()\n",
    "\n",
    "mrcdf = pd.read_csv('/home/sophia/ma_py/psych_lists/mrclists_c_p.csv', sep='\\t', names=['word', 'cmean', 'pmean'])\n",
    "cpdf = mrcdf[['cmean', 'pmean']]\n",
    "cpmatrix = cpdf.to_numpy()\n",
    "mrc = mrcdf['word'].values.tolist()\n",
    "\n",
    "# function for other wordlists\n",
    "\n",
    "def counter(df, vocab):\n",
    "    inputtext = []\n",
    "    for row in df['complete_body']:\n",
    "        text = ' '.join(row)\n",
    "        inputtext.append(text)\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,1), vocabulary = vocab)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "def multiply(matrix, ratings):\n",
    "    # matrix multiplication \n",
    "    result = np.matmul(matrix, ratings)\n",
    "    # divide each score with the number of words in the list to normalize\n",
    "    result = result/(len(ratings))\n",
    "    return result\n",
    "\n",
    "def aggregator(df, vocab, ratings, name):\n",
    "    count = counter(df, vocab)\n",
    "    result = multiply(count, ratings)\n",
    "    num_rows, num_cols = result.shape\n",
    "    \n",
    "    if num_cols ==1:\n",
    "        df[name] = result\n",
    "    else:\n",
    "        resultdf = pd.DataFrame(result)\n",
    "        for i in range(len(name)):\n",
    "            # first i is zero\n",
    "            column = name[i]\n",
    "            df[column] = resultdf[i]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordlists created manually\n",
    "\n",
    "negations = [\"no\", \"not\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nowhere\", \"never\", \"nay\"]\n",
    "articles = [\"a\", \"an\", \"the\"]\n",
    "future = [\"will\", \"gonna\"]\n",
    "\n",
    "def list_counter(df, vocab, name):\n",
    "    inputtext = []\n",
    "    total = []\n",
    "    for row in df['complete_body']:\n",
    "        total.append(len(row))\n",
    "        text = ' '.join(row)\n",
    "        inputtext.append(text)\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,1), vocabulary = vocab)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    v = vectors.toarray()\n",
    "    averagev = v.sum(axis=1)\n",
    "    totalvector =  np.array(total)\n",
    "    score = np.divide(averagev, totalvector)\n",
    "    df[name] = score\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for wordlists\n",
    "\n",
    "def extract_wordlist_features(df):\n",
    "    print(\"Empath...\")\n",
    "    empdf = apply_empath(df)\n",
    "    # create scores for each word list and add them to df\n",
    "    print(\"Count Wordlist Concreteness: \\n\")\n",
    "    psychdf = aggregator(empdf, concrete, cmatrix, \"concreteness\")\n",
    "    print(\"Count Wordlist Happiness: \\n\")\n",
    "    psychdf = aggregator(empdf, happiness, hmatrix, \"happiness\")\n",
    "    print(\"Count Wordlist Good_Curse: \\n\")\n",
    "    psychdf = aggregator(empdf, curse, cumatrix, \"good_curse\")\n",
    "    print(\"Count 17 further wordlists: \\n\")\n",
    "    psychdf = aggregator(empdf, allsens, allmatrix, ['emotion', 'polarity', 'social', 'moral', 'motionself', 'thought', 'color', 'tastesmell', 'tactile', 'visualform', 'auditory', 'space', 'quantity', 'time', 'CNC', 'IMG', 'FAM'])\n",
    "    print(\"Count Wordlist SER: \\n\")\n",
    "    psychdf = aggregator(empdf, ser, sermatrix, \"SER\")\n",
    "    print(\"Count Wordlists Valence, Arousal, Dominance: \\n\")\n",
    "    psychdf = aggregator(empdf, vad, vadmatrix, ['valence', 'arousal', 'dominance'])\n",
    "    print(\"Count Wordlist Negation: \\n\")\n",
    "    psychdf = list_counter(empdf, negations, \"negations\")\n",
    "    print(\"Count Wordlist Articles: \\n\")\n",
    "    psychdf = list_counter(empdf, articles, \"articles\")\n",
    "    print(\"Count Wordlist Future: \\n\")\n",
    "    psychdf = list_counter(empdf, future, \"future\")\n",
    "    print(\"Count Wordlists from MRC (2): \\n\")\n",
    "    psychdf = aggregator(empdf, mrc, cpmatrix, [\"mrc_cmean\", \"mrc_pmean\"])\n",
    "    \n",
    "    return psychdf\n",
    "\n",
    "psychdf = extract_wordlist_features(lin_ngrams_df)\n",
    "psychdf.to_pickle(\"wordlists_lin_feat_df_withoutuserfeat_allcomments.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "# histogram of distribution of traits in dataset\n",
    "def all_hist_true(df):\n",
    "    plt.figure(figsize = (16, 8))\n",
    "#     plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(df['agreeableness'], bins = 20)\n",
    "    plt.title('Agreeableness')\n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.hist(df['openness'], bins = 20)\n",
    "    plt.title('Openness')\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.hist(df['conscientiousness'], bins = 20)\n",
    "    plt.title('Conscientiousness')\n",
    "    \n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.hist(df['extraversion'], bins = 20)\n",
    "    plt.title('Extraversion')\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.hist(df['neuroticism'], bins = 20)\n",
    "    plt.title('Neuroticism')\n",
    "    \n",
    "    plt.suptitle(\"Histograms of the true trait values\")\n",
    "    plt.subplots_adjust(left=0.1, \n",
    "                    bottom=0.1,  \n",
    "                    right=0.9,  \n",
    "                    top=0.9,  \n",
    "                    wspace=0.4,  \n",
    "                    hspace=0.4) \n",
    "    plt.show()\n",
    "\n",
    "#split dataset in features and target variable depending on which trait to focus on\n",
    "def trait(df, trait_name, startnumber):\n",
    "    featurelist = df.columns.tolist()\n",
    "    feature_cols = featurelist[startnumber:]\n",
    "    x = df[feature_cols] \n",
    "    \n",
    "    if trait_name == 'agree':\n",
    "        y = df.agree\n",
    "    elif trait_name == 'openn':\n",
    "        y = df.openn\n",
    "    elif trait_name == 'consc':\n",
    "        y = df.consc\n",
    "    elif trait_name == 'extra':\n",
    "        y = df.extra\n",
    "    elif trait_name == 'neuro':\n",
    "        y = df.neuro       \n",
    "    return x,y \n",
    "\n",
    "# create pipeline\n",
    "def create_pipeline(x_train, y_train ,classifier):\n",
    "    if classifier == \"log\":\n",
    "        pipeline = Pipeline([\n",
    "          ('variance_threshold', VarianceThreshold()),\n",
    "          ('feature_selection',  SelectKBest(f_classif, k=30)),\n",
    "          ('scaler', StandardScaler()),\n",
    "          ('classification',LogisticRegression(n_jobs=-1))\n",
    "        ])\n",
    "        \n",
    "    pipeline.fit(x_train, y_train)\n",
    "    return pipeline\n",
    "\n",
    "def get_names(x, pipeline):\n",
    "    features = pipeline.named_steps['feature_selection']\n",
    "    names = x.columns[features.get_support(indices=True)]\n",
    "    return names\n",
    "\n",
    "def get_pvalues(pipeline, x):\n",
    "    features = pipeline.named_steps['feature_selection']\n",
    "    pvalues = features.pvalues_\n",
    "    dfpvalues = pd.DataFrame(features.pvalues_)\n",
    "    dfscores = pd.DataFrame(features.scores_)\n",
    "    dfcolumns = pd.DataFrame(x.columns)\n",
    "    # concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores, dfpvalues],axis=1)\n",
    "    featureScores.columns = ['Specs','Score', 'P-Value']\n",
    "    # plot\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.hist(pvalues)\n",
    "    plt.show()\n",
    "    return featureScores\n",
    "\n",
    "def scores(y_test, y_pred, presentationtype):\n",
    "    if presentationtype == \"scores\":\n",
    "        accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "        precision=metrics.precision_score(y_test, y_pred)\n",
    "        recall=metrics.recall_score(y_test, y_pred)\n",
    "        f_one=metrics.f1_score(y_test, y_pred)\n",
    "        return accuracy, precision, recall, f_one\n",
    "    if presentationtype == \"report\":\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        return report\n",
    "    \n",
    "def score_plot(logreg, y_test, x_test):\n",
    "    lr_probs = logreg.predict_proba(x_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # predict class values\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "    # plot the precision-recall curves\n",
    "    no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return lr_precision, lr_recall\n",
    "\n",
    "def create_cnfmatrix(y_test, y_pred, plotting=True):\n",
    "    cnfpipe_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n",
    "    sumpositive = tp + fn\n",
    "    sumnegative = fp + tn\n",
    "    sumcorrect = tp + tn\n",
    "    sumwrong = fp + fn\n",
    "    sumall = tn+fp+fn+tp\n",
    "    print(\"TN, FP, FN, TP: \", tn, fp, fn, tp, \"\\nSum: \", sumall, \"\\nSum correct predictions: \", \n",
    "          sumcorrect, \"Percent: \", sumcorrect/sumall, \"\\nSum wrong predictions: \", sumwrong, \"\\tPercent: \",\n",
    "          sumwrong/sumall, \"\\nSum actual positives: \", sumpositive, \"\\tPercent: \", sumpositive/sumall,\n",
    "          \"\\nSum actual negatives: \", sumnegative, \"\\tPercent: \", sumnegative/sumall)\n",
    "    \n",
    "    if plotting:\n",
    "        %matplotlib inline\n",
    "        class_names=[0,1] # name  of classes\n",
    "        fig, ax = plt.subplots()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "        # create heatmap\n",
    "        sns.heatmap(pd.DataFrame(cnfpipe_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "        ax.xaxis.set_label_position(\"bottom\")\n",
    "        plt.tight_layout()\n",
    "        plt.title('Confusion matrix', y=1.1)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for classifier\n",
    "\n",
    "def classify(df, trait_name, startnumber, plotting=True):\n",
    "    print(\"Trait to predict: \", trait_name)\n",
    "    x,y = trait(df, trait_name, startnumber)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "    print(\"Number of authors in y_train: \", len(y_train))\n",
    "    print(\"Number of authors in y_test: \", len(y_test))\n",
    "    logpipe = create_pipeline(x_train, y_train, 'log')\n",
    "    y_pred=logpipe.predict(x_test)\n",
    "    print(\"Number of authors in y_pred: \", len(y_pred))\n",
    "    names = get_names(x, logpipe)\n",
    "    print(\"Names of the top\", len(names), \"features: \\n\", names, \"\\n\")\n",
    "    pvalues = get_pvalues(logpipe, x)\n",
    "    print(\"\\nP-Values: \")\n",
    "    print(pvalues.nsmallest(30,'P-Value'))\n",
    "    print(\"\\n\")\n",
    "    cnfmatrix = create_cnfmatrix(y_test, y_pred, plotting=True) \n",
    "#     accuracy, precision, recall, f_one = scores(y_test, y_pred, \"scores\")\n",
    "#     print(\"Scores:\\nAccuracy:\",accuracy, \"\\nPrecision:\",precision, \"\\nRecall:\",recall, \"\\nF1 score:\",f_one)\n",
    "    report = scores(y_test, y_pred, \"report\")\n",
    "    print(\"Classification report: \\n\", report)\n",
    "    lr_precision, lr_recall = score_plot(logpipe, y_test, x_test)\n",
    "    print(\"\\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psychdf.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 18\n",
    "print (\"Number of authors: \", len(psychdf))\n",
    "\n",
    "# personality prediction on test set\n",
    "all_hist_true(psychdf)\n",
    "classify(psychdf, \"agree\", start, plotting=True)\n",
    "classify(psychdf, \"openn\", start, plotting=True)\n",
    "classify(psychdf, \"consc\", start, plotting=True)\n",
    "classify(psychdf, \"extra\", start, plotting=True)\n",
    "classify(psychdf, \"neuro\", start, plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for the train set\n",
    "\n",
    "def classify_trainset(df, trait_name, startnumber, plotting=True):\n",
    "    print(\"Trait to predict: \", trait_name)\n",
    "    x,y = trait(df, trait_name, startnumber)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "    logpipe = create_pipeline(x_train, y_train, 'log')\n",
    "    y_pred=logpipe.predict(x_train)\n",
    "    print(\"Number of authors in y_pred: \", len(y_pred))\n",
    "    names = get_names(x, logpipe)\n",
    "    print(\"Names of the top\", len(names), \"features: \\n\", names, \"\\n\")\n",
    "    pvalues = get_pvalues(logpipe, x)\n",
    "#     print(\"p-values of\", len(pvalues), \"features: \\n\", pvalues, \"\\n\")\n",
    "    print(\"\\nP-Values: \")\n",
    "    print(pvalues.nsmallest(30,'P-Value'))\n",
    "    print(\"\\n\")\n",
    "    cnfmatrix = create_cnfmatrix(y_train, y_pred, plotting=True) \n",
    "#     accuracy, precision, recall, f_one = scores(y_test, y_pred, \"scores\")\n",
    "#     print(\"Scores:\\nAccuracy:\",accuracy, \"\\nPrecision:\",precision, \"\\nRecall:\",recall, \"\\nF1 score:\",f_one)\n",
    "    report = scores(y_train, y_pred, \"report\")\n",
    "    print(\"Classification report: \\n\", report)\n",
    "    lr_precision, lr_recall = score_plot(logpipe, y_train, x_train)\n",
    "#     print(\"Scores:\\nLR_Precision:\",lr_precision, \"\\nLR_Recall:\",lr_recall)\n",
    "    plt.show()\n",
    "    print(\"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_trainset(psychdf, \"agree\", start, plotting=True)\n",
    "classify_trainset(psychdf, \"openn\", start, plotting=True)\n",
    "classify_trainset(psychdf, \"consc\", start, plotting=True)\n",
    "classify_trainset(psychdf, \"extra\", start, plotting=True)\n",
    "classify_trainset(psychdf, \"neuro\", start, plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
