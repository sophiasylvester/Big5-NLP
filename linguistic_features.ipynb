{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "random.seed(32)\n",
    "from lexicalrichness import LexicalRichness\n",
    "import textblob\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams, ngrams\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data (preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87 entries, 0 to 86\n",
      "Data columns (total 31 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   index                 87 non-null     int64  \n",
      " 1   author                87 non-null     object \n",
      " 2   body                  87 non-null     object \n",
      " 3   probody               87 non-null     object \n",
      " 4   tokens                87 non-null     object \n",
      " 5   senttokens            87 non-null     object \n",
      " 6   agreeableness         87 non-null     float64\n",
      " 7   openness              87 non-null     float64\n",
      " 8   conscientiousness     87 non-null     float64\n",
      " 9   extraversion          87 non-null     float64\n",
      " 10  neuroticism           87 non-null     float64\n",
      " 11  agree                 87 non-null     int64  \n",
      " 12  openn                 87 non-null     int64  \n",
      " 13  consc                 87 non-null     int64  \n",
      " 14  extra                 87 non-null     int64  \n",
      " 15  neuro                 87 non-null     int64  \n",
      " 16  language              87 non-null     int64  \n",
      " 17  author_flair_text     87 non-null     object \n",
      " 18  downs                 87 non-null     float64\n",
      " 19  created_utc           87 non-null     float64\n",
      " 20  subreddit_id          87 non-null     object \n",
      " 21  link_id               87 non-null     object \n",
      " 22  parent_id             87 non-null     object \n",
      " 23  score                 87 non-null     float64\n",
      " 24  controversiality      87 non-null     float64\n",
      " 25  gilded                87 non-null     float64\n",
      " 26  id                    87 non-null     object \n",
      " 27  subreddit             87 non-null     object \n",
      " 28  ups                   87 non-null     float64\n",
      " 29  word_count            87 non-null     float64\n",
      " 30  word_count_quoteless  87 non-null     float64\n",
      "dtypes: float64(13), int64(7), object(11)\n",
      "memory usage: 21.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"preprocessed.pkl\")\n",
    "df.head()\n",
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features not mentioned in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(workdata):\n",
    "\n",
    "    # Total number of characters (including space)\n",
    "    workdata['char_count'] = workdata['body'].str.len()\n",
    "\n",
    "    # Total number of stopwords\n",
    "    stopwordList = stopwords.words('english')\n",
    "    workdata['stopwords'] = workdata['body'].apply(lambda x: len([x for x in x.split() if x in stopwordList]))\n",
    "\n",
    "    # Total number of punctuation or special characters\n",
    "    workdata['total_punc'] = workdata['body'].apply(lambda x: len([x for x in x.split() for j in x if j in string.punctuation]))\n",
    "\n",
    "    # Total number of numerics\n",
    "    workdata['total_num'] = workdata['body'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "    # Total number of uppercase words\n",
    "    workdata['total_uppercase'] = workdata['body'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "    \n",
    "    return workdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type-Token Ratio (ttr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typetokenratio(df):\n",
    "    ratiolst = []\n",
    "    for text in df['body']:\n",
    "        lex = LexicalRichness(text)\n",
    "        ratio = lex.ttr\n",
    "        ratiolst.append(ratio)\n",
    "    df['ttr'] = ratiolst\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words per sentence\n",
    "\n",
    "def wordcounter(df):\n",
    "    lengthscore = []\n",
    "    for row in df['senttokens']:\n",
    "        tempscore = []\n",
    "        for sentence in row:\n",
    "            length = len(sentence.split())\n",
    "            tempscore.append(length)\n",
    "        score = sum(tempscore)\n",
    "        lengthscore.append(score)\n",
    "    df['words_per_sent'] = lengthscore\n",
    "    return df\n",
    "\n",
    "# words longer than six characters\n",
    "\n",
    "def charcounter(df):\n",
    "    charscore = []\n",
    "    for row in df['tokens']:\n",
    "        lenrow = len(row)\n",
    "        if lenrow == 0:\n",
    "            score = 0\n",
    "        else:\n",
    "            number = 0\n",
    "            for token in row:\n",
    "                length = len(token)\n",
    "                if length > 5:\n",
    "                    number+=1\n",
    "            score = number/lenrow\n",
    "        charscore.append(score)\n",
    "    df['wordslongersix'] = charscore\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagger\n",
    "\n",
    "def tagging(df):\n",
    "    past = [] #VPA\n",
    "    presence = [] #VPR\n",
    "    adverbs = [] #RB\n",
    "    prepositions = [] #PREP\n",
    "    pronouns = [] #PR\n",
    "    for text in df['tokens']:\n",
    "        tags = nltk.pos_tag(text)\n",
    "        counts = Counter(tag for word,tag in tags)\n",
    "        total = sum(counts.values())\n",
    "        pron = counts['PRP'] + counts['PRP$']\n",
    "        verbspr = counts['VB'] + counts['VBG'] + counts['VBP'] + counts['VBZ'] + counts['MD']\n",
    "        verbspa = counts['VBD'] + counts['VBN']\n",
    "        preps = counts['IN'] + counts['TO']\n",
    "        counts['PR'] = pron\n",
    "        counts['PREP'] = preps\n",
    "        counts['VPR'] = verbspr #present tense\n",
    "        counts['VPA'] = verbspa #past tense\n",
    "        if total == 0:\n",
    "            allcounts = dict((word, float(count)/1) for word,count in counts.items())\n",
    "        else:\n",
    "            allcounts = dict((word, float(count)/total) for word,count in counts.items())\n",
    "        try:\n",
    "            past.append(allcounts['VPA'])\n",
    "        except KeyError:\n",
    "            past.append(0)\n",
    "        try:\n",
    "            presence.append(allcounts['VPR'])\n",
    "        except KeyError:\n",
    "            presence.append(0)\n",
    "        try:\n",
    "            adverbs.append(allcounts['RB'])\n",
    "        except KeyError:\n",
    "            adverbs.append(0)\n",
    "        try:\n",
    "            prepositions.append(allcounts['PREP'])\n",
    "        except KeyError:\n",
    "            prepositions.append(0)\n",
    "        try:\n",
    "            pronouns.append(allcounts['PR'])\n",
    "        except KeyError:\n",
    "            pronouns.append(0)\n",
    "    df['pasttense'] = past\n",
    "    df['presencetense'] = presence\n",
    "    df['adverbs'] = adverbs\n",
    "    df['prepositions'] = prepositions\n",
    "    df['pronouns'] = pronouns\n",
    "    return df\n",
    "\n",
    "# nltk.help.upenn_tagset('RB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(df, n_min, n_max, ngramtype):\n",
    "    # convert input from list to string\n",
    "    ngrams = []\n",
    "    inputtext = []\n",
    "    for sentence in df['tokens']:\n",
    "        text = ' '.join(sentence)\n",
    "        inputtext.append(text)\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n_min,n_max), analyzer=ngramtype)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    print(\"Get feature names...\")\n",
    "    names = vectorizer.get_feature_names()\n",
    "    print(\"Create df...\")\n",
    "    ngramdf = pd.DataFrame(denselist, columns=names)\n",
    "    ngramdf['author'] = df['author']\n",
    "    print(\"Done\")\n",
    "#     newdf = pd.merge(df, ngramdf, on='author', how='outer')\n",
    "#     ngramdict = ngramdf.to_dict('index')\n",
    "#     dict_items = list(ngramdict.items())    \n",
    "    return ngramdf\n",
    "\n",
    "# stemmeddf['wordngrams'] = ngrams(stemmeddf, 1, 3, 'word')\n",
    "# stemmeddf['charngrams'] = ngrams(stemmeddf, 2, 3, 'char')\n",
    "# stemmeddf.head()\n",
    "\n",
    "# wordngramsdf = ngrams(stemmeddf, 1, 3, 'word')\n",
    "# print(\"NaN in wordngramsdf: \", wordngramsdf.isnull().any().any())\n",
    "# charngramsdf = ngrams(stemmeddf, 2, 3, 'char')\n",
    "# print(\"NaN in charngramsdf: \", charngramsdf.isnull().any().any())\n",
    "\n",
    "\n",
    "def merge_dfs(df1, df2, df3):\n",
    "    cwngramsdf = pd.merge(df1, df2, on='author', how='outer')\n",
    "    gramsdf = pd.merge(df3, cwngramsdf, on='author', how='outer')\n",
    "    return gramsdf\n",
    "\n",
    "# gramsdf = merge_dfs(wordngramsdf, charngramsdf, stemmeddf)\n",
    "# gramsdf.head()\n",
    "# cwngramsdf = ngrams(wordngramsdf, 2, 3, 'char')\n",
    "# print(cwngramsdf.isnull().any().any())\n",
    "# cwngramsdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lin_features(df):\n",
    "    df = create_features(df)\n",
    "    df = typetokenratio(df)\n",
    "    df = wordcounter(df)\n",
    "    df = charcounter(df)\n",
    "    df = tagging(df)\n",
    "    return df\n",
    "\n",
    "lin_feat_df = extract_lin_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_feat_df.to_pickle(\"linguistic_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657acf1a1f78444eb61e5993a27e054f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get feature names...\n",
      "Create df...\n",
      "Done\n",
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4847d0f855e441ef87d37680601b85c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get feature names...\n",
      "Create df...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def extract_lin_ngrams(df):\n",
    "    df = create_features(df)\n",
    "    df = typetokenratio(df)\n",
    "    df = wordcounter(df)\n",
    "    df = charcounter(df)\n",
    "    df = tagging(df)\n",
    "    wordngramsdf = ngrams(df, 1, 3, 'word')\n",
    "    charngramsdf = ngrams(df, 2, 3, 'char')\n",
    "    gramsdf = merge_dfs(wordngramsdf, charngramsdf, df)\n",
    "    return gramsdf\n",
    "\n",
    "lin_ngrams_df = extract_lin_ngrams(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_ngrams_df.to_pickle(\"linguistic_ngrams.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d890ad2bf76f4b9ab0b4bc362eb5c82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get feature names...\n",
      "Create df...\n",
      "Done\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "def extract_lin_wordngrams(df):\n",
    "    df = create_features(df)\n",
    "    df = typetokenratio(df)\n",
    "    df = wordcounter(df)\n",
    "    df = charcounter(df)\n",
    "    df = tagging(df)\n",
    "    wordngrams = ngrams(df, 1, 3, 'word')\n",
    "    wordngramsdf = pd.DataFrame(wordngrams)\n",
    "    gramsdf = pd.merge(df, wordngramsdf, on='author', how='outer')\n",
    "    return gramsdf\n",
    "\n",
    "lin_wordngrams_df = extract_lin_wordngrams(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_wordngrams_df.to_pickle(\"linguistic_wordngrams.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
