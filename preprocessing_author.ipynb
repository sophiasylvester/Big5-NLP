{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Version with one row per author:\n",
    "\n",
    "### Features (as in paper):\n",
    "1. 11.140 ngram features: tf and tf-idf weighted word and character ngrams stemmed with Porter's stemmer\n",
    "2. type-token ratio\n",
    "3. ratio of comments in English\n",
    "4. ratio of British english vs. American English words\n",
    "5. 93 features from LIWC \n",
    "6. 26 PSYCH features (Preotiuc: Paraphrase Database and MRC Psycholinguistics Database)\n",
    "\n",
    "### Columns (from the description of the dataset):\n",
    "1. 'global':[7,10], #subreddits_commented, subreddits_commented_mbti, num_comments\n",
    "2. 'liwc':[10,103], #liwc\n",
    "3. 'word':[103,3938], #top1000 word ngram (1,2,3) per dimension based on chi2\n",
    "4. 'char':[3938,7243], #top1000 char ngrams (2,3) per dimension based on chi2\n",
    "5. 'sub':[7243,12228], #number of comments in each subreddit\n",
    "6. 'ent':[12228,12229], #entropy\n",
    "7. 'subtf':[12229,17214], #tf-idf on subreddits\n",
    "8. 'subcat':[17214,17249], #manually crafted subreddit categories\n",
    "9. 'lda50':[17249,17299], #50 LDA topics\n",
    "10. 'posts':[17299,17319], #posts statistics\n",
    "11. 'lda100':[17319,17419], #100 LDA topics\n",
    "12. 'psy':[17419,17443], #psycholinguistic features\n",
    "13. 'en':[17443,17444], #ratio of english comments\n",
    "14. 'ttr':[17444,17445], #type token ratio\n",
    "15. 'meaning':[17445,17447], #additional pyscholinguistic features\n",
    "16. 'time_diffs':[17447,17453], #commenting time diffs\n",
    "17. 'month':[17453,17465], #monthly distribution\n",
    "18. 'hour':[17465,17489], #hourly distribution\n",
    "19. 'day_of_week':[17489,17496], #daily distribution\n",
    "20. 'word_an':[17496,21496], #word ngrams selected by F-score\n",
    "21. 'word_an_tf':[21496,25496], #tf-idf ngrams selected by F-score\n",
    "22. 'char_an':[25496,29496], #char ngrams selected by F-score\n",
    "23. 'char_an_tf':[29496,33496], #tf-idf char ngrams selected by F-score\n",
    "24. 'brit_amer':[33496,33499], #british vs american english ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sophia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import bigrams, ngrams\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from collections import Counter\n",
    "from num2words import num2words \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import random\n",
    "random.seed(32)\n",
    "\n",
    "# close nltk download window to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_quoteless</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sabata11792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not seeing any break or signal lights and no p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1534890968</td>\n",
       "      <td>t5_3fqup</td>\n",
       "      <td>t3_995l9s</td>\n",
       "      <td>t1_e4lbrls</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>e4lkg2l</td>\n",
       "      <td>ATBGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Swarels</td>\n",
       "      <td>INTP</td>\n",
       "      <td>Multiverses, matrix theory, consciousness. Scr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1499749893</td>\n",
       "      <td>t5_2qhvl</td>\n",
       "      <td>t3_6mjw62</td>\n",
       "      <td>t1_dk26jre</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dk26vpo</td>\n",
       "      <td>INTP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>46</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pearlz176</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>Hope you've enjoyed the ride :D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1485613795</td>\n",
       "      <td>t5_2qi58</td>\n",
       "      <td>t3_5qnd1v</td>\n",
       "      <td>t1_dd0mdqg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dd0nxif</td>\n",
       "      <td>soccer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rainbowhotpocket</td>\n",
       "      <td>Colts</td>\n",
       "      <td>Idk, in the AFC if i recall correctly since 20...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1466965660</td>\n",
       "      <td>t5_2qmg3</td>\n",
       "      <td>t3_4pypuh</td>\n",
       "      <td>t1_d4oulvo</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d4our0i</td>\n",
       "      <td>nfl</td>\n",
       "      <td>11.0</td>\n",
       "      <td>62</td>\n",
       "      <td>61</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amathyx</td>\n",
       "      <td>http://myanimelist.net/profile/amathy</td>\n",
       "      <td>22 hours later and the music is still going[co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1495851189</td>\n",
       "      <td>t5_2qh22</td>\n",
       "      <td>t3_6ddiow</td>\n",
       "      <td>t3_6ddiow</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>di3inz7</td>\n",
       "      <td>anime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                      author_flair_text  \\\n",
       "0       Sabata11792                                    NaN   \n",
       "1           Swarels                                   INTP   \n",
       "2         pearlz176                      Manchester United   \n",
       "3  rainbowhotpocket                                  Colts   \n",
       "4           amathyx  http://myanimelist.net/profile/amathy   \n",
       "\n",
       "                                                body  downs  created_utc  \\\n",
       "0  Not seeing any break or signal lights and no p...    NaN   1534890968   \n",
       "1  Multiverses, matrix theory, consciousness. Scr...    NaN   1499749893   \n",
       "2                    Hope you've enjoyed the ride :D    NaN   1485613795   \n",
       "3  Idk, in the AFC if i recall correctly since 20...    NaN   1466965660   \n",
       "4  22 hours later and the music is still going[co...    NaN   1495851189   \n",
       "\n",
       "  subreddit_id    link_id   parent_id  score  controversiality  gilded  \\\n",
       "0     t5_3fqup  t3_995l9s  t1_e4lbrls    1.0                 0       0   \n",
       "1     t5_2qhvl  t3_6mjw62  t1_dk26jre    7.0                 0       0   \n",
       "2     t5_2qi58  t3_5qnd1v  t1_dd0mdqg    2.0                 0       0   \n",
       "3     t5_2qmg3  t3_4pypuh  t1_d4oulvo   11.0                 0       0   \n",
       "4     t5_2qh22  t3_6ddiow   t3_6ddiow    6.0                 0       0   \n",
       "\n",
       "        id subreddit   ups  word_count  word_count_quoteless lang  \n",
       "0  e4lkg2l     ATBGE   NaN          19                    19   en  \n",
       "1  dk26vpo      INTP   NaN          51                    46   en  \n",
       "2  dd0nxif    soccer   NaN           6                     6   en  \n",
       "3  d4our0i       nfl  11.0          62                    61   en  \n",
       "4  di3inz7     anime   NaN          32                    29   en  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/sophia/ma_py/pandora_bigfive1000.csv')\n",
    "# print(pandora.info(verbose=True))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change language to numeric representation\n",
    "def adjust(df):\n",
    "    # change lang to numerical representation\n",
    "    language = df['lang'].values.tolist()\n",
    "    language = set(language)\n",
    "    language\n",
    "    df['language']= np.select([df.lang == 'en', df.lang == 'es', df.lang == 'nl'], \n",
    "                            [0, 1, 2], \n",
    "                            default=3)\n",
    "    # print(gramsdf['language'])\n",
    "    df = df.drop(columns=['lang'])\n",
    "\n",
    "    return df\n",
    "\n",
    "df = adjust(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timecolumns(df):\n",
    "    readable = []\n",
    "    weekday = []\n",
    "    month = []\n",
    "    year = []\n",
    "    for row in df['created_utc']:\n",
    "        item = datetime.datetime.fromtimestamp(row)\n",
    "        weekday_item = item.strftime('%A')\n",
    "        readable_item = datetime.datetime.fromtimestamp(row).isoformat()\n",
    "        month.append(str(readable_item[5:7]))\n",
    "        year.append(str(readable_item[0:4]))\n",
    "        readable.append(readable_item)\n",
    "        weekday.append(weekday_item)\n",
    "    df['time'] = readable\n",
    "    df['weekday'] = weekday\n",
    "    df['month'] = month\n",
    "    df['year'] = year\n",
    "    return df\n",
    "\n",
    "# pandora = create_timecolumns(pandora)\n",
    "# pandora.head()\n",
    "# test = pandora.iloc[0]['time']\n",
    "# print(test)\n",
    "# print(test[0:4])\n",
    "# lst = pandora.weekday.tolist()\n",
    "# lstset = set(lst)\n",
    "# print(lstset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timecounter(lst, vocablst):\n",
    "    if vocablst == 'weekday':\n",
    "        vocab = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    elif vocablst == 'month':\n",
    "        vocab = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    elif vocablst == 'year':\n",
    "        vocab = ['2015', '2016', '2017', '2018', '2019']\n",
    "    else:\n",
    "        print(\"No valid input: vocab list\")\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", vocabulary=vocab)\n",
    "    vectors = vectorizer.fit_transform(lst)\n",
    "    v = vectors.toarray()\n",
    "#     is_all_zero = np.all((v == 0))\n",
    "#     names = vectorizer.get_feature_names()\n",
    "    return v\n",
    "\n",
    "# item = ['Sunday Tuesday']\n",
    "# print(item)\n",
    "# test = timecounter(item, 'weekday')\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subredditcounter(df, lst):\n",
    "    lst = df['subreddit'].tolist()\n",
    "    subredditset = set(lst)\n",
    "    subredditlist = list(subredditset)\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", vocabulary=subredditlist)\n",
    "    vectors = vectorizer.fit_transform(lst)\n",
    "    v = vectors.toarray()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>complete_body</th>\n",
       "      <th>all_utc</th>\n",
       "      <th>mean_controversiality</th>\n",
       "      <th>mean_gilded</th>\n",
       "      <th>num_subreddits</th>\n",
       "      <th>subreddit_dist</th>\n",
       "      <th>weekday_dist</th>\n",
       "      <th>month_dist</th>\n",
       "      <th>year_dist</th>\n",
       "      <th>all_lang</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>-BigSexy-</td>\n",
       "      <td>Oooh i see</td>\n",
       "      <td>[1510236798]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>Oooh i see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-BlitzN9ne</td>\n",
       "      <td>**Quality** material right here</td>\n",
       "      <td>[1549708109]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>**Quality** material right here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>-CrestiaBell</td>\n",
       "      <td>A slidewhistle or a meow-meow board That's bec...</td>\n",
       "      <td>[1538664591, 1475867279, 1505862626, 151267621...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2]</td>\n",
       "      <td>[0, 2, 3, 1, 1]</td>\n",
       "      <td>0 0 0 0 0 0 0</td>\n",
       "      <td>A slidewhistle or a meow-meow board That's bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>-tactical-throw-away</td>\n",
       "      <td>Sorry for your feelings. Kek &amp;lt;------- This ...</td>\n",
       "      <td>[1498536785, 1486701409, 1506834463]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 3, 0, 0]</td>\n",
       "      <td>0 0 0</td>\n",
       "      <td>Sorry for your feelings. Kek &amp;lt;------- This ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>137288</td>\n",
       "      <td>Carly's so glad to get your .0000003 cents Exc...</td>\n",
       "      <td>[1536611153, 1550537879, 1516548513, 1523299682]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 3, 1]</td>\n",
       "      <td>0 0 0 0</td>\n",
       "      <td>Carly's so glad to get your .0000003 cents Exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>xanthraxoid</td>\n",
       "      <td>I'd really like this video to include some inf...</td>\n",
       "      <td>[1469892161, 1486826547, 1498046590, 1550346594]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 0, 1]</td>\n",
       "      <td>0 0 3 0</td>\n",
       "      <td>I'd really like this video to include some inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>xenomouse</td>\n",
       "      <td>You're a guy, aren't you? I can definitely see...</td>\n",
       "      <td>[1506710219, 1502740906, 1517847908, 1506874589]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 3, 1, 0]</td>\n",
       "      <td>0 0 0 0</td>\n",
       "      <td>You're a guy, aren't you? I can definitely see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>xeroctr3</td>\n",
       "      <td>man even the thought of it makes me depressed....</td>\n",
       "      <td>[1521414051]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>man even the thought of it makes me depressed....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>xzack18</td>\n",
       "      <td>Not all of us are out to kill</td>\n",
       "      <td>[1533749569]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>Not all of us are out to kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>zugzwang_03</td>\n",
       "      <td>Institutions should accommodate religious or s...</td>\n",
       "      <td>[1514216199, 1459000262, 1500701643, 151759545...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1]</td>\n",
       "      <td>[0, 3, 2, 1, 0]</td>\n",
       "      <td>0 0 0 0 0 0</td>\n",
       "      <td>Institutions should accommodate religious or s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>429 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author                                      complete_body  \\\n",
       "906             -BigSexy-                                         Oooh i see   \n",
       "145            -BlitzN9ne                    **Quality** material right here   \n",
       "367          -CrestiaBell  A slidewhistle or a meow-meow board That's bec...   \n",
       "295  -tactical-throw-away  Sorry for your feelings. Kek &lt;------- This ...   \n",
       "791                137288  Carly's so glad to get your .0000003 cents Exc...   \n",
       "..                    ...                                                ...   \n",
       "324           xanthraxoid  I'd really like this video to include some inf...   \n",
       "954             xenomouse  You're a guy, aren't you? I can definitely see...   \n",
       "208              xeroctr3  man even the thought of it makes me depressed....   \n",
       "990               xzack18                      Not all of us are out to kill   \n",
       "936           zugzwang_03  Institutions should accommodate religious or s...   \n",
       "\n",
       "                                               all_utc  mean_controversiality  \\\n",
       "906                                       [1510236798]                    0.0   \n",
       "145                                       [1549708109]                    0.0   \n",
       "367  [1538664591, 1475867279, 1505862626, 151267621...                    0.0   \n",
       "295               [1498536785, 1486701409, 1506834463]                    0.0   \n",
       "791   [1536611153, 1550537879, 1516548513, 1523299682]                    0.0   \n",
       "..                                                 ...                    ...   \n",
       "324   [1469892161, 1486826547, 1498046590, 1550346594]                    0.0   \n",
       "954   [1506710219, 1502740906, 1517847908, 1506874589]                    0.0   \n",
       "208                                       [1521414051]                    0.0   \n",
       "990                                       [1533749569]                    0.0   \n",
       "936  [1514216199, 1459000262, 1500701643, 151759545...                    0.0   \n",
       "\n",
       "     mean_gilded  num_subreddits  \\\n",
       "906          0.0               1   \n",
       "145          0.0               1   \n",
       "367          0.0               4   \n",
       "295          0.0               1   \n",
       "791          0.0               1   \n",
       "..           ...             ...   \n",
       "324          0.0               4   \n",
       "954          0.0               2   \n",
       "208          0.0               1   \n",
       "990          0.0               1   \n",
       "936          0.0               5   \n",
       "\n",
       "                                        subreddit_dist           weekday_dist  \\\n",
       "906  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "145  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "367  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "295  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "791  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "..                                                 ...                    ...   \n",
       "324  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "954  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "208  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "990  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "936  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                               month_dist        year_dist       all_lang  \\\n",
       "906  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  [0, 0, 1, 0, 0]              0   \n",
       "145  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 1]              0   \n",
       "367  [1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2]  [0, 2, 3, 1, 1]  0 0 0 0 0 0 0   \n",
       "295  [0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]  [0, 0, 3, 0, 0]          0 0 0   \n",
       "791  [1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]  [0, 0, 0, 3, 1]        0 0 0 0   \n",
       "..                                    ...              ...            ...   \n",
       "324  [0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]  [0, 1, 2, 0, 1]        0 0 3 0   \n",
       "954  [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]  [0, 0, 3, 1, 0]        0 0 0 0   \n",
       "208  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 1, 0]              0   \n",
       "990  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  [0, 0, 0, 1, 0]              0   \n",
       "936  [0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1]  [0, 3, 2, 1, 0]    0 0 0 0 0 0   \n",
       "\n",
       "                                                  body  \n",
       "906                                         Oooh i see  \n",
       "145                    **Quality** material right here  \n",
       "367  A slidewhistle or a meow-meow board That's bec...  \n",
       "295  Sorry for your feelings. Kek &lt;------- This ...  \n",
       "791  Carly's so glad to get your .0000003 cents Exc...  \n",
       "..                                                 ...  \n",
       "324  I'd really like this video to include some inf...  \n",
       "954  You're a guy, aren't you? I can definitely see...  \n",
       "208  man even the thought of it makes me depressed....  \n",
       "990                      Not all of us are out to kill  \n",
       "936  Institutions should accommodate religious or s...  \n",
       "\n",
       "[429 rows x 12 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_authordf(df):    \n",
    "    # body\n",
    "    df['complete_body'] = df.groupby(['author'])['body'].transform(lambda x : ' '. join(x))\n",
    "    # language\n",
    "    df['lang'] = df['language'].apply(lambda x: str(x))\n",
    "    df['all_lang'] = df.groupby(['author'])['lang'].transform(lambda x : ' '. join(x))\n",
    "    # created_utc\n",
    "    df['utc_lst'] = df['created_utc'].apply(lambda x: str(x))\n",
    "    df['all_utc'] = df.groupby(['author'])['utc_lst'].transform(lambda x : ' '. join(x))\n",
    "    df['all_utc'] = df['all_utc'].apply(lambda x: x.split())\n",
    "    # controversiality\n",
    "    df['mean_controversiality'] = df.groupby(['author']).agg({'controversiality': ['mean']})\n",
    "    df['mean_controversiality'] = df['mean_controversiality'].fillna(0)\n",
    "    # gilded\n",
    "    df['mean_gilded'] = df.groupby(['author']).agg({'gilded': ['mean']})\n",
    "    df['mean_gilded'] = df['mean_gilded'].fillna(0)\n",
    "    # number of subreddits\n",
    "    df['num_subreddits'] = df.groupby(['author'])['subreddit'].transform(lambda x : ' '. join(x))\n",
    "    df['num_subreddits'] = df['num_subreddits'].apply(lambda x: len(set(x.split())))\n",
    "    # number of comments per subreddit\n",
    "    df['subreddit_dist'] = df.groupby(['author'])['subreddit'].transform(lambda x : ' '. join(x))\n",
    "    subreddit = subredditcounter(df, df['subreddit_dist'])\n",
    "    df['subreddit_dist'] = subreddit.tolist()\n",
    "    # time\n",
    "    df = create_timecolumns(df)\n",
    "    df['weekday_dist'] = df.groupby(['author'])['weekday'].transform(lambda x : ' '. join(x))\n",
    "    weekday = timecounter(df['weekday_dist'], 'weekday')\n",
    "    df['weekday_dist'] = weekday.tolist()\n",
    "    df['month_dist'] = df.groupby(['author'])['month'].transform(lambda x : ' '. join(x))\n",
    "    month = timecounter(df['month_dist'], 'month')\n",
    "    df['month_dist'] = month.tolist()\n",
    "    df['year_dist'] = df.groupby(['author'])['year'].transform(lambda x : ' '. join(x))\n",
    "    year = timecounter(df['year_dist'], 'year')\n",
    "    df['year_dist'] = year.tolist()\n",
    "    \n",
    "    newdf = df[['author', 'complete_body', 'all_utc', 'mean_controversiality', \n",
    "                'mean_gilded', 'num_subreddits', 'subreddit_dist', 'weekday_dist', \n",
    "                'month_dist', 'year_dist', 'all_lang']]\n",
    "    newdf = newdf.sort_values(by='author')\n",
    "    newdf = newdf.drop_duplicates(subset=['author'])\n",
    "    return newdf\n",
    "\n",
    "\n",
    "pandora = create_authordf(df)\n",
    "pandora\n",
    "pandora['body'] = pandora['complete_body']\n",
    "pandora\n",
    "# print(type(newdf.iloc[428]['weekday_dist']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author search:  True\n",
      "Author search:  True\n",
      "Author search:  True\n",
      "Length of dataframe:  422\n",
      "NaN in df?  False\n",
      "Sum of NaN in agreeableness 0\n",
      "Sum of NaN in openness 0\n",
      "Sum of NaN in conscientiousness 0\n",
      "Sum of NaN in extraversion 0\n",
      "Sum of NaN in neuroticism 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>complete_body</th>\n",
       "      <th>all_utc</th>\n",
       "      <th>mean_controversiality</th>\n",
       "      <th>mean_gilded</th>\n",
       "      <th>num_subreddits</th>\n",
       "      <th>subreddit_dist</th>\n",
       "      <th>weekday_dist</th>\n",
       "      <th>month_dist</th>\n",
       "      <th>year_dist</th>\n",
       "      <th>all_lang</th>\n",
       "      <th>body</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-BigSexy-</td>\n",
       "      <td>Oooh i see</td>\n",
       "      <td>[1510236798]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>Oooh i see</td>\n",
       "      <td>39.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-BlitzN9ne</td>\n",
       "      <td>**Quality** material right here</td>\n",
       "      <td>[1549708109]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>**Quality** material right here</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-CrestiaBell</td>\n",
       "      <td>A slidewhistle or a meow-meow board That's bec...</td>\n",
       "      <td>[1538664591, 1475867279, 1505862626, 151267621...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2]</td>\n",
       "      <td>[0, 2, 3, 1, 1]</td>\n",
       "      <td>0 0 0 0 0 0 0</td>\n",
       "      <td>A slidewhistle or a meow-meow board That's bec...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-tactical-throw-away</td>\n",
       "      <td>Sorry for your feelings. Kek &amp;lt;------- This ...</td>\n",
       "      <td>[1498536785, 1486701409, 1506834463]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 3, 0, 0]</td>\n",
       "      <td>0 0 0</td>\n",
       "      <td>Sorry for your feelings. Kek &amp;lt;------- This ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>137288</td>\n",
       "      <td>Carly's so glad to get your .0000003 cents Exc...</td>\n",
       "      <td>[1536611153, 1550537879, 1516548513, 1523299682]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 3, 1]</td>\n",
       "      <td>0 0 0 0</td>\n",
       "      <td>Carly's so glad to get your .0000003 cents Exc...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                author  \\\n",
       "0      0             -BigSexy-   \n",
       "1      1            -BlitzN9ne   \n",
       "2      2          -CrestiaBell   \n",
       "3      3  -tactical-throw-away   \n",
       "4      4                137288   \n",
       "\n",
       "                                       complete_body  \\\n",
       "0                                         Oooh i see   \n",
       "1                    **Quality** material right here   \n",
       "2  A slidewhistle or a meow-meow board That's bec...   \n",
       "3  Sorry for your feelings. Kek &lt;------- This ...   \n",
       "4  Carly's so glad to get your .0000003 cents Exc...   \n",
       "\n",
       "                                             all_utc  mean_controversiality  \\\n",
       "0                                       [1510236798]                    0.0   \n",
       "1                                       [1549708109]                    0.0   \n",
       "2  [1538664591, 1475867279, 1505862626, 151267621...                    0.0   \n",
       "3               [1498536785, 1486701409, 1506834463]                    0.0   \n",
       "4   [1536611153, 1550537879, 1516548513, 1523299682]                    0.0   \n",
       "\n",
       "   mean_gilded  num_subreddits  \\\n",
       "0          0.0               1   \n",
       "1          0.0               1   \n",
       "2          0.0               4   \n",
       "3          0.0               1   \n",
       "4          0.0               1   \n",
       "\n",
       "                                      subreddit_dist           weekday_dist  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                             month_dist        year_dist       all_lang  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  [0, 0, 1, 0, 0]              0   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 1]              0   \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2]  [0, 2, 3, 1, 1]  0 0 0 0 0 0 0   \n",
       "3  [0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]  [0, 0, 3, 0, 0]          0 0 0   \n",
       "4  [1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]  [0, 0, 0, 3, 1]        0 0 0 0   \n",
       "\n",
       "                                                body  agreeableness  openness  \\\n",
       "0                                         Oooh i see           39.0      92.0   \n",
       "1                    **Quality** material right here           50.0      85.0   \n",
       "2  A slidewhistle or a meow-meow board That's bec...           50.0      85.0   \n",
       "3  Sorry for your feelings. Kek &lt;------- This ...            2.0      92.0   \n",
       "4  Carly's so glad to get your .0000003 cents Exc...           10.0      87.0   \n",
       "\n",
       "   conscientiousness  extraversion  neuroticism  \n",
       "0                1.0          18.0          4.0  \n",
       "1               15.0          50.0         30.0  \n",
       "2               50.0          85.0         50.0  \n",
       "3               31.0          60.0         53.0  \n",
       "4               49.0           7.0         87.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv('/home/sophia/ma_py/author_profiles.csv')\n",
    "# find missing data in big five traits\n",
    "authorslst = authors['author'].tolist()\n",
    "print(\"Author search: \", 'DarthHedonist' in authorslst)\n",
    "print(\"Author search: \", 'FonsoTheWhitesican' in authorslst)\n",
    "print(\"Author search: \", 'chaosking121' in authorslst)\n",
    "\n",
    "bigfive = authors[['author','agreeableness','openness','conscientiousness','extraversion','neuroticism']]\n",
    "bigfive = bigfive.dropna()\n",
    "# print(bigfive[bigfive['author'] == \"DarthHedonist\"])\n",
    "\n",
    "# pandoradf = pd.merge(pandora, bigfive, how='left', on='author')\n",
    "pandoradf = pandora.merge(bigfive, how='left', on=['author'])\n",
    "# pandoradf = pandoradf.dropna()\n",
    "pandoradf = pandoradf.sort_values(by='author')\n",
    "pandoradf = pandoradf[pandoradf['agreeableness'].notna()]\n",
    "pandoradf = pandoradf.reset_index()\n",
    "\n",
    "\n",
    "print(\"Length of dataframe: \", len(pandoradf))\n",
    "print(\"NaN in df? \", pandoradf.isnull().any().any())\n",
    "print(\"Sum of NaN in agreeableness\", pandoradf['agreeableness'].isnull().values.sum())\n",
    "print(\"Sum of NaN in openness\", pandoradf['openness'].isnull().values.sum())\n",
    "print(\"Sum of NaN in conscientiousness\", pandoradf['conscientiousness'].isnull().values.sum())\n",
    "print(\"Sum of NaN in extraversion\", pandoradf['extraversion'].isnull().values.sum())\n",
    "print(\"Sum of NaN in neuroticism\", pandoradf['neuroticism'].isnull().values.sum())\n",
    "# nan_values = pandoradf[pandoradf['neuroticism'].isna()]\n",
    "# nan_values\n",
    "pandoradf.head()\n",
    "# pandoradf[pandoradf.isnull().any(axis=1)]\n",
    "\n",
    "# number of entries does not fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigfive_cat(df):\n",
    "    # change big five to binary representation\n",
    "    df['agree'] = df['agreeableness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['openn'] = df['openness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['consc'] = df['conscientiousness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['extra'] = df['extraversion'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['neuro'] = df['neuroticism'].apply(lambda x: 0 if x<50 else 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust representations of some columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_stopwordlist(df, mode):\n",
    "    if mode == 'NLTK':\n",
    "        stopwordList = stopwords.words('english')\n",
    "    if mode == 'NLTK-neg':\n",
    "        stopwordList = stopwords.words('english')\n",
    "        stopwordList.remove('no')\n",
    "        stopwordList.remove('nor')\n",
    "        stopwordList.remove('not')\n",
    "    return stopwordList\n",
    "\n",
    "# stopwordList = choose_stopwordlist(pandoradf, mode='NLTK-neg')\n",
    "\n",
    "# print(stopwordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "1. lower \n",
    "2. tokenize\n",
    "3. numbers to words\n",
    "4. delete special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# featuredf['probody'] = featuredf['body'].apply(lambda x:(decontracted(''.join(x))))\n",
    "# print(featuredf.iloc[5]['probody'])\n",
    "\n",
    "def senttokenize(df):\n",
    "    sentbody = []\n",
    "    for row in df['body']:\n",
    "        sentences = sent_tokenize(row)\n",
    "        sentbody.append(sentences)\n",
    "    df['senttokens'] = sentbody\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_stop_num_token(workdf, stopwordList):\n",
    "    # lower, remove special characters, remove stopwords\n",
    "    workdf['probody'] = workdf['probody'].apply(lambda x: ' '.join([x.lower() for x in x.split() if x.isalnum()]))\n",
    "    workdf['probody'] = workdf['probody'].apply(lambda x: ' '.join([x for x in x.split() if (x not in stopwordList)]))\n",
    "    newbody = []\n",
    "    newprobody = []\n",
    "    # num2words\n",
    "    for sentence in tqdm(workdf['probody']):\n",
    "        # string to list\n",
    "        inputtext = sentence.split()\n",
    "        numlist = []\n",
    "        for i in range(len(inputtext)):\n",
    "            if inputtext[i].isnumeric():\n",
    "                numlist.append(i)\n",
    "        for number in numlist:\n",
    "            inputtext[number] = num2words(inputtext[number])\n",
    "        \n",
    "        # list to string\n",
    "        celltext = ' '.join(inputtext)\n",
    "        newprobody.append(celltext)\n",
    "        # tokenize\n",
    "        words = word_tokenize(celltext)\n",
    "        newbody.append(words)\n",
    "    workdf['probody'] = newprobody\n",
    "    workdf['tokens'] = newbody\n",
    "    return workdf\n",
    "\n",
    "# preprocesseddf = preprocessing(featuredf)\n",
    "# print(preprocesseddf.iloc[2]['body'])\n",
    "# preprocesseddf.head()\n",
    "# preprocesseddf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "# Porter Stemmer\n",
    "def stemming(df):\n",
    "    ps = PorterStemmer()\n",
    "    df['tokens'] = df['tokens'].progress_apply(lambda x:([ps.stem(word) for word in x]))\n",
    "    return df\n",
    "\n",
    "# stemmeddf = stemming(preprocesseddf)\n",
    "# print(stemmeddf.iloc[1]['tokens'])\n",
    "# stemmeddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gramsdf.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordering(df):\n",
    "    lst = []\n",
    "    for i in range(len(df)):\n",
    "        lst.append(df.author[i] + str(i))\n",
    "    df['ident'] = lst\n",
    "    \n",
    "    cols_tomove = ['index', 'author', 'ident', 'body', 'probody', 'tokens', 'senttokens', 'agreeableness', 'openness', 'conscientiousness', 'extraversion', 'neuroticism', 'agree', 'openn', 'consc', 'extra', 'neuro']\n",
    "    orderdf  = df[cols_tomove + [col for col in df.columns if col not in cols_tomove]]\n",
    "#     orderdf.info(verbose=True)\n",
    "    return orderdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598fe0bd0f434a27a63e5fa91176492f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbb2869b8dc4d39a37dd878f5b0d11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 -BigSexy-0\n",
      "1                -BlitzN9ne1\n",
      "2              -CrestiaBell2\n",
      "3      -tactical-throw-away3\n",
      "4                    1372884\n",
      "               ...          \n",
      "417           xanthraxoid417\n",
      "418             xenomouse418\n",
      "419              xeroctr3419\n",
      "420               xzack18420\n",
      "421           zugzwang_03421\n",
      "Name: ident, Length: 422, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    # adjust some column representations\n",
    "    df = bigfive_cat(df)\n",
    "    # choose stopwordlist with or without negation\n",
    "    stopwordList = choose_stopwordlist(df, mode='NLTK-neg')\n",
    "    # decontract abbreviations (e.g., n't to not)\n",
    "    df['probody'] = df['body'].apply(lambda x:(decontracted(''.join(x))))\n",
    "    # create sentence tokens\n",
    "    df = senttokenize(df)\n",
    "    # lower, remove stopwords, num2words, tokenize\n",
    "    df = low_stop_num_token(df, stopwordList)\n",
    "    # porters stemmer\n",
    "    df = stemming(df)\n",
    "    df = ordering(df)\n",
    "    return df\n",
    "\n",
    "predf = preprocess(pandoradf)\n",
    "print(predf.ident)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 422 entries, 0 to 421\n",
      "Data columns (total 27 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   index                  422 non-null    int64  \n",
      " 1   author                 422 non-null    object \n",
      " 2   ident                  422 non-null    object \n",
      " 3   body                   422 non-null    object \n",
      " 4   probody                422 non-null    object \n",
      " 5   tokens                 422 non-null    object \n",
      " 6   senttokens             422 non-null    object \n",
      " 7   agreeableness          422 non-null    float64\n",
      " 8   openness               422 non-null    float64\n",
      " 9   conscientiousness      422 non-null    float64\n",
      " 10  extraversion           422 non-null    float64\n",
      " 11  neuroticism            422 non-null    float64\n",
      " 12  agree                  422 non-null    int64  \n",
      " 13  openn                  422 non-null    int64  \n",
      " 14  consc                  422 non-null    int64  \n",
      " 15  extra                  422 non-null    int64  \n",
      " 16  neuro                  422 non-null    int64  \n",
      " 17  complete_body          422 non-null    object \n",
      " 18  all_utc                422 non-null    object \n",
      " 19  mean_controversiality  422 non-null    float64\n",
      " 20  mean_gilded            422 non-null    float64\n",
      " 21  num_subreddits         422 non-null    int64  \n",
      " 22  subreddit_dist         422 non-null    object \n",
      " 23  weekday_dist           422 non-null    object \n",
      " 24  month_dist             422 non-null    object \n",
      " 25  year_dist              422 non-null    object \n",
      " 26  all_lang               422 non-null    object \n",
      "dtypes: float64(7), int64(7), object(13)\n",
      "memory usage: 89.1+ KB\n"
     ]
    }
   ],
   "source": [
    "predf.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               oooh see\n",
       "1                                         material right\n",
       "2      slidewhistle board watch cartoon school martin...\n",
       "3                   sorry kek onekek kek kek kek nothing\n",
       "4      carly glad get cents except uk debuted modern ...\n",
       "                             ...                        \n",
       "417    would really like video include information cr...\n",
       "418    not definitely see would make boy scene feel n...\n",
       "419    man even thought makes loving someone cant kno...\n",
       "420                                          not us kill\n",
       "421    institutions accommodate religious serious med...\n",
       "Name: probody, Length: 422, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predf['probody']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predf.to_pickle(\"preprocessed_author.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
