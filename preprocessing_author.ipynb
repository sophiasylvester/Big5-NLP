{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Version with one row per author:\n",
    "\n",
    "### Features (as in paper):\n",
    "1. 11.140 ngram features: tf and tf-idf weighted word and character ngrams stemmed with Porter's stemmer\n",
    "2. type-token ratio\n",
    "3. ratio of comments in English\n",
    "4. ratio of British english vs. American English words\n",
    "5. 93 features from LIWC \n",
    "6. 26 PSYCH features (Preotiuc: Paraphrase Database and MRC Psycholinguistics Database)\n",
    "\n",
    "### Columns (from the description of the dataset):\n",
    "1. 'global':[7,10], #subreddits_commented, subreddits_commented_mbti, num_comments\n",
    "2. 'liwc':[10,103], #liwc\n",
    "3. 'word':[103,3938], #top1000 word ngram (1,2,3) per dimension based on chi2\n",
    "4. 'char':[3938,7243], #top1000 char ngrams (2,3) per dimension based on chi2\n",
    "5. 'sub':[7243,12228], #number of comments in each subreddit\n",
    "6. 'ent':[12228,12229], #entropy\n",
    "7. 'subtf':[12229,17214], #tf-idf on subreddits\n",
    "8. 'subcat':[17214,17249], #manually crafted subreddit categories\n",
    "9. 'lda50':[17249,17299], #50 LDA topics\n",
    "10. 'posts':[17299,17319], #posts statistics\n",
    "11. 'lda100':[17319,17419], #100 LDA topics\n",
    "12. 'psy':[17419,17443], #psycholinguistic features\n",
    "13. 'en':[17443,17444], #ratio of english comments\n",
    "14. 'ttr':[17444,17445], #type token ratio\n",
    "15. 'meaning':[17445,17447], #additional pyscholinguistic features\n",
    "16. 'time_diffs':[17447,17453], #commenting time diffs\n",
    "17. 'month':[17453,17465], #monthly distribution\n",
    "18. 'hour':[17465,17489], #hourly distribution\n",
    "19. 'day_of_week':[17489,17496], #daily distribution\n",
    "20. 'word_an':[17496,21496], #word ngrams selected by F-score\n",
    "21. 'word_an_tf':[21496,25496], #tf-idf ngrams selected by F-score\n",
    "22. 'char_an':[25496,29496], #char ngrams selected by F-score\n",
    "23. 'char_an_tf':[29496,33496], #tf-idf char ngrams selected by F-score\n",
    "24. 'brit_amer':[33496,33499], #british vs american english ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sophia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import bigrams, ngrams\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from collections import Counter\n",
    "from num2words import num2words \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import random\n",
    "random.seed(32)\n",
    "\n",
    "# close nltk download window to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_quoteless</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sabata11792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not seeing any break or signal lights and no p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1534890968</td>\n",
       "      <td>t5_3fqup</td>\n",
       "      <td>t3_995l9s</td>\n",
       "      <td>t1_e4lbrls</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>e4lkg2l</td>\n",
       "      <td>ATBGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Swarels</td>\n",
       "      <td>INTP</td>\n",
       "      <td>Multiverses, matrix theory, consciousness. Scr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1499749893</td>\n",
       "      <td>t5_2qhvl</td>\n",
       "      <td>t3_6mjw62</td>\n",
       "      <td>t1_dk26jre</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dk26vpo</td>\n",
       "      <td>INTP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>46</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pearlz176</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>Hope you've enjoyed the ride :D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1485613795</td>\n",
       "      <td>t5_2qi58</td>\n",
       "      <td>t3_5qnd1v</td>\n",
       "      <td>t1_dd0mdqg</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dd0nxif</td>\n",
       "      <td>soccer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rainbowhotpocket</td>\n",
       "      <td>Colts</td>\n",
       "      <td>Idk, in the AFC if i recall correctly since 20...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1466965660</td>\n",
       "      <td>t5_2qmg3</td>\n",
       "      <td>t3_4pypuh</td>\n",
       "      <td>t1_d4oulvo</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d4our0i</td>\n",
       "      <td>nfl</td>\n",
       "      <td>11.0</td>\n",
       "      <td>62</td>\n",
       "      <td>61</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amathyx</td>\n",
       "      <td>http://myanimelist.net/profile/amathy</td>\n",
       "      <td>22 hours later and the music is still going[co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1495851189</td>\n",
       "      <td>t5_2qh22</td>\n",
       "      <td>t3_6ddiow</td>\n",
       "      <td>t3_6ddiow</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>di3inz7</td>\n",
       "      <td>anime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>AureliusPendragon</td>\n",
       "      <td>�� You are your dragon. Slay yourself.</td>\n",
       "      <td>Fair enough.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1511890614</td>\n",
       "      <td>t5_32jqy</td>\n",
       "      <td>t3_7g4acj</td>\n",
       "      <td>t1_dqgo961</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dqgonrb</td>\n",
       "      <td>JordanPeterson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>ksvr</td>\n",
       "      <td>Bengals</td>\n",
       "      <td>maybe him and Marvin Lewis have the same secre...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1481133329</td>\n",
       "      <td>t5_2qmg3</td>\n",
       "      <td>t3_5h055f</td>\n",
       "      <td>t1_dawe1yo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dawlszw</td>\n",
       "      <td>nfl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>vipertongn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>how would you flash the kernel using stock rom...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1470043422</td>\n",
       "      <td>t5_39zt6</td>\n",
       "      <td>t3_4vk0ch</td>\n",
       "      <td>t1_d5z2og3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d5ze762</td>\n",
       "      <td>Nexus6P</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>32</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>lordoftheslums</td>\n",
       "      <td>Bulls</td>\n",
       "      <td>Which ever one of the Bulls/Celtics first roun...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1450723952</td>\n",
       "      <td>t5_2qo4s</td>\n",
       "      <td>t3_3xot3k</td>\n",
       "      <td>t3_3xot3k</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cy6ukte</td>\n",
       "      <td>nba</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>HellhoundsOnMyTrail</td>\n",
       "      <td>prototypical non-conformist</td>\n",
       "      <td>Obviously unstable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1467132062</td>\n",
       "      <td>t5_2rct2</td>\n",
       "      <td>t3_4qab38</td>\n",
       "      <td>t1_d4rdk6k</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d4rdn40</td>\n",
       "      <td>OkCupid</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author                       author_flair_text  \\\n",
       "0            Sabata11792                                     NaN   \n",
       "1                Swarels                                    INTP   \n",
       "2              pearlz176                       Manchester United   \n",
       "3       rainbowhotpocket                                   Colts   \n",
       "4                amathyx   http://myanimelist.net/profile/amathy   \n",
       "..                   ...                                     ...   \n",
       "995    AureliusPendragon  �� You are your dragon. Slay yourself.   \n",
       "996                 ksvr                                 Bengals   \n",
       "997           vipertongn                                     NaN   \n",
       "998       lordoftheslums                                   Bulls   \n",
       "999  HellhoundsOnMyTrail             prototypical non-conformist   \n",
       "\n",
       "                                                  body  downs  created_utc  \\\n",
       "0    Not seeing any break or signal lights and no p...    NaN   1534890968   \n",
       "1    Multiverses, matrix theory, consciousness. Scr...    NaN   1499749893   \n",
       "2                      Hope you've enjoyed the ride :D    NaN   1485613795   \n",
       "3    Idk, in the AFC if i recall correctly since 20...    NaN   1466965660   \n",
       "4    22 hours later and the music is still going[co...    NaN   1495851189   \n",
       "..                                                 ...    ...          ...   \n",
       "995                                       Fair enough.    NaN   1511890614   \n",
       "996  maybe him and Marvin Lewis have the same secre...    0.0   1481133329   \n",
       "997  how would you flash the kernel using stock rom...    NaN   1470043422   \n",
       "998  Which ever one of the Bulls/Celtics first roun...    NaN   1450723952   \n",
       "999                                 Obviously unstable    NaN   1467132062   \n",
       "\n",
       "    subreddit_id    link_id   parent_id  score  controversiality  gilded  \\\n",
       "0       t5_3fqup  t3_995l9s  t1_e4lbrls    1.0                 0       0   \n",
       "1       t5_2qhvl  t3_6mjw62  t1_dk26jre    7.0                 0       0   \n",
       "2       t5_2qi58  t3_5qnd1v  t1_dd0mdqg    2.0                 0       0   \n",
       "3       t5_2qmg3  t3_4pypuh  t1_d4oulvo   11.0                 0       0   \n",
       "4       t5_2qh22  t3_6ddiow   t3_6ddiow    6.0                 0       0   \n",
       "..           ...        ...         ...    ...               ...     ...   \n",
       "995     t5_32jqy  t3_7g4acj  t1_dqgo961    2.0                 0       0   \n",
       "996     t5_2qmg3  t3_5h055f  t1_dawe1yo    1.0                 0       0   \n",
       "997     t5_39zt6  t3_4vk0ch  t1_d5z2og3    1.0                 0       0   \n",
       "998     t5_2qo4s  t3_3xot3k   t3_3xot3k    1.0                 0       0   \n",
       "999     t5_2rct2  t3_4qab38  t1_d4rdk6k    1.0                 0       0   \n",
       "\n",
       "          id       subreddit   ups  word_count  word_count_quoteless lang  \n",
       "0    e4lkg2l           ATBGE   NaN          19                    19   en  \n",
       "1    dk26vpo            INTP   NaN          51                    46   en  \n",
       "2    dd0nxif          soccer   NaN           6                     6   en  \n",
       "3    d4our0i             nfl  11.0          62                    61   en  \n",
       "4    di3inz7           anime   NaN          32                    29   en  \n",
       "..       ...             ...   ...         ...                   ...  ...  \n",
       "995  dqgonrb  JordanPeterson   NaN           2                     2   en  \n",
       "996  dawlszw             nfl   0.0          14                    14   en  \n",
       "997  d5ze762         Nexus6P   1.0          33                    32   en  \n",
       "998  cy6ukte             nba   1.0          32                    32   en  \n",
       "999  d4rdn40         OkCupid   1.0           2                     2   en  \n",
       "\n",
       "[1000 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/sophia/ma_py/pandora_bigfive1000.csv')\n",
    "# print(pandora.info(verbose=True))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change language to numeric representation\n",
    "def numeric_lang(df):\n",
    "    # change lang to numerical representation\n",
    "    language = df['lang'].values.tolist()\n",
    "    language = set(language)\n",
    "    language\n",
    "    df['language']= np.select([df.lang == 'en', df.lang == 'es', df.lang == 'nl'], \n",
    "                            [0, 1, 2], \n",
    "                            default=3)\n",
    "    # print(gramsdf['language'])\n",
    "    df = df.drop(columns=['lang'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timecolumns(df):\n",
    "    readable = []\n",
    "    weekday = []\n",
    "    month = []\n",
    "    year = []\n",
    "    for row in df['created_utc']:\n",
    "        item = datetime.datetime.fromtimestamp(row)\n",
    "        weekday_item = item.strftime('%A')\n",
    "        readable_item = datetime.datetime.fromtimestamp(row).isoformat()\n",
    "        month.append(str(readable_item[5:7]))\n",
    "        year.append(str(readable_item[0:4]))\n",
    "        readable.append(readable_item)\n",
    "        weekday.append(weekday_item)\n",
    "    df['time'] = readable\n",
    "    df['weekday'] = weekday\n",
    "    df['month'] = month\n",
    "    df['year'] = year\n",
    "    return df\n",
    "\n",
    "# pandora = create_timecolumns(pandora)\n",
    "# pandora.head()\n",
    "# test = pandora.iloc[0]['time']\n",
    "# print(test)\n",
    "# print(test[0:4])\n",
    "# lst = pandora.weekday.tolist()\n",
    "# lstset = set(lst)\n",
    "# print(lstset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timecounter(lst, vocablst):\n",
    "    if vocablst == 'weekday':\n",
    "        vocab = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    elif vocablst == 'month':\n",
    "        vocab = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    elif vocablst == 'year':\n",
    "        vocab = ['2015', '2016', '2017', '2018', '2019']\n",
    "    else:\n",
    "        print(\"No valid input: vocab list\")\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", vocabulary=vocab)\n",
    "    vectors = vectorizer.fit_transform(lst)\n",
    "    v = vectors.toarray()\n",
    "#     is_all_zero = np.all((v == 0))\n",
    "#     names = vectorizer.get_feature_names()\n",
    "    return v\n",
    "\n",
    "# item = ['Sunday Tuesday']\n",
    "# print(item)\n",
    "# test = timecounter(item, 'weekday')\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subredditcounter(df, lst):\n",
    "    lst = df['subreddit'].tolist()\n",
    "    subredditset = set(lst)\n",
    "    subredditlist = list(subredditset)\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", vocabulary=subredditlist)\n",
    "    vectors = vectorizer.fit_transform(lst)\n",
    "    v = vectors.toarray()\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>complete_body</th>\n",
       "      <th>doc_body</th>\n",
       "      <th>all_utc</th>\n",
       "      <th>mean_controversiality</th>\n",
       "      <th>mean_gilded</th>\n",
       "      <th>num_subreddits</th>\n",
       "      <th>subreddit_dist</th>\n",
       "      <th>weekday_dist</th>\n",
       "      <th>month_dist</th>\n",
       "      <th>year_dist</th>\n",
       "      <th>all_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>-BigSexy-</td>\n",
       "      <td>Oooh i see</td>\n",
       "      <td>[Oooh i see]</td>\n",
       "      <td>[1510236798]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-BlitzN9ne</td>\n",
       "      <td>**Quality** material right here</td>\n",
       "      <td>[**Quality** material right here]</td>\n",
       "      <td>[1549708109]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>-CrestiaBell</td>\n",
       "      <td>A slidewhistle or a meow-meow board That's bec...</td>\n",
       "      <td>[A slidewhistle or a meow-meow board, That's b...</td>\n",
       "      <td>[1538664591, 1475867279, 1505862626, 151267621...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2]</td>\n",
       "      <td>[0, 2, 3, 1, 1]</td>\n",
       "      <td>0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>-tactical-throw-away</td>\n",
       "      <td>Sorry for your feelings. Kek &amp;lt;------- This ...</td>\n",
       "      <td>[Sorry for your feelings., Kek &amp;lt;------- Thi...</td>\n",
       "      <td>[1498536785, 1486701409, 1506834463]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 3, 0, 0]</td>\n",
       "      <td>0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>137288</td>\n",
       "      <td>Carly's so glad to get your .0000003 cents Exc...</td>\n",
       "      <td>[Carly's so glad to get your .0000003 cents, E...</td>\n",
       "      <td>[1536611153, 1550537879, 1516548513, 1523299682]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 3, 1]</td>\n",
       "      <td>0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>xanthraxoid</td>\n",
       "      <td>I'd really like this video to include some inf...</td>\n",
       "      <td>[I'd really like this video to include some in...</td>\n",
       "      <td>[1469892161, 1486826547, 1498046590, 1550346594]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 0, 1]</td>\n",
       "      <td>0 0 3 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>xenomouse</td>\n",
       "      <td>You're a guy, aren't you? I can definitely see...</td>\n",
       "      <td>[You're a guy, aren't you? I can definitely se...</td>\n",
       "      <td>[1506710219, 1502740906, 1517847908, 1506874589]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 3, 1, 0]</td>\n",
       "      <td>0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>xeroctr3</td>\n",
       "      <td>man even the thought of it makes me depressed....</td>\n",
       "      <td>[man even the thought of it makes me depressed...</td>\n",
       "      <td>[1521414051]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>xzack18</td>\n",
       "      <td>Not all of us are out to kill</td>\n",
       "      <td>[Not all of us are out to kill]</td>\n",
       "      <td>[1533749569]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>zugzwang_03</td>\n",
       "      <td>Institutions should accommodate religious or s...</td>\n",
       "      <td>[Institutions should accommodate religious or ...</td>\n",
       "      <td>[1514216199, 1459000262, 1500701643, 151759545...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1]</td>\n",
       "      <td>[0, 3, 2, 1, 0]</td>\n",
       "      <td>0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>429 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author                                      complete_body  \\\n",
       "906             -BigSexy-                                         Oooh i see   \n",
       "145            -BlitzN9ne                    **Quality** material right here   \n",
       "367          -CrestiaBell  A slidewhistle or a meow-meow board That's bec...   \n",
       "295  -tactical-throw-away  Sorry for your feelings. Kek &lt;------- This ...   \n",
       "791                137288  Carly's so glad to get your .0000003 cents Exc...   \n",
       "..                    ...                                                ...   \n",
       "324           xanthraxoid  I'd really like this video to include some inf...   \n",
       "954             xenomouse  You're a guy, aren't you? I can definitely see...   \n",
       "208              xeroctr3  man even the thought of it makes me depressed....   \n",
       "990               xzack18                      Not all of us are out to kill   \n",
       "936           zugzwang_03  Institutions should accommodate religious or s...   \n",
       "\n",
       "                                              doc_body  \\\n",
       "906                                       [Oooh i see]   \n",
       "145                  [**Quality** material right here]   \n",
       "367  [A slidewhistle or a meow-meow board, That's b...   \n",
       "295  [Sorry for your feelings., Kek &lt;------- Thi...   \n",
       "791  [Carly's so glad to get your .0000003 cents, E...   \n",
       "..                                                 ...   \n",
       "324  [I'd really like this video to include some in...   \n",
       "954  [You're a guy, aren't you? I can definitely se...   \n",
       "208  [man even the thought of it makes me depressed...   \n",
       "990                    [Not all of us are out to kill]   \n",
       "936  [Institutions should accommodate religious or ...   \n",
       "\n",
       "                                               all_utc  mean_controversiality  \\\n",
       "906                                       [1510236798]                    0.0   \n",
       "145                                       [1549708109]                    0.0   \n",
       "367  [1538664591, 1475867279, 1505862626, 151267621...                    0.0   \n",
       "295               [1498536785, 1486701409, 1506834463]                    0.0   \n",
       "791   [1536611153, 1550537879, 1516548513, 1523299682]                    0.0   \n",
       "..                                                 ...                    ...   \n",
       "324   [1469892161, 1486826547, 1498046590, 1550346594]                    0.0   \n",
       "954   [1506710219, 1502740906, 1517847908, 1506874589]                    0.0   \n",
       "208                                       [1521414051]                    0.0   \n",
       "990                                       [1533749569]                    0.0   \n",
       "936  [1514216199, 1459000262, 1500701643, 151759545...                    0.0   \n",
       "\n",
       "     mean_gilded  num_subreddits  \\\n",
       "906          0.0               1   \n",
       "145          0.0               1   \n",
       "367          0.0               4   \n",
       "295          0.0               1   \n",
       "791          0.0               1   \n",
       "..           ...             ...   \n",
       "324          0.0               4   \n",
       "954          0.0               2   \n",
       "208          0.0               1   \n",
       "990          0.0               1   \n",
       "936          0.0               5   \n",
       "\n",
       "                                        subreddit_dist           weekday_dist  \\\n",
       "906  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "145  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "367  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "295  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "791  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "..                                                 ...                    ...   \n",
       "324  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "954  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "208  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "990  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "936  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                               month_dist        year_dist       all_lang  \n",
       "906  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  [0, 0, 1, 0, 0]              0  \n",
       "145  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 1]              0  \n",
       "367  [1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2]  [0, 2, 3, 1, 1]  0 0 0 0 0 0 0  \n",
       "295  [0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]  [0, 0, 3, 0, 0]          0 0 0  \n",
       "791  [1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]  [0, 0, 0, 3, 1]        0 0 0 0  \n",
       "..                                    ...              ...            ...  \n",
       "324  [0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]  [0, 1, 2, 0, 1]        0 0 3 0  \n",
       "954  [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]  [0, 0, 3, 1, 0]        0 0 0 0  \n",
       "208  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 1, 0]              0  \n",
       "990  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  [0, 0, 0, 1, 0]              0  \n",
       "936  [0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1]  [0, 3, 2, 1, 0]    0 0 0 0 0 0  \n",
       "\n",
       "[429 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_authordf(df): \n",
    "    df = numeric_lang(df)\n",
    "    # body\n",
    "    df['complete_body'] = df.groupby(['author'])['body'].transform(lambda x : ' '. join(x))\n",
    "    df['doc_body'] = df.groupby(['author'])['body'].transform(lambda x : '§'. join(x))\n",
    "    df['doc_body'] =  df['doc_body'].apply(lambda x: x.split(\"§\"))\n",
    "    \n",
    "    # language\n",
    "    df['lang'] = df['language'].apply(lambda x: str(x))\n",
    "    df['all_lang'] = df.groupby(['author'])['lang'].transform(lambda x : ' '. join(x))\n",
    "    # created_utc\n",
    "    df['utc_lst'] = df['created_utc'].apply(lambda x: str(x))\n",
    "    df['all_utc'] = df.groupby(['author'])['utc_lst'].transform(lambda x : ' '. join(x))\n",
    "    df['all_utc'] = df['all_utc'].apply(lambda x: x.split())\n",
    "    # controversiality\n",
    "    df['mean_controversiality'] = df.groupby(['author']).agg({'controversiality': ['mean']})\n",
    "    df['mean_controversiality'] = df['mean_controversiality'].fillna(0)\n",
    "    # gilded\n",
    "    df['mean_gilded'] = df.groupby(['author']).agg({'gilded': ['mean']})\n",
    "    df['mean_gilded'] = df['mean_gilded'].fillna(0)\n",
    "    # number of subreddits\n",
    "    df['num_subreddits'] = df.groupby(['author'])['subreddit'].transform(lambda x : ' '. join(x))\n",
    "    df['num_subreddits'] = df['num_subreddits'].apply(lambda x: len(set(x.split())))\n",
    "    # number of comments per subreddit\n",
    "    df['subreddit_dist'] = df.groupby(['author'])['subreddit'].transform(lambda x : ' '. join(x))\n",
    "    subreddit = subredditcounter(df, df['subreddit_dist'])\n",
    "    subreddit = subreddit.tolist()\n",
    "    subreddit_arr = [np.array(lst) for lst in subreddit]\n",
    "    df['subreddit_dist'] = subreddit_arr\n",
    "    # time\n",
    "    df = create_timecolumns(df)\n",
    "    df['weekday_dist'] = df.groupby(['author'])['weekday'].transform(lambda x : ' '. join(x))\n",
    "    weekday = timecounter(df['weekday_dist'], 'weekday')\n",
    "    weekday = weekday.tolist()\n",
    "    weekday_arr = [np.array(lst) for lst in weekday]\n",
    "    df['weekday_dist'] = weekday_arr\n",
    "    df['month_dist'] = df.groupby(['author'])['month'].transform(lambda x : ' '. join(x))\n",
    "    month = timecounter(df['month_dist'], 'month')\n",
    "    month = month.tolist()\n",
    "    month_arr = [np.array(lst) for lst in month]\n",
    "    df['month_dist'] = month_arr\n",
    "    df['year_dist'] = df.groupby(['author'])['year'].transform(lambda x : ' '. join(x))\n",
    "    year = timecounter(df['year_dist'], 'year')\n",
    "    year = year.tolist()\n",
    "    year_arr = [np.array(lst) for lst in year]\n",
    "    df['year_dist'] = year_arr\n",
    "    \n",
    "    newdf = df[['author', 'complete_body', 'doc_body', 'all_utc', 'mean_controversiality', \n",
    "                'mean_gilded', 'num_subreddits', 'subreddit_dist', 'weekday_dist', \n",
    "                'month_dist', 'year_dist', 'all_lang']]\n",
    "    newdf = newdf.sort_values(by='author')\n",
    "    newdf = newdf.drop_duplicates(subset=['author'])\n",
    "    return newdf\n",
    "\n",
    "\n",
    "pandora = create_authordf(df)\n",
    "pandora\n",
    "# print(type(pandora.iloc[0]['subreddit_dist']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 429 entries, 906 to 936\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   author                 429 non-null    object \n",
      " 1   complete_body          429 non-null    object \n",
      " 2   doc_body               429 non-null    object \n",
      " 3   all_utc                429 non-null    object \n",
      " 4   mean_controversiality  429 non-null    float64\n",
      " 5   mean_gilded            429 non-null    float64\n",
      " 6   num_subreddits         429 non-null    int64  \n",
      " 7   subreddit_dist         429 non-null    object \n",
      " 8   weekday_dist           429 non-null    object \n",
      " 9   month_dist             429 non-null    object \n",
      " 10  year_dist              429 non-null    object \n",
      " 11  all_lang               429 non-null    object \n",
      "dtypes: float64(2), int64(1), object(9)\n",
      "memory usage: 43.6+ KB\n"
     ]
    }
   ],
   "source": [
    "pandora.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataframe:  422\n",
      "NaN in df?  False\n",
      "Sum of NaN in agreeableness 0\n",
      "Sum of NaN in openness 0\n",
      "Sum of NaN in conscientiousness 0\n",
      "Sum of NaN in extraversion 0\n",
      "Sum of NaN in neuroticism 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>complete_body</th>\n",
       "      <th>doc_body</th>\n",
       "      <th>all_utc</th>\n",
       "      <th>mean_controversiality</th>\n",
       "      <th>mean_gilded</th>\n",
       "      <th>num_subreddits</th>\n",
       "      <th>subreddit_dist</th>\n",
       "      <th>weekday_dist</th>\n",
       "      <th>month_dist</th>\n",
       "      <th>year_dist</th>\n",
       "      <th>all_lang</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-BigSexy-</td>\n",
       "      <td>Oooh i see</td>\n",
       "      <td>[Oooh i see]</td>\n",
       "      <td>[1510236798]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-BlitzN9ne</td>\n",
       "      <td>**Quality** material right here</td>\n",
       "      <td>[**Quality** material right here]</td>\n",
       "      <td>[1549708109]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-CrestiaBell</td>\n",
       "      <td>A slidewhistle or a meow-meow board That's bec...</td>\n",
       "      <td>[A slidewhistle or a meow-meow board, That's b...</td>\n",
       "      <td>[1538664591, 1475867279, 1505862626, 151267621...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2]</td>\n",
       "      <td>[0, 2, 3, 1, 1]</td>\n",
       "      <td>0 0 0 0 0 0 0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-tactical-throw-away</td>\n",
       "      <td>Sorry for your feelings. Kek &amp;lt;------- This ...</td>\n",
       "      <td>[Sorry for your feelings., Kek &amp;lt;------- Thi...</td>\n",
       "      <td>[1498536785, 1486701409, 1506834463]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 3, 0, 0]</td>\n",
       "      <td>0 0 0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>137288</td>\n",
       "      <td>Carly's so glad to get your .0000003 cents Exc...</td>\n",
       "      <td>[Carly's so glad to get your .0000003 cents, E...</td>\n",
       "      <td>[1536611153, 1550537879, 1516548513, 1523299682]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 3, 1]</td>\n",
       "      <td>0 0 0 0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>424</td>\n",
       "      <td>xanthraxoid</td>\n",
       "      <td>I'd really like this video to include some inf...</td>\n",
       "      <td>[I'd really like this video to include some in...</td>\n",
       "      <td>[1469892161, 1486826547, 1498046590, 1550346594]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 0, 1]</td>\n",
       "      <td>0 0 3 0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>425</td>\n",
       "      <td>xenomouse</td>\n",
       "      <td>You're a guy, aren't you? I can definitely see...</td>\n",
       "      <td>[You're a guy, aren't you? I can definitely se...</td>\n",
       "      <td>[1506710219, 1502740906, 1517847908, 1506874589]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 3, 1, 0]</td>\n",
       "      <td>0 0 0 0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>426</td>\n",
       "      <td>xeroctr3</td>\n",
       "      <td>man even the thought of it makes me depressed....</td>\n",
       "      <td>[man even the thought of it makes me depressed...</td>\n",
       "      <td>[1521414051]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>427</td>\n",
       "      <td>xzack18</td>\n",
       "      <td>Not all of us are out to kill</td>\n",
       "      <td>[Not all of us are out to kill]</td>\n",
       "      <td>[1533749569]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>428</td>\n",
       "      <td>zugzwang_03</td>\n",
       "      <td>Institutions should accommodate religious or s...</td>\n",
       "      <td>[Institutions should accommodate religious or ...</td>\n",
       "      <td>[1514216199, 1459000262, 1500701643, 151759545...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1]</td>\n",
       "      <td>[0, 3, 2, 1, 0]</td>\n",
       "      <td>0 0 0 0 0 0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>422 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                author  \\\n",
       "0        0             -BigSexy-   \n",
       "1        1            -BlitzN9ne   \n",
       "2        2          -CrestiaBell   \n",
       "3        3  -tactical-throw-away   \n",
       "4        4                137288   \n",
       "..     ...                   ...   \n",
       "417    424           xanthraxoid   \n",
       "418    425             xenomouse   \n",
       "419    426              xeroctr3   \n",
       "420    427               xzack18   \n",
       "421    428           zugzwang_03   \n",
       "\n",
       "                                         complete_body  \\\n",
       "0                                           Oooh i see   \n",
       "1                      **Quality** material right here   \n",
       "2    A slidewhistle or a meow-meow board That's bec...   \n",
       "3    Sorry for your feelings. Kek &lt;------- This ...   \n",
       "4    Carly's so glad to get your .0000003 cents Exc...   \n",
       "..                                                 ...   \n",
       "417  I'd really like this video to include some inf...   \n",
       "418  You're a guy, aren't you? I can definitely see...   \n",
       "419  man even the thought of it makes me depressed....   \n",
       "420                      Not all of us are out to kill   \n",
       "421  Institutions should accommodate religious or s...   \n",
       "\n",
       "                                              doc_body  \\\n",
       "0                                         [Oooh i see]   \n",
       "1                    [**Quality** material right here]   \n",
       "2    [A slidewhistle or a meow-meow board, That's b...   \n",
       "3    [Sorry for your feelings., Kek &lt;------- Thi...   \n",
       "4    [Carly's so glad to get your .0000003 cents, E...   \n",
       "..                                                 ...   \n",
       "417  [I'd really like this video to include some in...   \n",
       "418  [You're a guy, aren't you? I can definitely se...   \n",
       "419  [man even the thought of it makes me depressed...   \n",
       "420                    [Not all of us are out to kill]   \n",
       "421  [Institutions should accommodate religious or ...   \n",
       "\n",
       "                                               all_utc  mean_controversiality  \\\n",
       "0                                         [1510236798]                    0.0   \n",
       "1                                         [1549708109]                    0.0   \n",
       "2    [1538664591, 1475867279, 1505862626, 151267621...                    0.0   \n",
       "3                 [1498536785, 1486701409, 1506834463]                    0.0   \n",
       "4     [1536611153, 1550537879, 1516548513, 1523299682]                    0.0   \n",
       "..                                                 ...                    ...   \n",
       "417   [1469892161, 1486826547, 1498046590, 1550346594]                    0.0   \n",
       "418   [1506710219, 1502740906, 1517847908, 1506874589]                    0.0   \n",
       "419                                       [1521414051]                    0.0   \n",
       "420                                       [1533749569]                    0.0   \n",
       "421  [1514216199, 1459000262, 1500701643, 151759545...                    0.0   \n",
       "\n",
       "     mean_gilded  num_subreddits  \\\n",
       "0            0.0               1   \n",
       "1            0.0               1   \n",
       "2            0.0               4   \n",
       "3            0.0               1   \n",
       "4            0.0               1   \n",
       "..           ...             ...   \n",
       "417          0.0               4   \n",
       "418          0.0               2   \n",
       "419          0.0               1   \n",
       "420          0.0               1   \n",
       "421          0.0               5   \n",
       "\n",
       "                                        subreddit_dist           weekday_dist  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "..                                                 ...                    ...   \n",
       "417  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "418  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "419  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "420  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "421  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                               month_dist        year_dist       all_lang  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  [0, 0, 1, 0, 0]              0   \n",
       "1    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 1]              0   \n",
       "2    [1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2]  [0, 2, 3, 1, 1]  0 0 0 0 0 0 0   \n",
       "3    [0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]  [0, 0, 3, 0, 0]          0 0 0   \n",
       "4    [1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]  [0, 0, 0, 3, 1]        0 0 0 0   \n",
       "..                                    ...              ...            ...   \n",
       "417  [0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]  [0, 1, 2, 0, 1]        0 0 3 0   \n",
       "418  [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]  [0, 0, 3, 1, 0]        0 0 0 0   \n",
       "419  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 1, 0]              0   \n",
       "420  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  [0, 0, 0, 1, 0]              0   \n",
       "421  [0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1]  [0, 3, 2, 1, 0]    0 0 0 0 0 0   \n",
       "\n",
       "     agreeableness  openness  conscientiousness  extraversion  neuroticism  \n",
       "0             39.0      92.0                1.0          18.0          4.0  \n",
       "1             50.0      85.0               15.0          50.0         30.0  \n",
       "2             50.0      85.0               50.0          85.0         50.0  \n",
       "3              2.0      92.0               31.0          60.0         53.0  \n",
       "4             10.0      87.0               49.0           7.0         87.0  \n",
       "..             ...       ...                ...           ...          ...  \n",
       "417           86.0      45.0                8.0          62.0         72.0  \n",
       "418           26.0      93.0               49.0          70.0         16.0  \n",
       "419            3.0      75.0               27.0           3.0         77.0  \n",
       "420            4.0      19.0               11.0          27.0         16.0  \n",
       "421           10.0      41.0               86.0          83.0         18.0  \n",
       "\n",
       "[422 rows x 18 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = pd.read_csv('/home/sophia/ma_py/author_profiles.csv')\n",
    "# find missing data in big five traits\n",
    "# authorslst = authors['author'].tolist()\n",
    "# print(\"Author search: \", 'DarthHedonist' in authorslst)\n",
    "# print(\"Author search: \", 'FonsoTheWhitesican' in authorslst)\n",
    "# print(\"Author search: \", 'chaosking121' in authorslst)\n",
    "\n",
    "bigfive = authors[['author','agreeableness','openness','conscientiousness','extraversion','neuroticism']]\n",
    "bigfive = bigfive.dropna()\n",
    "# print(bigfive[bigfive['author'] == \"DarthHedonist\"])\n",
    "\n",
    "# pandoradf = pd.merge(pandora, bigfive, how='left', on='author')\n",
    "pandoradf = pandora.merge(bigfive, how='left', on=['author'])\n",
    "# pandoradf = pandoradf.dropna()\n",
    "pandoradf = pandoradf.sort_values(by='author')\n",
    "pandoradf = pandoradf[pandoradf['agreeableness'].notna()]\n",
    "pandoradf = pandoradf.reset_index()\n",
    "\n",
    "\n",
    "print(\"Length of dataframe: \", len(pandoradf))\n",
    "print(\"NaN in df? \", pandoradf.isnull().any().any())\n",
    "print(\"Sum of NaN in agreeableness\", pandoradf['agreeableness'].isnull().values.sum())\n",
    "print(\"Sum of NaN in openness\", pandoradf['openness'].isnull().values.sum())\n",
    "print(\"Sum of NaN in conscientiousness\", pandoradf['conscientiousness'].isnull().values.sum())\n",
    "print(\"Sum of NaN in extraversion\", pandoradf['extraversion'].isnull().values.sum())\n",
    "print(\"Sum of NaN in neuroticism\", pandoradf['neuroticism'].isnull().values.sum())\n",
    "# nan_values = pandoradf[pandoradf['neuroticism'].isna()]\n",
    "# nan_values\n",
    "pandoradf\n",
    "# pandoradf[pandoradf.isnull().any(axis=1)]\n",
    "\n",
    "# number of entries does not fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigfive_cat(df):\n",
    "    # change big five to binary representation\n",
    "    df['agree'] = df['agreeableness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['openn'] = df['openness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['consc'] = df['conscientiousness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['extra'] = df['extraversion'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['neuro'] = df['neuroticism'].apply(lambda x: 0 if x<50 else 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust representations of some columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_stopwordlist(df, mode):\n",
    "    if mode == 'NLTK':\n",
    "        stopwordList = stopwords.words('english')\n",
    "    if mode == 'NLTK-neg':\n",
    "        stopwordList = stopwords.words('english')\n",
    "        stopwordList.remove('no')\n",
    "        stopwordList.remove('nor')\n",
    "        stopwordList.remove('not')\n",
    "    return stopwordList\n",
    "\n",
    "# stopwordList = choose_stopwordlist(pandoradf, mode='NLTK-neg')\n",
    "\n",
    "# print(stopwordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "1. lower \n",
    "2. tokenize\n",
    "3. numbers to words\n",
    "4. delete special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senttokenize(df):\n",
    "    sentbody = []\n",
    "    for row in df['doc_body']:\n",
    "        sentitem = []\n",
    "        for item in row:\n",
    "            sentences = sent_tokenize(item)\n",
    "            sentitem.append(sentences)\n",
    "        sentbody.append(sentitem)\n",
    "    df['senttokens'] = sentbody\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_special(df):\n",
    "    # lower, remove special characters, remove stopwords\n",
    "#     df['probody'] = df['probody'].apply(lambda x: [char.lower() for word in x for char in word.split() if char.isalnum()])\n",
    "#     pandora['probody'] = pandora['probody'].apply(lambda x: [text for row in x for text in row.split() if (text not in stopwordList)])\n",
    "    newrow = []\n",
    "    for row in tqdm(df['probody']):\n",
    "        newcomment = []\n",
    "        for comment in row:\n",
    "            text_pre = \"\"\n",
    "            for character in comment:\n",
    "                if character.isalnum() or character.isspace():\n",
    "                    character = character.lower()\n",
    "                    text_pre += character\n",
    "                else:\n",
    "                    text_pre += \" \"\n",
    "            newcomment.append(text_pre)\n",
    "        newrow.append(newcomment)   \n",
    "    df['probody'] = newrow\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, stopwordList):\n",
    "    newprobody = []\n",
    "    for row in tqdm(df['probody']):\n",
    "        newrowprobody = []\n",
    "        for comment in row:\n",
    "            words = [word for word in comment.split() if (word not in stopwordList)]\n",
    "            newcomment = ' '.join(words)\n",
    "            newrowprobody.append(newcomment)\n",
    "        newprobody.append(newrowprobody)\n",
    "    df['probody'] = newprobody\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokenize(df):    \n",
    "    newbody_complete = []\n",
    "    newprobody_complete = []\n",
    "    # num2words\n",
    "    for row in tqdm(df['probody']):\n",
    "        newbody = []\n",
    "        newprobody = []\n",
    "        for sentence in row:\n",
    "            # string to list\n",
    "            inputtext = sentence.split()\n",
    "            numlist = []\n",
    "            for i in range(len(inputtext)):\n",
    "                if inputtext[i].isnumeric():\n",
    "                    numlist.append(i)\n",
    "            for number in numlist:\n",
    "                inputtext[number] = num2words(inputtext[number])\n",
    "\n",
    "            # list to string\n",
    "            celltext = ' '.join(inputtext)\n",
    "            newprobody.append(celltext)\n",
    "            # tokenize\n",
    "            words = word_tokenize(celltext)\n",
    "            newbody.append(words)\n",
    "        newbody_complete.append(newbody)\n",
    "        newprobody_complete.append(newprobody)\n",
    "    df['probody'] = newprobody_complete\n",
    "    df['tokens'] = newbody_complete\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "# Porter Stemmer\n",
    "def stemming(df):\n",
    "    ps = PorterStemmer()\n",
    "#     df['tokens'] = df['tokens'].progress_apply(lambda x:([ps.stem(word)for row in x for word in row.split() ]))\n",
    "    for row in tqdm(df['tokens']):\n",
    "        for comment in row:\n",
    "            words = [ps.stem(word) for word in comment]\n",
    "            comment = ' '.join(words)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordering(df):\n",
    "    cols_tomove = ['index', 'author', 'complete_body', 'doc_body', 'probody', 'tokens', 'senttokens', 'agreeableness', 'openness', 'conscientiousness', 'extraversion', 'neuroticism', 'agree', 'openn', 'consc', 'extra', 'neuro']\n",
    "    orderdf  = df[cols_tomove + [col for col in df.columns if col not in cols_tomove]]\n",
    "#     orderdf.info(verbose=True)\n",
    "    return orderdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decontract...\n",
      "Tokenize Sentences...\n",
      "Lower words and remove special characters...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e3bcb8433a4aec9bbc1f622142fe8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d09bb9a26d40959b3799435c5ab9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change numbers to words and tokenize words...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7acac3fd9442789e0363e5eb6c237a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porters Stemmer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04648129156e42d29b084e06be38c7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order df...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>complete_body</th>\n",
       "      <th>doc_body</th>\n",
       "      <th>probody</th>\n",
       "      <th>tokens</th>\n",
       "      <th>senttokens</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>...</th>\n",
       "      <th>neuro</th>\n",
       "      <th>all_utc</th>\n",
       "      <th>mean_controversiality</th>\n",
       "      <th>mean_gilded</th>\n",
       "      <th>num_subreddits</th>\n",
       "      <th>subreddit_dist</th>\n",
       "      <th>weekday_dist</th>\n",
       "      <th>month_dist</th>\n",
       "      <th>year_dist</th>\n",
       "      <th>all_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-BigSexy-</td>\n",
       "      <td>Oooh i see</td>\n",
       "      <td>[Oooh i see]</td>\n",
       "      <td>[oooh see]</td>\n",
       "      <td>[[oooh, see]]</td>\n",
       "      <td>[[Oooh i see]]</td>\n",
       "      <td>39.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1510236798]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-BlitzN9ne</td>\n",
       "      <td>**Quality** material right here</td>\n",
       "      <td>[**Quality** material right here]</td>\n",
       "      <td>[quality material right]</td>\n",
       "      <td>[[quality, material, right]]</td>\n",
       "      <td>[[**Quality** material right here]]</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1549708109]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-CrestiaBell</td>\n",
       "      <td>A slidewhistle or a meow-meow board That's bec...</td>\n",
       "      <td>[A slidewhistle or a meow-meow board, That's b...</td>\n",
       "      <td>[slidewhistle meow meow board, watch cartoon s...</td>\n",
       "      <td>[[slidewhistle, meow, meow, board], [watch, ca...</td>\n",
       "      <td>[[A slidewhistle or a meow-meow board], [That'...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1538664591, 1475867279, 1505862626, 151267621...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2]</td>\n",
       "      <td>[0, 2, 3, 1, 1]</td>\n",
       "      <td>0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-tactical-throw-away</td>\n",
       "      <td>Sorry for your feelings. Kek &amp;lt;------- This ...</td>\n",
       "      <td>[Sorry for your feelings., Kek &amp;lt;------- Thi...</td>\n",
       "      <td>[sorry feelings, kek lt onekek kek kek kek, no...</td>\n",
       "      <td>[[sorry, feelings], [kek, lt, onekek, kek, kek...</td>\n",
       "      <td>[[Sorry for your feelings.], [Kek &amp;lt;------- ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1498536785, 1486701409, 1506834463]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 3, 0, 0]</td>\n",
       "      <td>0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>137288</td>\n",
       "      <td>Carly's so glad to get your .0000003 cents Exc...</td>\n",
       "      <td>[Carly's so glad to get your .0000003 cents, E...</td>\n",
       "      <td>[carly glad get three cents, except uk debuted...</td>\n",
       "      <td>[[carly, glad, get, three, cents], [except, uk...</td>\n",
       "      <td>[[Carly's so glad to get your .0000003 cents],...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1536611153, 1550537879, 1516548513, 1523299682]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 3, 1]</td>\n",
       "      <td>0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>424</td>\n",
       "      <td>xanthraxoid</td>\n",
       "      <td>I'd really like this video to include some inf...</td>\n",
       "      <td>[I'd really like this video to include some in...</td>\n",
       "      <td>[would really like video include information c...</td>\n",
       "      <td>[[would, really, like, video, include, informa...</td>\n",
       "      <td>[[I'd really like this video to include some i...</td>\n",
       "      <td>86.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1469892161, 1486826547, 1498046590, 1550346594]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 0, 1]</td>\n",
       "      <td>0 0 3 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>425</td>\n",
       "      <td>xenomouse</td>\n",
       "      <td>You're a guy, aren't you? I can definitely see...</td>\n",
       "      <td>[You're a guy, aren't you? I can definitely se...</td>\n",
       "      <td>[guy not definitely see would make boy scene f...</td>\n",
       "      <td>[[guy, not, definitely, see, would, make, boy,...</td>\n",
       "      <td>[[You're a guy, aren't you?, I can definitely ...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1506710219, 1502740906, 1517847908, 1506874589]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 3, 1, 0]</td>\n",
       "      <td>0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>426</td>\n",
       "      <td>xeroctr3</td>\n",
       "      <td>man even the thought of it makes me depressed....</td>\n",
       "      <td>[man even the thought of it makes me depressed...</td>\n",
       "      <td>[man even thought makes depressed loving someo...</td>\n",
       "      <td>[[man, even, thought, makes, depressed, loving...</td>\n",
       "      <td>[[man even the thought of it makes me depresse...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1521414051]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>427</td>\n",
       "      <td>xzack18</td>\n",
       "      <td>Not all of us are out to kill</td>\n",
       "      <td>[Not all of us are out to kill]</td>\n",
       "      <td>[not us kill]</td>\n",
       "      <td>[[not, us, kill]]</td>\n",
       "      <td>[[Not all of us are out to kill]]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1533749569]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>428</td>\n",
       "      <td>zugzwang_03</td>\n",
       "      <td>Institutions should accommodate religious or s...</td>\n",
       "      <td>[Institutions should accommodate religious or ...</td>\n",
       "      <td>[institutions accommodate religious serious me...</td>\n",
       "      <td>[[institutions, accommodate, religious, seriou...</td>\n",
       "      <td>[[Institutions should accommodate religious or...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1514216199, 1459000262, 1500701643, 151759545...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1]</td>\n",
       "      <td>[0, 3, 2, 1, 0]</td>\n",
       "      <td>0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>422 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                author  \\\n",
       "0        0             -BigSexy-   \n",
       "1        1            -BlitzN9ne   \n",
       "2        2          -CrestiaBell   \n",
       "3        3  -tactical-throw-away   \n",
       "4        4                137288   \n",
       "..     ...                   ...   \n",
       "417    424           xanthraxoid   \n",
       "418    425             xenomouse   \n",
       "419    426              xeroctr3   \n",
       "420    427               xzack18   \n",
       "421    428           zugzwang_03   \n",
       "\n",
       "                                         complete_body  \\\n",
       "0                                           Oooh i see   \n",
       "1                      **Quality** material right here   \n",
       "2    A slidewhistle or a meow-meow board That's bec...   \n",
       "3    Sorry for your feelings. Kek &lt;------- This ...   \n",
       "4    Carly's so glad to get your .0000003 cents Exc...   \n",
       "..                                                 ...   \n",
       "417  I'd really like this video to include some inf...   \n",
       "418  You're a guy, aren't you? I can definitely see...   \n",
       "419  man even the thought of it makes me depressed....   \n",
       "420                      Not all of us are out to kill   \n",
       "421  Institutions should accommodate religious or s...   \n",
       "\n",
       "                                              doc_body  \\\n",
       "0                                         [Oooh i see]   \n",
       "1                    [**Quality** material right here]   \n",
       "2    [A slidewhistle or a meow-meow board, That's b...   \n",
       "3    [Sorry for your feelings., Kek &lt;------- Thi...   \n",
       "4    [Carly's so glad to get your .0000003 cents, E...   \n",
       "..                                                 ...   \n",
       "417  [I'd really like this video to include some in...   \n",
       "418  [You're a guy, aren't you? I can definitely se...   \n",
       "419  [man even the thought of it makes me depressed...   \n",
       "420                    [Not all of us are out to kill]   \n",
       "421  [Institutions should accommodate religious or ...   \n",
       "\n",
       "                                               probody  \\\n",
       "0                                           [oooh see]   \n",
       "1                             [quality material right]   \n",
       "2    [slidewhistle meow meow board, watch cartoon s...   \n",
       "3    [sorry feelings, kek lt onekek kek kek kek, no...   \n",
       "4    [carly glad get three cents, except uk debuted...   \n",
       "..                                                 ...   \n",
       "417  [would really like video include information c...   \n",
       "418  [guy not definitely see would make boy scene f...   \n",
       "419  [man even thought makes depressed loving someo...   \n",
       "420                                      [not us kill]   \n",
       "421  [institutions accommodate religious serious me...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0                                        [[oooh, see]]   \n",
       "1                         [[quality, material, right]]   \n",
       "2    [[slidewhistle, meow, meow, board], [watch, ca...   \n",
       "3    [[sorry, feelings], [kek, lt, onekek, kek, kek...   \n",
       "4    [[carly, glad, get, three, cents], [except, uk...   \n",
       "..                                                 ...   \n",
       "417  [[would, really, like, video, include, informa...   \n",
       "418  [[guy, not, definitely, see, would, make, boy,...   \n",
       "419  [[man, even, thought, makes, depressed, loving...   \n",
       "420                                  [[not, us, kill]]   \n",
       "421  [[institutions, accommodate, religious, seriou...   \n",
       "\n",
       "                                            senttokens  agreeableness  \\\n",
       "0                                       [[Oooh i see]]           39.0   \n",
       "1                  [[**Quality** material right here]]           50.0   \n",
       "2    [[A slidewhistle or a meow-meow board], [That'...           50.0   \n",
       "3    [[Sorry for your feelings.], [Kek &lt;------- ...            2.0   \n",
       "4    [[Carly's so glad to get your .0000003 cents],...           10.0   \n",
       "..                                                 ...            ...   \n",
       "417  [[I'd really like this video to include some i...           86.0   \n",
       "418  [[You're a guy, aren't you?, I can definitely ...           26.0   \n",
       "419  [[man even the thought of it makes me depresse...            3.0   \n",
       "420                  [[Not all of us are out to kill]]            4.0   \n",
       "421  [[Institutions should accommodate religious or...           10.0   \n",
       "\n",
       "     openness  conscientiousness  ...  neuro  \\\n",
       "0        92.0                1.0  ...      0   \n",
       "1        85.0               15.0  ...      0   \n",
       "2        85.0               50.0  ...      1   \n",
       "3        92.0               31.0  ...      1   \n",
       "4        87.0               49.0  ...      1   \n",
       "..        ...                ...  ...    ...   \n",
       "417      45.0                8.0  ...      1   \n",
       "418      93.0               49.0  ...      0   \n",
       "419      75.0               27.0  ...      1   \n",
       "420      19.0               11.0  ...      0   \n",
       "421      41.0               86.0  ...      0   \n",
       "\n",
       "                                               all_utc  mean_controversiality  \\\n",
       "0                                         [1510236798]                    0.0   \n",
       "1                                         [1549708109]                    0.0   \n",
       "2    [1538664591, 1475867279, 1505862626, 151267621...                    0.0   \n",
       "3                 [1498536785, 1486701409, 1506834463]                    0.0   \n",
       "4     [1536611153, 1550537879, 1516548513, 1523299682]                    0.0   \n",
       "..                                                 ...                    ...   \n",
       "417   [1469892161, 1486826547, 1498046590, 1550346594]                    0.0   \n",
       "418   [1506710219, 1502740906, 1517847908, 1506874589]                    0.0   \n",
       "419                                       [1521414051]                    0.0   \n",
       "420                                       [1533749569]                    0.0   \n",
       "421  [1514216199, 1459000262, 1500701643, 151759545...                    0.0   \n",
       "\n",
       "     mean_gilded  num_subreddits  \\\n",
       "0            0.0               1   \n",
       "1            0.0               1   \n",
       "2            0.0               4   \n",
       "3            0.0               1   \n",
       "4            0.0               1   \n",
       "..           ...             ...   \n",
       "417          0.0               4   \n",
       "418          0.0               2   \n",
       "419          0.0               1   \n",
       "420          0.0               1   \n",
       "421          0.0               5   \n",
       "\n",
       "                                        subreddit_dist           weekday_dist  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "..                                                 ...                    ...   \n",
       "417  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "418  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "419  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "420  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "421  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  [0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                               month_dist        year_dist       all_lang  \n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  [0, 0, 1, 0, 0]              0  \n",
       "1    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 0, 1]              0  \n",
       "2    [1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2]  [0, 2, 3, 1, 1]  0 0 0 0 0 0 0  \n",
       "3    [0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]  [0, 0, 3, 0, 0]          0 0 0  \n",
       "4    [1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]  [0, 0, 0, 3, 1]        0 0 0 0  \n",
       "..                                    ...              ...            ...  \n",
       "417  [0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]  [0, 1, 2, 0, 1]        0 0 3 0  \n",
       "418  [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0]  [0, 0, 3, 1, 0]        0 0 0 0  \n",
       "419  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [0, 0, 0, 1, 0]              0  \n",
       "420  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  [0, 0, 0, 1, 0]              0  \n",
       "421  [0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1]  [0, 3, 2, 1, 0]    0 0 0 0 0 0  \n",
       "\n",
       "[422 rows x 26 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    # adjust some column representations\n",
    "    df = bigfive_cat(df)\n",
    "    # choose stopwordlist with or without negation\n",
    "    stopwordList = choose_stopwordlist(df, mode='NLTK-neg')\n",
    "    # decontract abbreviations (e.g., n't to not)\n",
    "    print(\"Decontract...\")\n",
    "    df['probody'] = df['doc_body'].apply(lambda x:([decontracted(x) for x in x]))\n",
    "    # create sentence tokens\n",
    "    print(\"Tokenize Sentences...\")\n",
    "    df = senttokenize(df)\n",
    "    # lower, remove stopwords, num2words, tokenize\n",
    "    print(\"Lower words and remove special characters...\")\n",
    "    df = lower_special(df)\n",
    "    print(\"Remove stopwords...\")\n",
    "    df = remove_stopwords(df, stopwordList)\n",
    "    print(\"Change numbers to words and tokenize words...\")\n",
    "    df = num_tokenize(df)\n",
    "    # porters stemmer\n",
    "    print(\"Porters Stemmer...\")\n",
    "    df = stemming(df)\n",
    "    print(\"Order df...\")\n",
    "    df = ordering(df)\n",
    "    print(\"Done!\")\n",
    "    return df\n",
    "\n",
    "predf = preprocess(pandoradf)\n",
    "\n",
    "predf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 422 entries, 0 to 421\n",
      "Data columns (total 26 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   index                  422 non-null    int64  \n",
      " 1   author                 422 non-null    object \n",
      " 2   complete_body          422 non-null    object \n",
      " 3   doc_body               422 non-null    object \n",
      " 4   probody                422 non-null    object \n",
      " 5   tokens                 422 non-null    object \n",
      " 6   senttokens             422 non-null    object \n",
      " 7   agreeableness          422 non-null    float64\n",
      " 8   openness               422 non-null    float64\n",
      " 9   conscientiousness      422 non-null    float64\n",
      " 10  extraversion           422 non-null    float64\n",
      " 11  neuroticism            422 non-null    float64\n",
      " 12  agree                  422 non-null    int64  \n",
      " 13  openn                  422 non-null    int64  \n",
      " 14  consc                  422 non-null    int64  \n",
      " 15  extra                  422 non-null    int64  \n",
      " 16  neuro                  422 non-null    int64  \n",
      " 17  all_utc                422 non-null    object \n",
      " 18  mean_controversiality  422 non-null    float64\n",
      " 19  mean_gilded            422 non-null    float64\n",
      " 20  num_subreddits         422 non-null    int64  \n",
      " 21  subreddit_dist         422 non-null    object \n",
      " 22  weekday_dist           422 non-null    object \n",
      " 23  month_dist             422 non-null    object \n",
      " 24  year_dist              422 non-null    object \n",
      " 25  all_lang               422 non-null    object \n",
      "dtypes: float64(7), int64(7), object(12)\n",
      "memory usage: 85.8+ KB\n"
     ]
    }
   ],
   "source": [
    "predf.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predf.to_pickle(\"preprocessed_author.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
