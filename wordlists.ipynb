{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from empath import Empath\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "random.seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87 entries, 0 to 86\n",
      "Data columns (total 35 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   index                 87 non-null     int64  \n",
      " 1   author                87 non-null     object \n",
      " 2   body                  87 non-null     object \n",
      " 3   probody               87 non-null     object \n",
      " 4   tokens                87 non-null     object \n",
      " 5   agreeableness         87 non-null     float64\n",
      " 6   openness              87 non-null     float64\n",
      " 7   conscientiousness     87 non-null     float64\n",
      " 8   extraversion          87 non-null     float64\n",
      " 9   neuroticism           87 non-null     float64\n",
      " 10  agree                 87 non-null     int64  \n",
      " 11  openn                 87 non-null     int64  \n",
      " 12  consc                 87 non-null     int64  \n",
      " 13  extra                 87 non-null     int64  \n",
      " 14  neuro                 87 non-null     int64  \n",
      " 15  language              87 non-null     int64  \n",
      " 16  author_flair_text     87 non-null     object \n",
      " 17  downs                 87 non-null     float64\n",
      " 18  created_utc           87 non-null     float64\n",
      " 19  subreddit_id          87 non-null     object \n",
      " 20  link_id               87 non-null     object \n",
      " 21  parent_id             87 non-null     object \n",
      " 22  score                 87 non-null     float64\n",
      " 23  controversiality      87 non-null     float64\n",
      " 24  gilded                87 non-null     float64\n",
      " 25  id                    87 non-null     object \n",
      " 26  subreddit             87 non-null     object \n",
      " 27  ups                   87 non-null     float64\n",
      " 28  word_count            87 non-null     float64\n",
      " 29  word_count_quoteless  87 non-null     float64\n",
      " 30  char_count            87 non-null     int64  \n",
      " 31  stopwords             87 non-null     int64  \n",
      " 32  total_punc            87 non-null     int64  \n",
      " 33  total_num             87 non-null     int64  \n",
      " 34  total_uppercase       87 non-null     int64  \n",
      "dtypes: float64(13), int64(12), object(10)\n",
      "memory usage: 23.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"preprocessed.pkl\")\n",
    "df.head()\n",
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empath\n",
    "\n",
    "as a replacement for LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new categories with empath\n",
    "def new_cat():\n",
    "    empath = Empath()\n",
    "    social = empath.create_category(\"social\",[\"mate\",\"talk\",\"they\"])\n",
    "    humans = empath.create_category(\"humans\",[\"adult\",\"baby\",\"boy\"])\n",
    "    cognitive = empath.create_category(\"cognitive\",[\"cause\",\"know\",\"ought\"])\n",
    "    insight = empath.create_category(\"insight\",[\"think\",\"know\",\"consider\"])\n",
    "    causation = empath.create_category(\"causation\",[\"because\",\"effect\",\"hence\"])\n",
    "    discrepancy = empath.create_category(\"discrepancy\",[\"should\",\"would\",\"could\"])\n",
    "    tentative = empath.create_category(\"tentative\",[\"maybe\",\"perhaps\",\"guess\"])\n",
    "    certainty = empath.create_category(\"certainty\",[\"always\",\"never\", \"proof\"])\n",
    "    inhibition = empath.create_category(\"inhibition\",[\"block\",\"constrain\",\"stop\"])\n",
    "    inclusive = empath.create_category(\"inclusive\",[\"and\",\"with\",\"include\"])\n",
    "    exclusive = empath.create_category(\"exclusive\",[\"but\",\"without\",\"exclude\"])\n",
    "    perceptual = empath.create_category(\"perceptual\",[\"observing\",\"hear\",\"feeling\"])\n",
    "    see = empath.create_category(\"see\",[\"view\",\"saw\",\"seen\"])\n",
    "    feel = empath.create_category(\"feel\",[\"feels\",\"touch\",\"feeling\"])\n",
    "    biological = empath.create_category(\"biological\",[\"eat\",\"blood\",\"pain\"])\n",
    "    relativity = empath.create_category(\"relativity\",[\"area\",\"bend\",\"go\"])\n",
    "    space = empath.create_category(\"space\",[\"down\",\"in\",\"thin\"])\n",
    "    time = empath.create_category(\"time\",[\"end\",\"until\",\"season\"])\n",
    "    agreement = empath.create_category(\"agreement\", [\"agree\", \"ok\", \"yes\"])\n",
    "    fillers = empath.create_category(\"fillers\", [\"like\", \"Imean\", \"yaknow\"])\n",
    "    nonfluencies = empath.create_category(\"nonfluencies\", [\"umm\", \"hm\", \"er\"])\n",
    "    conjunctions = empath.create_category(\"conjunctions\", [\"and\", \"but\", \"whereas\"])\n",
    "    quantifiers = empath.create_category(\"quantifiers\", [\"few\", \"many\", \"much\"])\n",
    "    numbers = empath.create_category(\"numbers\", [\"two\", \"fourteen\", \"thousand\"])\n",
    "    z = empath.analyze(\"I am not thinking\", categories=[\"negations\"], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"talk\", \"mates\", \"mate\", \"Because\", \"friends\", \"anyone\", \"anything\", \"mean\", \"though\", \"anyway\", \"guess\", \"anymore\", \"should\", \"why\", \"knew\", \"someone\", \"trust\", \"wanted\", \"actually\", \"family\", \"anybody\", \"Well\", \"care\", \"parents\", \"knowing\", \"understand\", \"Now\", \"Maybe\", \"else\", \"probably\", \"happen\", \"yet\", \"honestly\", \"maybe\", \"either\", \"If\", \"always\", \"thought\", \"leave\", \"suppose\", \"talk\", \"own_friends\", \"telling\", \"nt\", \"right\", \"either\", \"cause\", \"talking\", \"cause\", \"anyways\"]\n",
      "[\"child\", \"kid\", \"girl\", \"baby\", \"adult\", \"teenager\", \"boy\", \"little_girl\", \"little_boy\", \"young\", \"age\", \"baby_girl\", \"teen\", \"woman\", \"princess\", \"toddler\", \"grown_man\", \"baby_sister\", \"daughter\", \"six_year_old\", \"sister\", \"teenage_girl\", \"newborn\", \"guy\", \"baby_boy\", \"brother\", \"three_year_old\", \"sixteen_year_old\", \"four_year_old\", \"6_year_old\", \"ten_year_old\", \"new_man\", \"one\", \"seven_year_old\", \"person\", \"babies\", \"12_year_old\", \"twelve_year_old\", \"4_year_old\", \"10_year_old\", \"nine_year_old\", \"teenage_boy\", \"little_baby\", \"mother\", \"thing\", \"eighteen_year_old\", \"fourteen_year_old\", \"daddy\", \"lady\", \"Because\", \"grandchild\", \"big_brother\", \"man\", \"14_year_old\", \"someone\", \"female\", \"Now\", \"stranger\", \"old_man\", \"father\", \"old\", \"kids\", \"husband\", \"eight_year_old\", \"little_sister\", \"orphan\", \"wife\", \"sibling\", \"5_year_old\", \"dad\", \"young_adult\", \"old_lady\", \"young_lad\", \"2_year_old\", \"young_lady\", \"eleven_year_old\", \"doll\", \"angel\", \"always\"]\n",
      "[\"cause\", \"suppose\", \"should\", \"Because\", \"might\", \"mean\", \"anyway\", \"cause\", \"If\", \"Maybe\", \"probably\", \"maybe\", \"guess\", \"Or\", \"though\", \"honestly\", \"expect\", \"Now\", \"understand\", \"bet\", \"either\", \"meant\", \"like\", \"wish\", \"anything\", \"unless\", \"actually\", \"anyways\", \"trust\", \"Besides\", \"Honestly\", \"Plus\", \"cause\", \"why\", \"Well\", \"care\", \"anyone\", \"need\", \"seriously\", \"anybody\", \"anymore\", \"thought\", \"either\", \"wanted\", \"anyhow\", \"whatever\", \"afraid\", \"Obviously\", \"Sometimes\", \"nt\", \"better\", \"besides\", \"personally\", \"Now\", \"reason\", \"wo\", \"does\", \"anyways\", \"surely\", \"happen\", \"doubt\", \"otherwise\", \"always\", \"reckon\", \"sometimes\", \"knowing\", \"Otherwise\"]\n",
      "[\"mean\", \"consider\", \"understand\", \"Because\", \"believe\", \"honestly\", \"like\", \"doubt\", \"guess\", \"trust\", \"care\", \"anyway\", \"thought\", \"suppose\", \"actually\", \"though\", \"Honestly\", \"Maybe\", \"maybe\", \"If\", \"probably\", \"blame\", \"agree\", \"Well\", \"expect\", \"bet\", \"seriously\", \"should\", \"Actually\", \"Besides\", \"besides\", \"hate\", \"wonder\", \"Plus\", \"Or\", \"anyways\", \"admit\", \"either\", \"anything\", \"explain\", \"thinks\", \"wish\", \"thing\", \"remember\", \"ask\", \"Obviously\", \"appreciate\", \"Besides\", \"matter\", \"why\", \"judge\", \"imagine\", \"suspect\", \"think\", \"accept\", \"See\", \"remind\", \"might\", \"anyone\", \"Secondly\", \"assume\", \"happen\", \"knowing\", \"forget\", \"anyhow\", \"knows\", \"Not\", \"thinking\", \"seem\", \"talk\", \"question\", \"Trust\", \"warn\"]\n",
      "[\"effect\", \"though\", \"Although\", \"although\", \"though\", \"Though\", \"Because\", \"fact\", \"actually\", \"affect\", \"probably\", \"Obviously\", \"reason\", \"However\", \"especially\", \"only\", \"Yet\", \"Though\", \"considering\", \"obviously\", \"Plus\", \"yet\", \"cause\", \"purpose\", \"either\", \"part\", \"meant\", \"also\", \"knowing\", \"honestly\", \"appeal\", \"yet\", \"cause\", \"either\", \"however\", \"Besides\", \"which\", \"apparently\", \"extent\", \"always\", \"definitely\", \"Apparently\", \"kind\", \"good_thing\", \"otherwise\", \"idea\", \"sometimes\", \"knew\", \"Even\", \"therefore\", \"Maybe\", \"surely\", \"Unfortunately\", \"mean\", \"certainly\", \"If\", \"Honestly\", \"Besides\", \"Which\", \"thing\", \"normally\", \"Clearly\", \"might\", \"seriously\", \"somehow\", \"Knowing\", \"point\"]\n",
      "[\"should\", \"'d\", \"might\", \"wanted\", \"knew\", \"able\", \"possibly\", \"needed\", \"must\", \"though\", \"thought\", \"probably\", \"meant\", \"If\", \"maybe\", \"Maybe\", \"Or\", \"figured\", \"wish\", \"wouldn\\u2019t\", \"suppose\", \"wo\", \"actually\", \"knowing\", \"wanting\", \"enough\", \"wished\", \"anything\", \"otherwise\", \"either\", \"only\", \"Should\", \"hoped\", \"anyway\", \"hoping\", \"willing\", \"surely\", \"though\", \"Now\", \"honestly\", \"anyone\", \"Because\", \"either\", \"chance\", \"ought\", \"couldn\\u2019t\", \"supposed\", \"best\", \"does\", \"better\", \"didn\\u2019t\", \"yet\", \"decided\", \"personally\", \"anymore\", \"any\", \"Would\", \"else\", \"Did\", \"Sometimes\"]\n",
      "[\"guess\", \"well\", \"Well\", \"anyways\", \"suppose\", \"maybe\", \"anyway\", \"anyway\", \"Yeah\", \"Guess\", \"Maybe\", \"actually\", \"anyways\", \"mean\", \"cause\", \"Actually\", \"Yea\", \"probably\", \"though\", \"glad\", \"kinda\", \"kinda\", \"figured\", \"Yea\", \"cause\", \"guess\", \"so\", \"honestly\", \"bet\", \"Because\", \"yeah\", \"should\", \"hope\", \"Plus\", \"because\", \"thought\", \"Nah\", \"why\", \"Okay\", \"anyways\", \"Yup\", \"Sure\", \"sure\", \"good_thing\", \"Anyway\", \"Ok\", \"y'know\", \"anyways\", \"Well\", \"though\", \"cause\", \"sure\", \"sorta\", \"like\", \"whatever\", \"Anyways\", \"Honestly\", \"Now\", \"Hopefully\", \"Obviously\", \"good\", \"Ya\", \".but\", \"dunno\", \"sure\", \"See\", \"anyhow\", \"okay\", \"Yea\", \"So\", \"yea\", \"great\", \"Anyway\", \"guessing\", \"whatever\"]\n",
      "[\"proof\", \"evidence\", \"motive\", \"witnesses\", \"guarantee\", \"claims\", \"witness\", \"sources\", \"fraud\", \"justification\", \"leads\", \"prove\", \"claim\", \"justice\", \"killing\", \"last_resort\", \"doubt\", \"theory\", \"cure\", \"If\", \"proves\", \"evidences\", \"false\", \"significance\", \"enough_evidence\", \"real_danger\", \"hypothesis\", \"redemption\", \"relation\", \"suspect\", \"clues\", \"verify\", \"free_will\", \"suspect\", \"confirm\", \"suspects\", \"slight_chance\", \"otherwise\", \"blackmail\", \"human_life\", \"testimony\", \"Surely\", \"hoax\", \"faults\", \"conspiracy\", \"slim_chance\", \"death_sentence\", \"telling\", \"compromised\", \"Psy\", \"relevance\", \"value\", \"believed\", \"crimes\", \"scheme\", \"enough_proof\", \"potential\", \"Alchemists\", \"error\", \"good_chance\", \"shinigami\", \"benefit\", \"more_evidence\", \"identity\", \"WICKED\", \"consequence\", \"justify\", \"consolation\", \"threat\", \"identification\", \"investigations\", \"other_means\", \"connections\", \"true\", \"restrictions\", \"&acirc;&euro;&brvbar\", \"deception\", \"kidnapping\", \"assure\", \"allegations\", \"warrant\", \"such_thing\", \"eliminate\", \"potential\", \"liability\", \"fatal\", \"Council\", \"PsyNet\", \"theft\", \"the_Holy_Grail\", \"use\", \"saving\", \"conclude\", \"deeds\", \"much_power\", \"single_clue\", \"imply\", \"unless\", \"reason\"]\n",
      "[\"block\", \"stop\", \"keep\", \"ignore\", \"blocked\", \"distract\", \"avoid\", \"prevent\", \"focus\", \"blocking\", \"escape\", \"push\", \"control\", \"try\", \"concentrate\", \"kept\", \"hold\", \"cover\", \"continue\", \"shield\", \"start\", \"back\", \"hear\", \"stand\", \"restrain\", \"help\", \"force\", \"move\", \"catch\", \"hard\", \"dodge\", \"hide\", \"scream\", \"stopping\"]\n",
      "[\"include\", \"included\", \"also\", \"includes\", \"consist\", \"add\", \"hence\", \"Basically\", \"one\", \"Also\", \"involves\", \"either\", \"involve\", \"mainly\", \"part\", \"involving\", \"including\", \"mention\", \"apparently\", \"besides\", \"one\", \"mostly\", \"namely\", \"Along\", \"same_kind\", \"Hence\", \"assume\", \"consists\", \"Except\", \"addition\", \"which\", \"associated\", \"typically\", \"Mostly\", \"having\", \"minus\", \"basically\", \"create\", \"two\", \"obviously\", \"represent\", \"Including\", \"courtesy\", \"contributed\", \"Besides\", \"excluding\", \"most\", \"use\", \"especially\", \"instead\", \"share\", \"course\", \"example\", \"involved\", \"specifically\", \"Probably\", \"different_type\", \"kind\", \"opposed\", \"Plus\", \"both\", \"used\", \"supplied\", \"exception\", \"combined\", \"only\", \"sorts\", \"limited\", \"lastly\", \"listed\", \"plus\", \"Apparently\", \"however\", \"theirs\", \"consisting\"]\n",
      "[\"without\", \"but\", \"simply\", \"else\", \"nothing\", \"either\", \"Yet\", \"only\", \"either\", \"instead\", \"anything\", \"though\", \"any\", \"nor\", \"whatsoever\", \"though\", \"yet\", \"hardly\", \"anymore\", \"Because\", \"Not\", \"anyone\", \"Without\", \"unless\", \"Yet\", \"possibly\", \"knowing\", \"yet\", \"besides\", \"help\", \"one\", \"own\", \"Still\", \"meant\", \"Nor\", \"neither\", \"Though\", \"cause\", \"However\", \"somehow\", \"If\", \"imagine\", \"otherwise\", \"however\", \"obviously\", \"although\", \"more\", \"honestly\", \"Obviously\", \"anybody\", \"risk\", \"alone\", \"seeing\", \"clearly\", \"being\", \"someone\", \"bother\", \"anyway\", \"cause\"]\n",
      "[\"feeling\", \"hearing\", \"seeing\", \"hear\", \"feel\", \"noticing\", \"knowing\", \"sense\", \"notice\", \"heard\", \"making\", \"thinking\", \"observing\", \"watching\", \"noticed\", \"aware\", \"quietly\", \"realizing\", \"sensing\", \"listening\", \"imagining\", \"stand\", \"realize\", \"clearly\", \"suddenly\", \"sensed\", \"though\", \"feeling\", \"seemed\", \"realise\", \"seeming\", \"faint\", \"noting\", \"listened\", \"moment\", \"ignore\", \"doing\", \"quiet\", \"realized\", \"judging\", \"ignoring\", \"keeping\", \"observed\", \"knew\", \"observe\", \"hiding\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"noticed\", \"seen\", \"view\", \"seeing\", \"spotted\", \"sight\", \"saw\", \"found\", \"realized\", \"spied\", \"veiw\", \"appeared\", \"realised\", \"showed\", \"recognized\", \"glimpsed\", \"glimpse\", \"faced\", \"notice\", \"noticing\", \"spot\", \"disappeared\", \"stopped\", \"standing\", \"shown\", \"remembered\", \"front\", \"caught\", \"watched\", \"recognised\", \"figure\", \"spotting\", \"observed\", \"silhouette\", \"clear_view\", \"guessed\", \"near\", \"met\", \"corner\", \"Seeing\", \"witnessed\", \"pictured\", \"passed\", \"approached\", \"entered\", \"first_glimpse\", \"emerged\", \"familiar_face\", \"imagined\", \"stood\", \"notice\", \"dissapeared\", \"before\"]\n",
      "[\"feel\", \"feels\", \"feeling\", \"feeling\", \"touch\", \"felt\", \"touching\", \"numb\", \"touch\", \"touched\", \"Feeling\", \"hurt\", \"feel\", \"sensation\", \"hurting\", \"hurts\", \"felling\", \"touches\", \"burn\", \"own_skin\", \"aching\", \"tingly\", \"weak\", \"body\", \"makes\", \"kiss\", \"pain\", \"tingling\", \"whole_body\", \"warm\", \"knowing\", \"cold\", \"breathe\", \"tingle\", \"heat\", \"own_body\", \"lie\", \"someone\", \"yet\", \"tingling\", \"burning\", \"though\", \"coldness\", \"though\"]\n",
      "[\"blood\", \"pain\", \"burn\", \"more_blood\", \"agony\", \"burns\", \"poison\", \"wound\", \"wounds\", \"hunger\", \"flesh\", \"fresh_blood\", \"own_blood\", \"sting\", \"burning\", \"excruciating_pain\", \"fluids\", \"acid\", \"fluid\", \"black_blood\", \"liquid\", \"body\", \"bleed\", \"water\", \"open_wound\", \"organs\", \"bleeding\", \"numb\", \"monster\", \"little_blood\", \"substance\", \"vomit\", \"innards\", \"stomach\", \"cuts\", \"fear\", \"bled\", \"enough_blood\", \"pains\", \"pain\", \"burning_sensation\", \"bloody_mess\", \"ache\", \"so_much_blood\", \"bones\", \"warm_blood\", \"burning_feeling\", \"energy\", \"more_pain\", \"scratches\", \"burning_pain\", \"aching\", \"thirst\", \"red_liquid\", \"infected\", \"gash\", \"kidneys\", \"infected\", \"bruises\", \"guilt\", \"numbness\", \"hurt\", \"body_parts\", \"marks\", \"horrible_pain\", \"stinging\", \"heat\", \"open_wounds\", \"blisters\", \"veins\", \"medicine\", \"bleeding\", \"unbearable_pain\", \"meat\", \"swelling\", \"internal_organs\", \"pits\", \"anger\", \"slime\", \"intestines\", \"feel\", \"weak\", \"agonizing_pain\", \"immense_pain\", \"burning\", \"hurting\", \"insides\"]\n",
      "[\"move\", \"bend\", \"area\", \"place\", \"reach\", \"stand\", \"so\", \"back\", \"walk\", \"spot\", \"step\", \"pass\", \"drop\", \"start\", \"bring\", \"settle\", \"near\", \"moving\", \"find\", \"places\", \"drag\", \"begin\", \"place\", \"pull\", \"travel\", \"until\", \"rush\", \"areas\", \"set\", \"push\", \"sit\", \"carry\", \"try\", \"head\", \"base\", \"mats\", \"climb\", \"down\", \"turn\", \"bending\", \"rest\", \"bases\", \"fly\", \"order\", \"once\", \"land\", \"slide\", \"right\", \"back\", \"spots\", \"out\", \"perimeter\", \"enter\", \"somewhere\", \"space\", \"continue\", \"leave\"]\n",
      "[\"down\", \"up\", \"across\", \"down\", \"along\", \"small\", \"thin\", \"thick\", \"past\", \"bottom\", \"under\", \"up\", \"straight\", \"underneath\", \"sides\", \"half\", \"above\", \"back\", \"side\", \"below\", \"onto\", \"out\", \"almost\", \"near\", \"off\", \"top\", \"which\", \"tiny\", \"beneath\", \"barely\", \"length\", \"inside\", \"middle\", \"while\", \"covered\", \"covering\", \"circular\", \"slowly\", \"practically\", \"lining\", \"large\", \"sown\", \"against\", \"left_side\", \"skeleton\", \"edge\", \"light\", \"flowing\", \"halfway\", \"full\", \"figure\", \"left\", \"reach\", \"literally\", \"running\", \"one\", \"mass\", \"patches\", \"towards\", \"As\", \"center\"]\n",
      "[\"end\", \"until\", \"till\", \"season\", \"beginning\", \"after\", \"march\", \"day\", \"start\", \"once\", \"only\", \"eventually\", \"part\", \"first\", \"finale\", \"during\", \"finally\", \"Finally\", \"rest\", \"which\", \"Then\", \"seasons\", \"third\", \"Eventually\", \"once\", \"whole\", \"last\", \"one\", \"the_spring\", \"festival\", \"the_third_day\", \"ending\", \"the_end_of_the\", \"spring\", \"summer\", \"last_one\", \"When\", \"cycle\", \"However\", \"the_day\", \"middle\", \"completion\", \"second_half\", \"one\", \"since\", \"very_end\", \"Until\", \"every\", \"scores\", \"place\", \"instead\", \"another\", \"As\", \"point\", \"year\", \"After\", \"so\", \"October\", \"best_part\", \"quarter\", \"Since\", \"countdown\", \"During\", \"first_half\", \"so\", \"already\"]\n",
      "[\"okay\", \"ok\", \"ok\", \"alright\", \"yes\", \"alright\", \"OK\", \"fine\", \"yes\", \"alright\", \"Yes\", \"ok\", \"agree\", \"okay\", \"okay\", \"ok\", \"Yea\", \"ask\", \"fine\", \"alright\", \"Ok\", \"alright\", \"ok\", \"Yea\", \"okay\", \"No\", \"Ofcourse\", \"asked.\\\"No\", \"Yeh\", \"sure\", \"ok\", \"Umm\", \"yeah\", \"OK.\", \"umm\", \"Ok\", \"Okay\", \"Yeah\", \"Sure\", \"Okay\", \"Yup\", \"like\", \"okey\", \"yes\", \"gonna_help\", \"fine\", \"ok\", \"asked.\\\"I\", \"Ok\", \"fine-\", \"Um\", \"ofcourse\", \"wrong\", \"sure\", \"yea\", \"no\", \"no\", \"don't\", \"OK\", \"Fine\", \"well\", \"Fine\", \"Well\", \"problem\", \"mean\", \"Fine\", \"asked.\\\"I\", \"ok\", \"Alright\", \"trust\", \"why\", \"need\", \"err\", \"Ok\", \"Yep\", \"Yup\", \"yep\", \"fine_mom\", \"Okay\", \"Well\", \"Ummm\", \"Nevermind\", \"worry\", \"serious_question\", \"yea\", \"umm\", \"good_thanks\", \"Erm\", \"wan't\", \"Ya\", \"should\", \"true\"]\n",
      "[\"Like\", \"like\", \"though\", \"Because\", \"actually\", \"weird\", \"Like\", \"probably\", \"mean\", \"crazy\", \"always\", \"strange\", \"like\", \"though\", \"normal\", \"literally\", \"normally\", \"horrible\", \"awful\", \"kid\", \"Plus\", \"only\", \"kind\", \"honestly\", \"seriously\", \"bad\", \"Honestly\", \"sometimes\", \"thought\", \"stupid\", \"swear\", \"same\", \"Sometimes\", \"good\", \"bet\", \"Even\", \"terrible\", \"type\", \"Or\", \"different\", \"weird_thing\", \"basically\", \"more\", \"girl\", \"Basically\", \"someone\", \"Besides\", \"odd\", \"maybe\", \"Probably\", \"cause\", \"better\", \"teenager\", \"right\", \"imagine\", \"acting\", \"act\", \"Maybe\", \"nothing\", \"either\"]\n",
      "[\"umm\", \"uhm\", \"um\", \"uhh\", \"ummm\", \"erm\", \"ummm\", \"uh\", \"umm\", \"hm\", \"ummm\", \"erm\", \"err\", \"umm\", \"umm\", \"uhm\", \"uhhh\", \"hm\", \"Ummm\", \"uhh\", \"uhh\", \"uhmm\", \"Umm\", \"U\", \"hmm\", \"hmmm\", \"hmm\", \"uhh\", \"umm\", \"yea\", \"umm\", \"umm\", \"Uhh\", \"umm\", \"uhm\", \"Erm\", \"uhhh\", \"uhm\", \"mm\", \"Ohh\", \"uhmm\", \"hmmmm\", \"Uhhh\", \"erm\", \"yea\", \"Ehm\", \"yea\", \"Ummm\", \"Uhhh\", \"Urm\", \"Umm\", \"uhhh\", \"ummm\", \"errr\", \"ok\", \"uhhhh\", \"uhhh\", \"Um\", \"Uhh\", \"Ummmm\", \"Uhm\", \"Err\", \"yea\", \"Ermm\", \"yeah\", \"uhh\", \"Erm\", \"ummmm\", \"Uhmm\", \"ohh\", \"uhmm\", \"urm\", \"Yea\", \"Erm\", \"serena\", \"yea\", \"okay\", \"Uh\", \"uhh\", \"ermm\", \"hmm\", \"..you\", \"mhm\", \".so\", \"ehm\", \"er\", \"urm\", \".yeah\", \"ye\", \"mhmm\", \"nevermind\", \"waht\", \"ok\", \"yea\", \"Uhhh\", \"Er\", \".what\", \"ummmm\", \"Yeh\", \"Uh-\"]\n",
      "[\"but\", \"whereas\", \"yet\", \"although\", \"however\", \"except\", \"Though\", \"Although\", \"unlike\", \"Yet\", \"However\", \"Except\", \"though\", \"besides\", \"Despite\", \"Unlike\", \"also\", \"only\", \"despite\", \"which\", \"Unfortunately\", \"Whereas\", \"Naturally\", \"especially\", \"nor\", \"always\", \"obviously\", \"Seeing\", \"Surprisingly\", \"simply\", \"undoubtedly\", \"though\", \"apparently\", \"Besides\", \"mostly\", \"either\", \"Even\", \"compared\", \"Yet\", \"Mostly\", \"fact\", \"typically\", \"Apparently\", \"nothing\", \"unfortunately\", \"instead\", \"being\", \"same_kind\", \"somehow\", \"either\", \"seemed\", \"Sadly\", \"normally\", \"seeing\", \"own\", \"hardly\", \"usually\", \"Obviously\", \"generally\", \"one\", \"else\", \"most\", \"Sometimes\", \"Nonetheless\", \"mainly\", \"considering\", \"Usually\", \"Compared\", \"comparison\", \"There\", \"Normally\", \"therefore\", \"Clearly\"]\n",
      "[\"many\", \"few\", \"couple\", \"much\", \"Many\", \"countless\", \"numerous\", \"several\", \"hundred\", \"lot\", \"those\", \"most\", \"all\", \"plenty\", \"a_hundred\", \"often\", \"these\", \"only\", \"at_least_three\", \"at_least_five\", \"Most\", \"much\", \"hundreds\", \"at_least_four\", \"thousand\", \"thousands\", \"at_least_two\", \"far\", \"fair_amount\", \"more\", \"Some\", \"at_least_ten\", \"a_thousand\", \"one\", \"majority\", \"thousand\", \"two\", \"three\", \"Several\", \"only_two\", \"multiple\", \"though\", \"many_people\", \"amount\", \"Often\", \"although\", \"actually\", \"ten\", \"well\", \"before\", \"two\", \"fifty\", \"whole_lot\", \"three\", \"twice\", \"just_a_couple\", \"twenty\", \"about_three\", \"lots\", \"'d\", \"only_a_couple\", \"ones\", \"people\", \"All\", \"dozen\", \"Plenty\", \"Although\", \"five\", \"Though\", \"though\", \"more_than_three\", \"mostly\", \"sometimes\", \"any\", \"tons\", \"million\", \"dozen\", \"only_a_handful\", \"more_than_two\", \"fair_share\", \"Those\", \"billion\"]\n",
      "[\"thousand\", \"gazillion\", \"fourteen\", \"Twenty-one\", \"1,000\", \"more_than_100\", \"Twenty-three\", \"4,000\", \"700\", \"more_than_10\", \"twenty-eight\", \"about_300\", \"five_million\", \"2.5\", \"big_number\", \"1,000,000\", \"thirty-one\", \"20\", \"1-2\", \"twenty_two\", \"30,000\", \"zillion\", \"400\", \"Twenty-two\", \".3\", \"One_hundred\", \"thirty-four\", \"600\", \"500,000\", \"at_least_2\", \"twenty_three\", \"twenty-nine\", \"one_hundred_and_fifty\", \"2k\", \"44\", \"49\", \"8\", \"32\", \"one_million\", \"only_17\", \"3.5\", \"seventy-five\", \"42\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca297ccef72746ffb2a45973677d93a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in new df:  False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>body_x</th>\n",
       "      <th>probody</th>\n",
       "      <th>tokens</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>achievement</th>\n",
       "      <th>leisure</th>\n",
       "      <th>home</th>\n",
       "      <th>money</th>\n",
       "      <th>religion</th>\n",
       "      <th>death</th>\n",
       "      <th>agreement</th>\n",
       "      <th>fillers</th>\n",
       "      <th>nonfluencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sabata11792</td>\n",
       "      <td>That's subtle enough to just look like a coinc...</td>\n",
       "      <td>subtle enough look like</td>\n",
       "      <td>[subtl, enough, look, like]</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>Shadow_Of_</td>\n",
       "      <td>Downturned nose, dirty skin, tattoos, small ch...</td>\n",
       "      <td>downturned dirty small small obvious would not</td>\n",
       "      <td>[downturn, dirti, small, small, obviou, would,...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>xenomouse</td>\n",
       "      <td>Yes, if I was a man they'd call it a man cave....</td>\n",
       "      <td>man would call man guess mouse agree not safe ...</td>\n",
       "      <td>[man, would, call, man, guess, mous, agre, not...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>eiznekk</td>\n",
       "      <td>Added you back! Thank you :D</td>\n",
       "      <td>added thank</td>\n",
       "      <td>[ad, thank]</td>\n",
       "      <td>70.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>vitrael2</td>\n",
       "      <td>I squatted 225x14 a couple weeks ago and I mad...</td>\n",
       "      <td>squatted 225x14 couple weeks ago made sad card...</td>\n",
       "      <td>[squat, 225x14, coupl, week, ago, made, sad, c...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       author                                             body_x  \\\n",
       "0      1  Sabata11792  That's subtle enough to just look like a coinc...   \n",
       "1     21   Shadow_Of_  Downturned nose, dirty skin, tattoos, small ch...   \n",
       "2     34    xenomouse  Yes, if I was a man they'd call it a man cave....   \n",
       "3     37      eiznekk                       Added you back! Thank you :D   \n",
       "4     62     vitrael2  I squatted 225x14 a couple weeks ago and I mad...   \n",
       "\n",
       "                                             probody  \\\n",
       "0                            subtle enough look like   \n",
       "1     downturned dirty small small obvious would not   \n",
       "2  man would call man guess mouse agree not safe ...   \n",
       "3                                        added thank   \n",
       "4  squatted 225x14 couple weeks ago made sad card...   \n",
       "\n",
       "                                              tokens  agreeableness  openness  \\\n",
       "0                        [subtl, enough, look, like]            8.0      11.0   \n",
       "1  [downturn, dirti, small, small, obviou, would,...           76.0      47.0   \n",
       "2  [man, would, call, man, guess, mous, agre, not...           26.0      93.0   \n",
       "3                                        [ad, thank]           70.0      64.0   \n",
       "4  [squat, 225x14, coupl, week, ago, made, sad, c...           26.0      98.0   \n",
       "\n",
       "   conscientiousness  extraversion  neuroticism  ...  work  achievement  \\\n",
       "0               74.0           1.0         25.0  ...   0.0          0.0   \n",
       "1                1.0           4.0         75.0  ...   0.0          0.0   \n",
       "2               49.0          70.0         16.0  ...   0.0          0.0   \n",
       "3                5.0           5.0         95.0  ...   0.0          0.0   \n",
       "4               75.0          93.0         29.0  ...   0.0          0.0   \n",
       "\n",
       "   leisure  home  money  religion death  agreement   fillers nonfluencies  \n",
       "0      0.0   0.0    0.0       0.0   0.0   0.111111  0.111111          0.0  \n",
       "1      0.0   0.0    0.0       0.0   0.0   0.000000  0.000000          0.0  \n",
       "2      0.0   0.0    0.0       0.0   0.0   0.022222  0.000000          0.0  \n",
       "3      0.0   0.0    0.0       0.0   0.0   0.000000  0.000000          0.0  \n",
       "4      0.0   0.0    0.0       0.0   0.0   0.000000  0.050000          0.0  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_empath(df):\n",
    "    empath = Empath()\n",
    "    new_cat()\n",
    "    empathvalues = []\n",
    "    empathcategories = [\"swearing_terms\", \"social\", \"family\", \"friends\", \"humans\", \"emotional\", \"positive_emotion\", \"negative_emotion\", \"fear\", \"anger\", \"sadness\", \"cognitive\", \"insight\", \"causation\", \"discrepancy\", \"tentative\", \"certainty\", \"inhibition\", \"inclusive\", \"exclusive\", \"perceptual\", \"see\", \"hear\", \"feel\", \"biological\", \"body\", \"health\", \"sexual\", \"eat\", \"relativity\", \"space\", \"time\", \"work\", \"achievement\", \"leisure\", \"home\", \"money\", \"religion\", \"death\" ,\"agreement\", \"fillers\", \"nonfluencies\"]\n",
    "    for sentence in tqdm(df['body']):\n",
    "        empathvalues.append(empath.analyze(sentence, categories=empathcategories, normalize=True))\n",
    "    empathdf = pd.DataFrame(empathvalues)\n",
    "    empathdf['author'] = df['author']\n",
    "\n",
    "    newdf = pd.merge(df, empathdf, on='author', how='outer')\n",
    "    return newdf\n",
    "\n",
    "empdf = apply_empath(df)\n",
    "print(\"NaN in new df: \", empdf.isnull().any().any())\n",
    "empdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSYCH Wordlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "concretenessdf = pd.read_csv('/home/sophia/ma_py/psych_lists/concreteness.csv')\n",
    "cdf = concretenessdf[['Conc.M']]\n",
    "cmatrix = cdf.to_numpy()\n",
    "concrete = concretenessdf['Word'].values.tolist()\n",
    "\n",
    "happinessdf = pd.read_csv('/home/sophia/ma_py/psych_lists/happiness_ratings.csv')\n",
    "hdf = happinessdf[['happiness_average']]\n",
    "hmatrix = hdf.to_numpy()\n",
    "happiness = happinessdf['word'].values.tolist()\n",
    "\n",
    "cursedf = pd.read_csv('/home/sophia/ma_py/psych_lists/mean_good_curse.csv')\n",
    "cudf = cursedf[['mean_good_curse']]\n",
    "cumatrix = cudf.to_numpy()\n",
    "curse = cursedf['word'].values.tolist()\n",
    "\n",
    "sensorydf = pd.read_csv('/home/sophia/ma_py/psych_lists/sensory_experience_ratings.csv')\n",
    "serdf = sensorydf[['Average SER']]\n",
    "sermatrix = serdf.to_numpy()\n",
    "ser = sensorydf['Word'].values.tolist()\n",
    "\n",
    "alldf = pd.read_csv('/home/sophia/ma_py/psych_lists/sensory_ratings_all.csv')\n",
    "newalldf = alldf[['Emotion', 'Polarity', 'Social', 'Moral', 'MotionSelf', 'Thought', 'Color', 'TasteSmell', 'Tactile', 'VisualForm', 'Auditory', 'Space', 'Quantity', 'Time', 'CNC', 'IMG', 'FAM']]\n",
    "allmatrix = newalldf.to_numpy()\n",
    "allsens = alldf['Word'].values.tolist()\n",
    "\n",
    "valarodomdf = pd.read_csv('/home/sophia/ma_py/psych_lists/valence_arousal_dominence.csv')\n",
    "vaddf = valarodomdf[['V.Mean.Sum', 'A.Mean.Sum', 'D.Mean.Sum']]\n",
    "vadmatrix = vaddf.to_numpy()\n",
    "vad = valarodomdf['Word'].values.tolist()\n",
    "\n",
    "mrcdf = pd.read_csv('/home/sophia/ma_py/psych_lists/mrclists_c_p.csv', sep='\\t', names=['word', 'cmean', 'pmean'])\n",
    "cpdf = mrcdf[['cmean', 'pmean']]\n",
    "cpmatrix = cpdf.to_numpy()\n",
    "mrc = mrcdf['word'].values.tolist()\n",
    "\n",
    "\n",
    "# num_rows, num_cols = matrix.shape\n",
    "# print (num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(df, vocab):\n",
    "    inputtext = []\n",
    "    for row in df['body_x']:\n",
    "        text = ' '.join(row)\n",
    "        inputtext.append(text)\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,1), vocabulary = vocab)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "# hmatrix = counter(empdf, happiness)\n",
    "# print(type(hmatrix))\n",
    "# print(\"Number of non zero elements: \", np.count_nonzero(hmatrix))\n",
    "# print(type(hmatrix))\n",
    "# num_rows, num_cols = hmatrix.shape\n",
    "# print (num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(matrix, ratings):\n",
    "    # matrix multiplication \n",
    "    result = np.matmul(matrix, ratings)\n",
    "    # divide each score with the number of words in the list to normalize\n",
    "    result = result/(len(ratings))\n",
    "    return result\n",
    "\n",
    "# test = multiply(hdf, hmatrix)\n",
    "# num_rows, num_cols = test.shape\n",
    "# print (num_rows, num_cols)\n",
    "# print(\"Number of non zero elements: \", np.count_nonzero(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregator(df, vocab, ratings, name):\n",
    "    count = counter(df, vocab)\n",
    "    result = multiply(count, ratings)\n",
    "    num_rows, num_cols = result.shape\n",
    "    \n",
    "    if num_cols ==1:\n",
    "        df[name] = result\n",
    "    else:\n",
    "        resultdf = pd.DataFrame(result)\n",
    "        for i in range(len(name)):\n",
    "            # first i is zero\n",
    "            column = name[i]\n",
    "            df[column] = resultdf[i]\n",
    "    return df\n",
    "\n",
    "# psychdf = aggregator(empdf, concrete, \"concreteness\")\n",
    "# psychdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "negations = [\"No\", \"Not\", \"None\", \"Nobody\", \"Nothing\", \"Neither\", \"Nowhere\", \"Never\"]\n",
    "articles = [\"a\", \"an\", \"the\"]\n",
    "future = [\"will\", \"gonna\"]\n",
    "\n",
    "def list_counter(df, vocab, name):\n",
    "    inputtext = []\n",
    "    total = []\n",
    "    for row in empdf['body_x']:\n",
    "        total.append(len(row))\n",
    "        text = ' '.join(row)\n",
    "        inputtext.append(text)\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,1), vocabulary = vocab)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    v = vectors.toarray()\n",
    "    averagev = v.sum(axis=1)\n",
    "    totalvector =  np.array(total)\n",
    "    score = np.divide(averagev, totalvector)\n",
    "    df[name] = score\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fc9901de504b9586f80210a8e28dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69b1898b9914b8b81c19300e8032885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cbddc8fd394fb9a130e3d251a347ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083921c530e64f7dbcf53676f6808a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1eb18e283824cc7bba0ac289824d2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e3a16079d8451d8b96eb6214237849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92567830a8084d83805398b54e42a176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a242d5b282d491ab92d0b3a325934b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749d78f6fe264927b6ebf904a8c78ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e06f5f4860434eb6bd959e17df87ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 199 entries, 0 to 198\n",
      "Data columns (total 103 columns):\n",
      " #    Column                Dtype  \n",
      "---   ------                -----  \n",
      " 0    index                 int64  \n",
      " 1    author                object \n",
      " 2    body_x                object \n",
      " 3    probody               object \n",
      " 4    tokens                object \n",
      " 5    agreeableness         float64\n",
      " 6    openness              float64\n",
      " 7    conscientiousness     float64\n",
      " 8    extraversion          float64\n",
      " 9    neuroticism           float64\n",
      " 10   agree                 int64  \n",
      " 11   openn                 int64  \n",
      " 12   consc                 int64  \n",
      " 13   extra                 int64  \n",
      " 14   neuro                 int64  \n",
      " 15   language              int64  \n",
      " 16   author_flair_text     object \n",
      " 17   downs                 float64\n",
      " 18   created_utc           float64\n",
      " 19   subreddit_id          object \n",
      " 20   link_id               object \n",
      " 21   parent_id             object \n",
      " 22   score                 float64\n",
      " 23   controversiality      float64\n",
      " 24   gilded                float64\n",
      " 25   id                    object \n",
      " 26   subreddit             object \n",
      " 27   ups                   float64\n",
      " 28   word_count            float64\n",
      " 29   word_count_quoteless  float64\n",
      " 30   char_count            int64  \n",
      " 31   stopwords             int64  \n",
      " 32   total_punc            int64  \n",
      " 33   total_num             int64  \n",
      " 34   total_uppercase       int64  \n",
      " 35   swearing_terms        float64\n",
      " 36   social                float64\n",
      " 37   family                float64\n",
      " 38   friends               float64\n",
      " 39   humans                float64\n",
      " 40   emotional             float64\n",
      " 41   positive_emotion      float64\n",
      " 42   negative_emotion      float64\n",
      " 43   fear                  float64\n",
      " 44   anger                 float64\n",
      " 45   sadness               float64\n",
      " 46   cognitive             float64\n",
      " 47   insight               float64\n",
      " 48   causation             float64\n",
      " 49   discrepancy           float64\n",
      " 50   tentative             float64\n",
      " 51   certainty             float64\n",
      " 52   inhibition            float64\n",
      " 53   inclusive             float64\n",
      " 54   exclusive             float64\n",
      " 55   perceptual            float64\n",
      " 56   see                   float64\n",
      " 57   hear                  float64\n",
      " 58   feel                  float64\n",
      " 59   biological            float64\n",
      " 60   body_y                float64\n",
      " 61   health                float64\n",
      " 62   sexual                float64\n",
      " 63   eat                   float64\n",
      " 64   relativity            float64\n",
      " 65   space                 float64\n",
      " 66   time                  float64\n",
      " 67   work                  float64\n",
      " 68   achievement           float64\n",
      " 69   leisure               float64\n",
      " 70   home                  float64\n",
      " 71   money                 float64\n",
      " 72   religion              float64\n",
      " 73   death                 float64\n",
      " 74   agreement             float64\n",
      " 75   fillers               float64\n",
      " 76   nonfluencies          float64\n",
      " 77   concreteness          float64\n",
      " 78   happiness             float64\n",
      " 79   good_curse            float64\n",
      " 80   emotion               float64\n",
      " 81   polarity              float64\n",
      " 82   moral                 float64\n",
      " 83   motionself            float64\n",
      " 84   thought               float64\n",
      " 85   color                 float64\n",
      " 86   tastesmell            float64\n",
      " 87   tactile               float64\n",
      " 88   visualform            float64\n",
      " 89   auditory              float64\n",
      " 90   quantity              float64\n",
      " 91   CNC                   float64\n",
      " 92   IMG                   float64\n",
      " 93   FAM                   float64\n",
      " 94   SER                   float64\n",
      " 95   valence               float64\n",
      " 96   arousal               float64\n",
      " 97   dominance             float64\n",
      " 98   negations             float64\n",
      " 99   mrc_cmean             float64\n",
      " 100  mrc_pmean             float64\n",
      " 101  articles              float64\n",
      " 102  future                float64\n",
      "dtypes: float64(81), int64(12), object(10)\n",
      "memory usage: 161.7+ KB\n"
     ]
    }
   ],
   "source": [
    "def extract_features(df):\n",
    "    # create scores for each word list and add them to df\n",
    "    psychdf = aggregator(df, concrete, cmatrix, \"concreteness\")\n",
    "    psychdf = aggregator(df, happiness, hmatrix, \"happiness\")\n",
    "    psychdf = aggregator(df, curse, cumatrix, \"good_curse\")\n",
    "    psychdf = aggregator(df, allsens, allmatrix, ['emotion', 'polarity', 'social', 'moral', 'motionself', 'thought', 'color', 'tastesmell', 'tactile', 'visualform', 'auditory', 'space', 'quantity', 'time', 'CNC', 'IMG', 'FAM'])\n",
    "    psychdf = aggregator(df, ser, sermatrix, \"SER\")\n",
    "    psychdf = aggregator(df, vad, vadmatrix, ['valence', 'arousal', 'dominance'])\n",
    "    psychdf = list_counter(df, negations, \"negations\")\n",
    "    psychdf = list_counter(df, articles, \"articles\")\n",
    "    psychdf = list_counter(df, future, \"future\")\n",
    "    psychdf = aggregator(df, mrc, cpmatrix, [\"mrc_cmean\", \"mrc_pmean\"])\n",
    "    \n",
    "    return psychdf\n",
    "\n",
    "psychdf = extract_features(empdf)\n",
    "psychdf.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psychdf.to_pickle(\"wordlists.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
