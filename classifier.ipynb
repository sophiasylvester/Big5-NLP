{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "1. Logistic regression (https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions:\n",
    "https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "example:\n",
    "https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-py\n",
    "\n",
    "tut:\n",
    "https://medium.com/@GouthamPeri/pipeline-with-tuning-scikit-learn-b2789dca9dc2\n",
    "\n",
    "https://stackoverflow.com/questions/33376078/python-feature-selection-in-pipeline-how-determine-feature-names\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/svm/plot_svm_anova.html#sphx-glr-auto-examples-svm-plot-svm-anova-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "import random\n",
    "random.seed(32)\n",
    "import sklearn\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, classification_report, precision_recall_curve, roc_auc_score, plot_roc_curve, plot_confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1606 entries, -Areopagan- to zyzee\n",
      "Columns: 103888 entries, body to mrc_pmean\n",
      "dtypes: float64(87790), int64(16092), object(6)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"wordlists_lin_feat_df_withoutuserfeat_allcomments.pkl\")\n",
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictors = df.columns.tolist()\n",
    "# i = 0\n",
    "# predictorsfile=open('predictorslist.txt','w')\n",
    "# # predictorsfile.writelines(predictors)\n",
    "# for element in predictors:\n",
    "#     predictorsfile.write(str(i))\n",
    "#     predictorsfile.write(\" \")\n",
    "#     predictorsfile.write(element)\n",
    "#     predictorsfile.write('\\n')\n",
    "#     i+=1\n",
    "# predictorsfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103888\n",
      "103888\n"
     ]
    }
   ],
   "source": [
    "# multilevel columns\n",
    "\n",
    "lst1 = (5)*[\"data\"]\n",
    "lst9 = (15-5)*[\"traits\"]\n",
    "lst10 = [\"data\"]\n",
    "lst2 = (21-17)*[\"global\"]\n",
    "lst3 = (45-21)*[\"time\"]\n",
    "lst4 = (16103-45)*[\"subreddits\"]\n",
    "lst5 = (16116-16103)*[\"extra_features\"]\n",
    "lst6 = (96308-16116)*[\"word_ngrams\"]\n",
    "lst7 = (103829-96308)*[\"char_ngrams\"]\n",
    "lst8 = (103889-103829)*[\"wordlists\"]\n",
    "headers = lst1 + lst9  + lst10 + lst2 + lst3 + lst4 +lst5 + lst6 + lst7 + lst8 \n",
    "columns = df.columns.values\n",
    "print(len(headers))\n",
    "print(len(columns))\n",
    "arrays = [headers] + [columns]\n",
    "df.columns=pd.MultiIndex.from_arrays(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['traits', 'agree5'] = df['traits', 'agreeableness'].apply(lambda x: 0 if x<20 else(1 if x>19 and x<40 else(2 if x>39 and x<60 else(3 if x>59 and x<80 else 4))))\n",
    "df['traits', 'openn5'] = df['traits', 'openness'].apply(lambda x: 0 if x<20 else(1 if x>19 and x<40 else(2 if x>39 and x<60 else(3 if x>59 and x<80 else 4))))\n",
    "df['traits', 'consc5'] = df['traits', 'conscientiousness'].apply(lambda x: 0 if x<20 else(1 if x>19 and x<40 else(2 if x>39 and x<60 else(3 if x>59 and x<80 else 4))))\n",
    "df['traits', 'extra5'] = df['traits', 'extraversion'].apply(lambda x: 0 if x<20 else(1 if x>19 and x<40 else(2 if x>39 and x<60 else(3 if x>59 and x<80 else 4))))\n",
    "df['traits', 'neuro5'] = df['traits', 'neuroticism'].apply(lambda x: 0 if x<20 else(1 if x>19 and x<40 else(2 if x>39 and x<60 else(3 if x>59 and x<80 else 4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0, how='all')\n",
    "df = df.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1606 entries, -Areopagan- to zyzee\n",
      "Columns: 87829 entries, ('traits', 'agreeableness') to ('wordlists', 'mrc_pmean')\n",
      "dtypes: float64(87788), int64(41)\n",
      "memory usage: 1.1+ GB\n"
     ]
    }
   ],
   "source": [
    "smalldf = df[['traits', 'global', 'time', 'extra_features', 'word_ngrams', 'char_ngrams', 'wordlists']]\n",
    "smalldf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 20\n",
    "\n",
    "def hist_true(df, trait):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.hist(df[trait], bins = 20)\n",
    "    plt.title(trait, y=1.1)\n",
    "    plt.xlabel(\"score\")\n",
    "\n",
    "# hist_true(df, \"openn\")\n",
    "\n",
    "def all_hist_true(df):\n",
    "    plt.figure(figsize = (16, 16))\n",
    "#     plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.hist(df['traits', 'openness'], bins = 20)\n",
    "    plt.title('Agreeableness')\n",
    "    \n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.hist(df['traits', 'conscientiousness'], bins = 20)\n",
    "    plt.title('Openness')\n",
    "    \n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.hist(df['traits', 'extraversion'], bins = 20)\n",
    "    plt.title('Conscientiousness')\n",
    "    \n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.hist(df['traits', 'agreeableness'], bins = 20)\n",
    "    plt.title('Extraversion')\n",
    "    \n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.hist(df['traits', 'neuroticism'], bins = 20)\n",
    "    plt.title('Neuroticism')\n",
    "    \n",
    "    plt.suptitle(\"Histograms of the true trait values\")\n",
    "    plt.subplots_adjust(left=0.1, \n",
    "                    bottom=0.1,  \n",
    "                    right=0.9,  \n",
    "                    top=0.9,  \n",
    "                    wspace=0.4,  \n",
    "                    hspace=0.4) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable depending on which trait to focus on\n",
    "def trait(df, classes, trait_name):\n",
    "    featuredf = df.drop(['data', 'traits'], axis=1, level=0)\n",
    "    feature_cols = featuredf.columns.tolist()\n",
    "    \n",
    "    x = df[feature_cols] \n",
    "    \n",
    "    if classes=='binary':\n",
    "    \n",
    "        if trait_name == 'agreeableness':\n",
    "            y = df['traits', 'agree']\n",
    "        elif trait_name == 'openness':\n",
    "            y = df['traits', 'openn']\n",
    "        elif trait_name == 'conscientiousness':\n",
    "            y = df['traits', 'consc']\n",
    "        elif trait_name == 'extraversion':\n",
    "            y = df['traits', 'extra']\n",
    "        elif trait_name == 'neuroticism':\n",
    "            y = df['traits', 'neuro']   \n",
    "    elif classes=='multi':\n",
    "        if trait_name == 'agreeableness':\n",
    "            y = df['traits', 'agree5']\n",
    "        elif trait_name == 'openness':\n",
    "            y = df['traits', 'openn5']\n",
    "        elif trait_name == 'conscientiousness':\n",
    "            y = df['traits', 'consc5']\n",
    "        elif trait_name == 'extraversion':\n",
    "            y = df['traits', 'extra5']\n",
    "        elif trait_name == 'neuroticism':\n",
    "            y = df['traits', 'neuro5'] \n",
    "    elif classes=='linear':\n",
    "        if trait_name == 'agreeableness':\n",
    "            y = df['traits', 'agreeableness']\n",
    "        elif trait_name == 'openness':\n",
    "            y = df['traits', 'openness']\n",
    "        elif trait_name == 'conscientiousness':\n",
    "            y = df['traits', 'conscientiousness']\n",
    "        elif trait_name == 'extraversion':\n",
    "            y = df['traits', 'extraversion']\n",
    "        elif trait_name == 'neuroticism':\n",
    "            y = df['traits', 'neuroticism']  \n",
    "    return x,y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA feature selection for numeric input and categorical output\n",
    "\n",
    "def create_pipeline(x_train, y_train, classifier, num_feat, weighted):\n",
    "    if weighted==True: \n",
    "        if classifier == \"log\":\n",
    "            pipeline = Pipeline([\n",
    "              ('variance_threshold', VarianceThreshold()),\n",
    "              ('scaler', StandardScaler()),\n",
    "              ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "              ('classification', LogisticRegression(class_weight='balanced', max_iter = 200, n_jobs=-1))\n",
    "            ])\n",
    "        elif classifier == \"multilog\":\n",
    "            pipeline = Pipeline([\n",
    "              ('variance_threshold', VarianceThreshold()),\n",
    "              ('scaler', StandardScaler()),\n",
    "              ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "              \n",
    "              ('classification', LogisticRegression(class_weight='balanced', multi_class='multinomial', \n",
    "                                                    max_iter = 200, solver='lbfgs', n_jobs=-1))\n",
    "            ])\n",
    "        elif classifier == \"svm\":\n",
    "            pipeline = Pipeline([\n",
    "              ('variance_threshold', VarianceThreshold()),\n",
    "              ('scaler', StandardScaler()),\n",
    "              ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "              ('classification', svm.SVC(class_weight='balanced', max_iter = 1000))\n",
    "            ])\n",
    "\n",
    "    else:\n",
    "        if classifier == \"log\":\n",
    "            pipeline = Pipeline([\n",
    "              ('variance_threshold', VarianceThreshold()),\n",
    "              ('scaler', StandardScaler()),\n",
    "              ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "              ('classification', LogisticRegression(max_iter = 200, n_jobs=-1))\n",
    "            ])\n",
    "        elif classifier == \"multilog\":\n",
    "            pipeline = Pipeline([\n",
    "              ('variance_threshold', VarianceThreshold()),\n",
    "              ('scaler', StandardScaler()),\n",
    "              ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "              ('classification', LogisticRegression(multi_class='multinomial', max_iter = 200, solver='lbfgs', \n",
    "                                                    n_jobs=-1))\n",
    "            ])\n",
    "        elif classifier == \"mlp\":\n",
    "            pipeline = Pipeline([\n",
    "              ('variance_threshold', VarianceThreshold()),\n",
    "              ('scaler', StandardScaler()),\n",
    "              ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "              ('classification', MLPClassifier(hidden_layer_sizes=(3,)))\n",
    "            ])\n",
    "        elif classifier == \"svm\":\n",
    "            pipeline = Pipeline([\n",
    "              ('variance_threshold', VarianceThreshold()),\n",
    "              ('scaler', StandardScaler()),\n",
    "              ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "              ('classification', svm.SVC(max_iter = 1000))\n",
    "            ])\n",
    "        elif classifier == \"svmlinear\":\n",
    "            pipeline = Pipeline([\n",
    "              ('variance_threshold', VarianceThreshold()),\n",
    "              ('scaler', StandardScaler()),\n",
    "              ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "              ('classification', svm.LinearSVC(max_iter = 1000))\n",
    "            ])\n",
    "        elif classifier == \"knn\":\n",
    "            pipeline = Pipeline([\n",
    "              ('variance_threshold', VarianceThreshold()),\n",
    "              ('scaler', StandardScaler()),\n",
    "              ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "              ('classification', KNeighborsClassifier(n_neighbors=1, n_jobs=-1))\n",
    "            ])\n",
    "        elif classifier == \"linear\":\n",
    "            pipeline = Pipeline([\n",
    "              ('variance_threshold', VarianceThreshold()),\n",
    "              ('scaler', StandardScaler()),\n",
    "              ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "              ('classification', LinearRegression(n_jobs=-1))\n",
    "            ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_cv(X_train, y_train, classifier, num_feat):\n",
    "    if classifier == \"log\":\n",
    "        pipeline = Pipeline([\n",
    "          ('variance_threshold', VarianceThreshold()),\n",
    "          ('scaler', StandardScaler()),\n",
    "          ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "          ('classification', LogisticRegression())\n",
    "        ])\n",
    "    elif classifier == \"multilog\":\n",
    "        pipeline = Pipeline([\n",
    "          ('variance_threshold', VarianceThreshold()),\n",
    "          ('scaler', StandardScaler()),\n",
    "          ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "          ('classification', LogisticRegression(multi_class='multinomial', n_jobs=-1))\n",
    "        ])\n",
    "    elif classifier == \"mlp\":\n",
    "        pipeline = Pipeline([\n",
    "          ('variance_threshold', VarianceThreshold()),\n",
    "          ('scaler', StandardScaler()),\n",
    "          ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "          ('classification', MLPClassifier())\n",
    "        ])\n",
    "    elif classifier == \"svm\":\n",
    "        pipeline = Pipeline([\n",
    "          ('variance_threshold', VarianceThreshold()),\n",
    "          ('scaler', StandardScaler()),\n",
    "          ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "          ('classification', svm.SVC())\n",
    "        ])\n",
    "    elif classifier == \"svmlinear\":\n",
    "        pipeline = Pipeline([\n",
    "          ('variance_threshold', VarianceThreshold()),\n",
    "          ('scaler', StandardScaler()),\n",
    "          ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "          ('classification', svm.LinearSVC())\n",
    "        ])\n",
    "    elif classifier == \"knn\":\n",
    "        pipeline = Pipeline([\n",
    "          ('variance_threshold', VarianceThreshold()),\n",
    "          ('scaler', StandardScaler()),\n",
    "          ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "          ('classification', KNeighborsClassifier(n_neighbors=1, n_jobs=-1))\n",
    "        ])\n",
    "    elif classifier == \"linear\":\n",
    "        pipeline = Pipeline([\n",
    "          ('variance_threshold', VarianceThreshold()),\n",
    "          ('scaler', StandardScaler()),\n",
    "          ('feature_selection',  SelectKBest(f_classif, k=num_feat)),\n",
    "          ('classification', LinearRegression(n_jobs=-1))\n",
    "        ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get names of 30 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names of the features\n",
    "def get_names(x, pipeline):\n",
    "    features = pipeline.named_steps['feature_selection']\n",
    "    names = x.columns[features.get_support(indices=True)]\n",
    "    return names\n",
    "# names = get_names(logpipe)\n",
    "# print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pvalues(pipeline, x):\n",
    "#     x_indices = np.arange(x.shape[-1])\n",
    "#     selector = SelectKBest(f_classif, k=30)\n",
    "#     selector.fit(x_train, y_train)\n",
    "#     scores = -np.log10(selector.pvalues_)\n",
    "    features = pipeline.named_steps['feature_selection']\n",
    "    pvalues = features.pvalues_\n",
    "#     pvalues /= pvalues.max()\n",
    "    dfpvalues = pd.DataFrame(features.pvalues_)\n",
    "    dfscores = pd.DataFrame(features.scores_)\n",
    "    dfcolumns = pd.DataFrame(x.columns)\n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores, dfpvalues],axis=1)\n",
    "    featureScores.columns = ['specs','score', 'pvalue']\n",
    "    featureScores.sort_values(by='pvalue')\n",
    "\n",
    "    plt.figure(figsize = (12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(pvalues, bins=20)\n",
    "    plt.title('All p-values')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    smallpvalues = pvalues[pvalues<0.1]\n",
    "    plt.hist(smallpvalues, bins=10)\n",
    "    plt.title('Small p-values')\n",
    "    \n",
    "    plt.suptitle(\"Histograms of the p-values\")\n",
    "    plt.subplots_adjust(left=0.1, \n",
    "                    bottom=0.1,  \n",
    "                    right=0.9,  \n",
    "                    top=0.9,  \n",
    "                    wspace=0.4,  \n",
    "                    hspace=0.4) \n",
    "    plt.show()\n",
    "    \n",
    "    return featureScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scores(y_test, y_pred, presentationtype):\n",
    "    \n",
    "    if presentationtype == \"scores\":\n",
    "        accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "        precision=metrics.precision_score(y_test, y_pred)\n",
    "        recall=metrics.recall_score(y_test, y_pred)\n",
    "        f_one=metrics.f1_score(y_test, y_pred)\n",
    "        return accuracy, precision, recall, f_one\n",
    "    if presentationtype == \"report\":\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        return report\n",
    "\n",
    "\n",
    "def score_plot(logreg, y_test, x_test):\n",
    "    lr_probs = logreg.predict_proba(x_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # predict class values\n",
    "#     yhat = logreg.predict(x_test)\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "#     lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "\n",
    "    # plot the precision-recall curves\n",
    "    no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    plt.plot(lr_recall, lr_precision, marker='.', label='Classifier')\n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return lr_precision, lr_recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score plot\n",
    "Smaller values on the x-axis of the plot indicate lower false positives and higher true negatives.\n",
    "\n",
    "\n",
    "Larger values on the y-axis of the plot indicate higher true positives and lower false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and print confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnfmatrix(clf, x_test, y_test, y_pred, plotting, detailed):\n",
    "    cnfpipe_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "#     print(cnfpipe_matrix)\n",
    "#     disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cnfpipe_matrixcmap=plt.cm.Blues, normalize=normalize)\n",
    "#     disp.plot() \n",
    "    if detailed:\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n",
    "        sumpositive = tp + fn\n",
    "        sumnegative = fp + tn\n",
    "        sumcorrect = tp + tn\n",
    "        sumwrong = fp + fn\n",
    "        sumall = tn+fp+fn+tp\n",
    "        print(\"TN, FP, FN, TP: \", tn, fp, fn, tp, \"\\nSum: \", sumall, \"\\nSum correct predictions: \", \n",
    "              sumcorrect, \"Percent: \", sumcorrect/sumall, \"\\nSum wrong predictions: \", sumwrong, \"\\tPercent: \",\n",
    "              sumwrong/sumall, \"\\nSum actual positives: \", sumpositive, \"\\tPercent: \", sumpositive/sumall,\n",
    "              \"\\nSum actual negatives: \", sumnegative, \"\\tPercent: \", sumnegative/sumall)\n",
    "\n",
    "    if plotting:\n",
    "#         %matplotlib inline\n",
    "#         class_names=[0,1] # name  of classes\n",
    "#         fig, ax = plt.subplots()\n",
    "#         tick_marks = np.arange(len(class_names))\n",
    "#         plt.xticks(tick_marks, class_names)\n",
    "#         plt.yticks(tick_marks, class_names)\n",
    "#         # create heatmap\n",
    "#         sns.heatmap(pd.DataFrame(cnfpipe_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "#         ax.xaxis.set_label_position(\"bottom\")\n",
    "#         disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cnfpipe_matrix, cmap=plt.cm.Blues, normalize=normalize)\n",
    "#         disp.plot() \n",
    "        plot_confusion_matrix(clf, x_test, y_test,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize='true')\n",
    "#         disp.ax_.set_title('Confusion matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.title('Confusion matrix', y=1.1)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "#         disp.plot() \n",
    "        plt.show()\n",
    "        \n",
    "# cnfmatrix = create_cnfmatrix(y_test, y_pred, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_aucscore(clf, x_test, y_test, classes, plotting, detailed):\n",
    "    if detailed:\n",
    "        print(roc_auc_score(y, clf.predict_proba(x), multi_class='ovo'))\n",
    "        return score\n",
    "    \n",
    "    if plotting and classes == 'binary':\n",
    "        plot_roc_curve(clf, x_test, y_test)\n",
    "        plt.title('ROC Curve', y=1.1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper\n",
    "\n",
    "\n",
    "nested stratified cv:\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py\n",
    "\n",
    "https://weina.me/nested-cross-validation/\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/nested-cross-validation-hyperparameter-optimization-and-model-selection-5885d84acda\n",
    "https://gist.github.com/krsatyam1996/9640ed8baa20d3dc11822564710a8d71#file-nested_cv-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "\n",
    "def switching(trait):\n",
    "    switcher={\n",
    "            'openness':'all',\n",
    "            'conscientiousness':'all',\n",
    "            'agreeableness':'all',\n",
    "            'extraversion':'all',\n",
    "            'neuroticism':'all'\n",
    "         }\n",
    "    return switcher.get(trait,\"Invalid\")\n",
    "\n",
    "def classify(df, classifier, classes, trainscores=False, plotting = False, weighted = False, detailed=False):\n",
    "    for trait_name in traits:\n",
    "        num_feat = switching(trait_name)\n",
    "        print(\"Trait to predict: \", trait_name)\n",
    "        x,y = trait(df, classes,trait_name)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "#         scores = cross_val_scores(pipeline,X_train,y_train,cv=5,scoring='f1_macro')\n",
    "        if trainscores:\n",
    "            x_test = x_train\n",
    "            y_test = y_train\n",
    "        if detailed: \n",
    "            print(\"Number of authors in y_train: \", len(y_train))\n",
    "            print(\"Number of authors in y_test: \", len(y_test))\n",
    "        clf = create_pipeline(x_train, y_train, classifier, num_feat, weighted)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred=clf.predict(x_test)\n",
    "        if classes=='linear':\n",
    "            print(\"Score (Reg: r sqared, SVM: accuracy): \", clf.score(x_test, y_test))\n",
    "#             print(y_pred)    \n",
    "        else:\n",
    "            if detailed:\n",
    "                print(\"Number of authors in y_pred: \", len(y_pred))\n",
    "                names = get_names(x, clf)\n",
    "                print(\"Names of the top\", len(names), \"features: \\n\", names, \"\\n\")\n",
    "                pvalues = get_pvalues(clf, x)\n",
    "            #     print(\"p-values of\", len(pvalues), \"features: \\n\", pvalues, \"\\n\")\n",
    "                if trait_name==\"openness\":\n",
    "                    count = pvalues['pvalue'].le(0.02).sum()\n",
    "                if trait_name==\"conscientiousness\":\n",
    "                    count = pvalues['pvalue'].le(0.07).sum()\n",
    "                if trait_name==\"extraversion\":\n",
    "                    count = pvalues['pvalue'].le(0.05).sum()\n",
    "                if trait_name==\"agreeableness\":\n",
    "                    count = pvalues['pvalue'].le(0.04).sum()\n",
    "                if trait_name==\"neuroticism\":\n",
    "                    count = pvalues['pvalue'].le(0.04).sum()\n",
    "                print(\"Number of features with this threshold: \", count)\n",
    "        #         print(\"\\nP-Values: \\nNumber of features: \", 30)\n",
    "        #         print(pvalues.nsmallest(count,'pvalue'))\n",
    "                print(\"\\n\")\n",
    "            report = scores(y_test, y_pred, \"report\")\n",
    "            print(\"Classification report: \\n\", report)\n",
    "            if plotting: \n",
    "                cnfmatrix = create_cnfmatrix(clf, x_test, y_test, y_pred, plotting, detailed) \n",
    "                rocplot = roc_aucscore(clf, x_test, y_test, classes, plotting, detailed)\n",
    "        #     accuracy, precision, recall, f_one = scores(y_test, y_pred, \"scores\")\n",
    "        #     print(\"Scores:\\nAccuracy:\",accuracy, \"\\nPrecision:\",precision, \"\\nRecall:\",recall, \"\\nF1 score:\",f_one)\n",
    "                if (classifier == 'log' and classes == 'binary'):\n",
    "                    lr_precision, lr_recall = score_plot(clf, y_test, x_test)\n",
    "            plt.show()\n",
    "    #     print(\"Scores:\\nLR_Precision:\",lr_precision, \"\\nLR_Recall:\",lr_recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper with nested stratified cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(classifier):\n",
    "    if classifier == 'log':\n",
    "        params = {'classification__class_weight': [None, 'balanced'], \n",
    "                  'classification__solver': ['lbfgs', 'liblinear', 'saga'], \n",
    "                  'classification__max_iter': [100, 200, 500, 1000]}\n",
    "    if classifier == 'multilog':\n",
    "        params = {'classification__class_weight': [None, 'balanced'], \n",
    "                  'classification__solver': ['lbfgs', 'saga'], \n",
    "                  'classification__max_iter': [100, 200, 500, 1000]}\n",
    "    elif classifier == 'mlp':\n",
    "        params = {'classification__hidden_layer_sizes': [(3,), (5,)]}\n",
    "    elif classifier == 'svm':\n",
    "        params = {'classification__gamma': ['scale', 'auto'], \n",
    "                  'classification__class_weight': [None, 'balanced'], \n",
    "                  'classification__max_iter': [100, 200, 500, 1000]}\n",
    "    return params\n",
    "\n",
    "\n",
    "def classify_cv(df, classifier, classes):   \n",
    "    for trait_name in tqdm(traits):\n",
    "        num_feat = switching(trait_name)\n",
    "        print(\"\\n\\nTrait to predict: \", trait_name, \"\\n\\n\")\n",
    "        x,y = trait(df, classes, trait_name)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "    \n",
    "        cv_outer = StratifiedKFold(n_splits=5)\n",
    "        cv_outer_lst = cv_outer.split(X_train, y_train)\n",
    "\n",
    "        for train_idx, val_idx in tqdm(cv_outer_lst):\n",
    "            train_data, val_data = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            train_target, val_target = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "\n",
    "            print(\"\\n\\tCreate pipeline with\", classifier, \"...\")\n",
    "            clf = create_pipeline_cv(X_train, y_train, classifier, num_feat)\n",
    "            cv_inner = StratifiedKFold(n_splits=5)\n",
    "            params = get_params(classifier)\n",
    "            print(\"\\tStart grid search...\")\n",
    "            t0 = time()\n",
    "            gd_search = GridSearchCV(clf, params, scoring='f1_macro', n_jobs=-1, cv=cv_inner).fit(train_data, train_target)\n",
    "            print(\"\\tGrid search done in %0.3fs\" % (time() - t0))\n",
    "            print(\"\\tGet best model...\")\n",
    "            best_model = gd_search.best_estimator_\n",
    "            print(best_model)\n",
    "\n",
    "            print(\"\\tFit best model...\")\n",
    "            clfnew = best_model.fit(train_data, train_target)\n",
    "            y_pred_prob = clfnew.predict_proba(val_data)[:,1]\n",
    "            y_pred = clfnew.predict(val_data)\n",
    "            f1_macro = f1_score(val_target, y_pred, average='macro')\n",
    "\n",
    "            print(\"Val Acc:\",f1_macro , \"Best GS Acc:\",gd_search.best_score_, \"Best Params:\",gd_search.best_params_)\n",
    "\n",
    "\n",
    "#       # Training final model\n",
    "\n",
    "#     model = LogisticRegression(random_state=7, C=0.001, class_weight='balanced', penalty='l2').fit(X_train, y_train)\n",
    "#     y_pred_prob = model.predict_proba(X_test)[:,1]\n",
    "#     print(\"AUC\", metrics.roc_auc_score(y_test, y_pred_prob))\n",
    "#     print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27267078a6e4e7ea805d2ff4e64eb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Trait to predict:  openness \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f16b1877dc432e9eefb2810c45f578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n",
      "\t Grid search done in 310.670s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification', LogisticRegression(solver='liblinear'))])\n",
      "\tFit best model...\n",
      "Val Acc: 0.5514646382909857 Best GS Acc: 0.55036819543346 Best Params: {'classification__class_weight': None, 'classification__max_iter': 100, 'classification__solver': 'liblinear'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 305.504s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.5137996297913936 Best GS Acc: 0.529393334170043 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 100, 'classification__solver': 'lbfgs'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n",
      "\t Grid search done in 312.968s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification', LogisticRegression(solver='liblinear'))])\n",
      "\tFit best model...\n",
      "Val Acc: 0.5376570332322546 Best GS Acc: 0.5391625361725126 Best Params: {'classification__class_weight': None, 'classification__max_iter': 100, 'classification__solver': 'liblinear'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 386.812s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=500,\n",
      "                                    solver='saga'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.4972220392208534 Best GS Acc: 0.5385393198732414 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 500, 'classification__solver': 'saga'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 460.345s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000,\n",
      "                                    solver='saga'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.5238095238095238 Best GS Acc: 0.5335258583136195 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 1000, 'classification__solver': 'saga'}\n",
      "\n",
      "\n",
      "Trait to predict:  conscientiousness \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e359f1170042b3b7e1649c2abe8f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 458.488s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000,\n",
      "                                    solver='saga'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.4647055636816729 Best GS Acc: 0.5412422574207272 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 1000, 'classification__solver': 'saga'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 295.785s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.47548879691736834 Best GS Acc: 0.5419309758615778 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 100, 'classification__solver': 'lbfgs'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n",
      "\t Grid search done in 313.903s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced',\n",
      "                                    solver='liblinear'))])\n",
      "\tFit best model...\n",
      "Val Acc: 0.46508159170410357 Best GS Acc: 0.5068945389954351 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 100, 'classification__solver': 'liblinear'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 295.134s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.5053366174055829 Best GS Acc: 0.49631713563178737 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 100, 'classification__solver': 'lbfgs'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 454.952s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(max_iter=1000, solver='saga'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.4675301313185825 Best GS Acc: 0.5069178407814906 Best Params: {'classification__class_weight': None, 'classification__max_iter': 1000, 'classification__solver': 'saga'}\n",
      "\n",
      "\n",
      "Trait to predict:  extraversion \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539d2e9d612d4c48943e67f0926e66e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 298.721s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification', LogisticRegression(solver='saga'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.5796936609038519 Best GS Acc: 0.5292681783489075 Best Params: {'classification__class_weight': None, 'classification__max_iter': 100, 'classification__solver': 'saga'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 443.614s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000,\n",
      "                                    solver='saga'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.5228649068322981 Best GS Acc: 0.5090691166465735 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 1000, 'classification__solver': 'saga'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n",
      "\t Grid search done in 306.634s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced',\n",
      "                                    solver='liblinear'))])\n",
      "\tFit best model...\n",
      "Val Acc: 0.5053366174055829 Best GS Acc: 0.5347280239715456 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 100, 'classification__solver': 'liblinear'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 294.489s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification', LogisticRegression())])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.47572758451739344 Best GS Acc: 0.5188261942117076 Best Params: {'classification__class_weight': None, 'classification__max_iter': 100, 'classification__solver': 'lbfgs'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 297.971s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification', LogisticRegression())])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.4987732922403131 Best GS Acc: 0.5470302758790675 Best Params: {'classification__class_weight': None, 'classification__max_iter': 100, 'classification__solver': 'lbfgs'}\n",
      "\n",
      "\n",
      "Trait to predict:  agreeableness \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315c4258d0044ce7b13fe5bbef9d6923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n",
      "\t Grid search done in 300.029s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced',\n",
      "                                    solver='liblinear'))])\n",
      "\tFit best model...\n",
      "Val Acc: 0.5517616507580011 Best GS Acc: 0.5116488196339418 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 100, 'classification__solver': 'liblinear'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 313.984s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=200,\n",
      "                                    solver='saga'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.5082254365530816 Best GS Acc: 0.5142845989823016 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 200, 'classification__solver': 'saga'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 448.776s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000,\n",
      "                                    solver='saga'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.5083016744055157 Best GS Acc: 0.5345825566009682 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 1000, 'classification__solver': 'saga'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 306.934s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced', solver='saga'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.5153110298875501 Best GS Acc: 0.5334018646729548 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 100, 'classification__solver': 'saga'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 354.230s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=500,\n",
      "                                    solver='saga'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.48772219332388655 Best GS Acc: 0.5358067284269417 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 500, 'classification__solver': 'saga'}\n",
      "\n",
      "\n",
      "Trait to predict:  neuroticism \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0efa94089e48549573e9ed9e67bc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 292.186s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification', LogisticRegression())])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.4800386626853307 Best GS Acc: 0.5131307759445545 Best Params: {'classification__class_weight': None, 'classification__max_iter': 100, 'classification__solver': 'lbfgs'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 453.189s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000,\n",
      "                                    solver='saga'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.5136855176695814 Best GS Acc: 0.5105812973241244 Best Params: {'classification__class_weight': 'balanced', 'classification__max_iter': 1000, 'classification__solver': 'saga'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 290.749s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification', LogisticRegression())])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.5128789841927961 Best GS Acc: 0.49406043837507063 Best Params: {'classification__class_weight': None, 'classification__max_iter': 100, 'classification__solver': 'lbfgs'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n",
      "\t Grid search done in 301.933s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification', LogisticRegression(solver='liblinear'))])\n",
      "\tFit best model...\n",
      "Val Acc: 0.5139872795904649 Best GS Acc: 0.5079274925602124 Best Params: {'classification__class_weight': None, 'classification__max_iter': 100, 'classification__solver': 'liblinear'}\n",
      "\n",
      "\tCreate pipeline with log ...\n",
      "\tStart grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Grid search done in 366.299s\n",
      "\tGet best model...\n",
      "Pipeline(steps=[('variance_threshold', VarianceThreshold()),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('feature_selection', SelectKBest(k='all')),\n",
      "                ('classification',\n",
      "                 LogisticRegression(max_iter=500, solver='saga'))])\n",
      "\tFit best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophia/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Acc: 0.4874199093608376 Best GS Acc: 0.5162688685748177 Best Params: {'classification__class_weight': None, 'classification__max_iter': 500, 'classification__solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "classify_cv(df, 'log', 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of true traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_imbalance(df, traits):\n",
    "    length = len(df)\n",
    "    o = df['traits', 'openn']\n",
    "    c = df['traits', 'consc']\n",
    "    e = df['traits', 'extra']\n",
    "    a = df['traits', 'agree']\n",
    "    n = df['traits', 'neuro']\n",
    "    binarylst = [o, c, e, a, n]\n",
    "    o5 = df['traits', 'openn5']\n",
    "    c5 = df['traits', 'consc5']\n",
    "    e5 = df['traits', 'extra5']\n",
    "    a5 = df['traits', 'agree5']\n",
    "    n5 = df['traits', 'neuro5']\n",
    "    multilst = [o5, c5, e5, a5, n5]\n",
    "    \n",
    "    result = []\n",
    "    for trait in binarylst: \n",
    "        result.append(np.bincount(trait) / length)\n",
    "    result5 = []\n",
    "    for trait in multilst:\n",
    "        result5.append(np.bincount(trait) / len(trait))\n",
    "    \n",
    "    print(\"Distribution of the true trait values in the classes (in %):\\n\")\n",
    "    for i in range(len(traits)):\n",
    "        print(traits[i], \"\\n\\tBinary: \", result[i], \"\\n\\t5 classes: \", result5[i], \"\\n\")\n",
    "    \n",
    "#     result =np.bincount(o) / len(o)\n",
    "#     result5 =np.bincount(o5) / len(o)\n",
    "#     print(\"Openness\\n\\tBinary: \", result, \"\\n\\t5 classes: \", result5)\n",
    "\n",
    "    \n",
    "check_imbalance(df, traits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true histogram plots\n",
    "all_hist_true(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# IPython.OutputArea.auto_scroll_threshold = 1000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate G&S2018R with k=30 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check results on train set with k nearest neighbor algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(df, 'knn', 'binary', trainscores=True, plotting=False, weighted=False, detailed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(df, 'knn', 'multi', trainscores=True, plotting=False, weighted=False, detailed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(df, 'log', 'binary', plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(df, 'log', 'binary', plotting=True, weighted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(df, 'multilog', 'multi', plotting=True, weighted=True, detailed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(df, 'mlp', 'binary', plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(df, 'mlp', 'multi', plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(df, 'svm', 'binary', plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(df, 'svm', 'multi', plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(df, 'svmlinear', 'linear', plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(df, 'linear', 'linear', plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results without predictor subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  binary log (without subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(smalldf, 'log', 'binary', plotting=True, weighted=True, detailed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi log (without subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(smalldf, 'multilog', 'multi', plotting=True, weighted=True, detailed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (without subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(smalldf, 'mlp', 'binary', plotting=True, weighted=False, detailed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (without subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(smalldf, 'lin', 'linear', plotting=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
