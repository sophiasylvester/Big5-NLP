{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and feature engineering with memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sophia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import bigrams, ngrams\n",
    "\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "from empath import Empath\n",
    "\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from lexicalrichness import LexicalRichness\n",
    "import textblob\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "random.seed(32)\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "from pympler import tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import dataset with comments (big five labels)\n",
    "# df = pd.read_csv('/home/sophia/ma_py/pandora_bigfive.csv')\n",
    "\n",
    "# Import augmented data with B5 labels\n",
    "comments = pd.read_pickle(\"pandora_b5_deter.pkl\")\n",
    "\n",
    "# Import dataset authors and delete not needed columns (big five labels)\n",
    "authors = pd.read_csv('/home/sophia/ma_py/author_profiles.csv')\n",
    "bigfive = authors[['author','agreeableness','openness','conscientiousness','extraversion','neuroticism']]\n",
    "bigfive = bigfive[bigfive['openness'].notna()]\n",
    "bigfive = bigfive[bigfive['conscientiousness'].notna()]\n",
    "bigfive = bigfive[bigfive['extraversion'].notna()]\n",
    "bigfive = bigfive[bigfive['agreeableness'].notna()]\n",
    "bigfive = bigfive[bigfive['neuroticism'].notna()]\n",
    "del authors\n",
    "\n",
    "# Datasets with mbti und big five labels\n",
    "# df = pd.read_pickle(\"comments_uniondf.pkl\")\n",
    "# bigfive = pd.read_pickle(\"uniondf.pkl\")\n",
    "# bigfive\n",
    "\n",
    "# remember to change name of output as well!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "traitlen = len(bigfive.columns.tolist())\n",
    "# minus 1 because of author column\n",
    "traitlen = traitlen-1 \n",
    "\n",
    "# create time columns from UTC\n",
    "def create_timecolumns(df):\n",
    "    readable = []\n",
    "    weekday = []\n",
    "    month = []\n",
    "    year = []\n",
    "    hour = []\n",
    "    for row in tqdm(df['created_utc']):\n",
    "        item = datetime.datetime.fromtimestamp(row)\n",
    "        weekday_item = item.strftime('%A')\n",
    "        readable_item = datetime.datetime.fromtimestamp(row).isoformat()\n",
    "        month.append(str(readable_item[5:7]))\n",
    "        year.append(str(readable_item[0:4]))\n",
    "        hour.append(str(readable_item[11:13]))\n",
    "        readable.append(readable_item)\n",
    "        weekday.append(weekday_item.lower())\n",
    "    df['time'] = readable\n",
    "    df['weekday'] = weekday\n",
    "    df['month'] = month\n",
    "    df['year'] = year\n",
    "    df['daily'] = hour\n",
    "    return df\n",
    "\n",
    "# count occurences in time columns to get time distribution\n",
    "def timecounter(lst, vocablst):\n",
    "    if vocablst == 'weekday':\n",
    "        vocab = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    elif vocablst == 'month':\n",
    "        vocab = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    elif vocablst == 'hour':\n",
    "        vocab = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', \n",
    "                 '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24']\n",
    "    elif vocablst == 'year':\n",
    "        vocab = ['2015', '2016', '2017', '2018', '2019']\n",
    "    else:\n",
    "        print(\"No valid input: vocab list\")\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", vocabulary=vocab)\n",
    "    vectors = vectorizer.fit_transform(lst)\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "def timeinterval(lst):\n",
    "    max_lst = []\n",
    "    median_lst = []\n",
    "    mean_lst = []\n",
    "    for sublst in lst:\n",
    "        intlst = [int(item) for item in sublst]\n",
    "        if len(intlst) > 1:\n",
    "            intlst.sort()\n",
    "            arr = np.array(intlst)\n",
    "            diff_lst = np.diff(arr)\n",
    "            max_lst.append(max(diff_lst))\n",
    "            median_lst.append(np.median(diff_lst))\n",
    "            mean_lst.append(np.mean(diff_lst))\n",
    "        else:\n",
    "            max_lst.append(-1)\n",
    "            median_lst.append(-1)\n",
    "            mean_lst.append(-1)\n",
    "    return mean_lst, median_lst, max_lst\n",
    "\n",
    "# create a list of all subreddits in the dataset\n",
    "lst = comments['subreddit'].tolist()\n",
    "lst = [item.lower() for item in lst]\n",
    "subredditset = set(lst)\n",
    "subredditlist = list(subredditset)\n",
    "subredditlength = len(subredditlist)\n",
    "\n",
    "# count occurences of subreddits \n",
    "def subredditcounter(lst, subredditlst):\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", vocabulary=subredditlist)\n",
    "    vectors = vectorizer.fit_transform(lst)\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "# aggregate dataset to get one row per author and create new columns for time and subreddit\n",
    "def create_groupdf(df): \n",
    "#     print(\"\\tCreate numeric language representation...\")\n",
    "#     df = numeric_lang(df)\n",
    "    print(\"\\tCreate time columns...\")\n",
    "    df = create_timecolumns(df)\n",
    "    # create dictionary for aggregation function\n",
    "    d = {'lang': ['nunique'], 'ratio_en': (lambda x : list(x)), 'controversiality': ['mean'], 'gilded': ['mean'], 'score':['mean'],\n",
    "         'body': (' '. join), 'doc_body': (lambda x : list(x)),\n",
    "         'utc': (lambda x : list(x)), 'subreddit': (' '. join), 'num_subreddit': ['nunique'],\n",
    "         'weekday': (' '. join), 'month': (' '. join), 'year': (' '. join), 'daily': (' '. join)}\n",
    " \n",
    "    # new ungrouped columns\n",
    "    print(\"\\tCreate new ungrouped columns...\")\n",
    "    df['body'] = df['body'].apply(lambda x: str(x))\n",
    "    df['doc_body'] = df['body']\n",
    "    df['num_subreddit'] = df['subreddit']\n",
    "#     df['lang'] = df['language'].apply(lambda x: str(x))\n",
    "    df['ratio_en'] = df['lang']\n",
    "    df['utc'] = df['created_utc'].apply(lambda x: str(x))\n",
    "#     df['subreddit'] = df['subreddit'].apply(lambda x: [x.lower()])\n",
    "    df['subreddit'] = df['subreddit'].apply(lambda x: ''.join(x.lower()))\n",
    "    counts = df['author'].value_counts()\n",
    "    ndf = pd.DataFrame(counts)\n",
    "    ndf.reset_index(inplace=True)\n",
    "    ndf.rename(columns = {'index':'author', 'author': 'n_comments'}, inplace = True)\n",
    "    \n",
    "    # create df groupd by author + transform\n",
    "    print(\"\\tGroup df by author...\")\n",
    "    groupdf = df.groupby(['author']).agg(d)\n",
    "    groupdf = groupdf.reset_index()\n",
    "    groupdf.columns = groupdf.columns.droplevel(1)\n",
    "    groupdf.merge(ndf, left_on='author', right_on='author')\n",
    "    \n",
    "    return groupdf\n",
    "    \n",
    "def create_new_columns(df):    \n",
    "    # controversiality\n",
    "    print(\"\\tCreate controversiality column...\")\n",
    "    df['controversiality'] = df['controversiality'].fillna(0)\n",
    "    # gilded\n",
    "    print(\"\\tCreate mean_gilded...\")\n",
    "    df['gilded'] = df['gilded'].fillna(0)\n",
    "    # ratio of english comments\n",
    "    newcolumn = []\n",
    "    for row in df['ratio_en']:\n",
    "        other = [value for value in row if value != 'en']\n",
    "        english = row.count('en')\n",
    "        if len(other) == 0:\n",
    "            newcolumn.append(1)\n",
    "        else: \n",
    "            newcolumn.append(english/len(other))\n",
    "    df['ratio_en'] = newcolumn\n",
    "    # number of comments per subreddit\n",
    "    print(\"\\tCreate subreddit_dist...\")\n",
    "    subreddit_predist = subredditcounter(df['subreddit'], subredditlist)\n",
    "    subreddit_predist = subreddit_predist.tolist()\n",
    "    df['subreddit_dist'] = subreddit_predist\n",
    "    # entropy\n",
    "    df['entropy'] = df['subreddit_dist'].apply(lambda x: entropy(x, base=2))\n",
    "    # time\n",
    "    print(\"\\tCompute time intervals...\")\n",
    "    df['mean_time'], df['median_time'], df['max_time'] = timeinterval(df['utc'])\n",
    "    print(\"\\tCreate weekday_dist...\")\n",
    "    weekday = timecounter(df['weekday'], 'weekday')\n",
    "    weekday = weekday.tolist()\n",
    "    df['weekday_dist'] = weekday\n",
    "    print(\"\\tCreate month_dist...\")\n",
    "    month = timecounter(df['month'], 'month')\n",
    "    month = month.tolist()\n",
    "    df['month_dist'] = month\n",
    "    print(\"\\tCreate year_dist...\")\n",
    "    year = timecounter(df['year'], 'year')\n",
    "    year = year.tolist()\n",
    "    df['year_dist'] = year\n",
    "    print(\"\\tCreate day_dist...\")\n",
    "    day = timecounter(df['daily'], 'hour')\n",
    "    day = day.tolist()\n",
    "    df['daily_dist'] = day\n",
    "    \n",
    "    print(\"\\tCreate new aggregated df...\")\n",
    "    newdf = df[['author', 'body', 'doc_body', 'utc', 'score', 'controversiality', \n",
    "                'gilded', 'ratio_en', 'num_subreddit', 'subreddit_dist', 'entropy', 'mean_time', 'median_time', 'max_time', 'weekday_dist', \n",
    "                'month_dist', 'year_dist', 'daily_dist', 'lang']]\n",
    "    print(\"\\tSort new aggregated df...\")\n",
    "    newdf = newdf.sort_values(by='author')\n",
    "    print(\"\\tDrop duplicates in new aggregated df...\")\n",
    "    newdf = newdf.drop_duplicates(subset=['author'])\n",
    "    return newdf\n",
    "\n",
    "# get one column for each feature in the distributions of time and subreddit\n",
    "weekday = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "month = ['january', 'february', 'march', 'april', 'may', 'june', 'juli', 'august', 'september', 'october', 'november', 'december']\n",
    "year = ['2015', '2016', '2017', '2018', '2019']\n",
    "hour = ['hour01', 'hour02', 'hour03', 'hour04', 'hour05', 'hour06', 'hour07', 'hour08', 'hour09', 'hour10', \n",
    "        'hour11', 'hour12', 'hour13', 'hour14', 'hour15', 'hour16', 'hour17', 'hour18', 'hour19', 'hour20', \n",
    "        'hour21', 'hour22', 'hour23', 'hour24']\n",
    "timelen = len(weekday+month+year+hour)\n",
    "\n",
    "def onecolumnperdatapoint(df, column, namelist):\n",
    "    for i in tqdm(range(len(namelist))):\n",
    "        df[namelist[i]] = df[column].apply(lambda x:[x[i]])\n",
    "        df[namelist[i]] = [item[0] for item in df[namelist[i]]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wrapper for commentdf\n",
    "def create_commentdf(df):\n",
    "    print(\"Create new df grouped by author...\")\n",
    "    groupdf = create_groupdf(df)\n",
    "    print(\"Create new columns with features...\")\n",
    "    pandora = create_new_columns(groupdf)\n",
    "    print(\"Distribute the weekday_dist to several columns...\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'weekday_dist', weekday)\n",
    "    print(\"Distribute the month_dist to several columns\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'month_dist', month)\n",
    "    print(\"Distribute the year_dist to several columns...\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'year_dist', year)\n",
    "    print(\"Distribute the daily_dist to several columns...\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'daily_dist', hour)\n",
    "    print(\"Distribute the subreddit_dist to several columns...\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'subreddit_dist', subredditlist)\n",
    "    print(\"Drop dist columns...\")\n",
    "    pandora.drop(['weekday_dist', 'month_dist', 'year_dist', 'daily_dist', 'subreddit_dist'], axis=1, inplace=True)\n",
    "    return pandora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_df_size():\n",
    "    print(\"Reduce floats...\")\n",
    "    floats = df.select_dtypes(include=['float64']).columns.tolist()\n",
    "    df[floats] = df[floats].astype('float32')\n",
    "    print(\"Reduce ints...\")\n",
    "    ints = df.select_dtypes(include=['int64']).columns.tolist()\n",
    "    df[ints] = df[ints].astype('int16')\n",
    "    print(df.memory_usage(index=True,deep=True).sum() / (1024**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create basis df with one row per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "print(\"Start time:\", str(start))\n",
    "print(\"Create comment df (name: pandora)...\")\n",
    "pandora = create_commentdf(comments)\n",
    "print(\"Pandora: \")\n",
    "print(pandora.info())\n",
    "# merge commentdf and authordf\n",
    "print(\"Sort pandora df...\")\n",
    "pandora= pandora.sort_values(by='author')\n",
    "print(\"Sort big five df...\")\n",
    "bigfive= bigfive.sort_values(by='author')\n",
    "if pandora.index.name != 'author':\n",
    "    print(\"Set pandora index...\")\n",
    "    pandora = pandora.set_index('author')\n",
    "if bigfive.index.name != 'author':\n",
    "    print(\"Set bigfive index...\")\n",
    "    bigfive = bigfive.set_index('author')\n",
    "print(\"Join commentdf and authordf\")\n",
    "global df\n",
    "df = pandora.join(bigfive)\n",
    "del pandora\n",
    "del bigfive\n",
    "gc.collect()\n",
    "print(\"Df before multiindex: \")\n",
    "print(df.memory_usage(index=True,deep=True).sum() / (1024**2))\n",
    "\n",
    "# create multiindex\n",
    "print(\"Create multiindex...\\n\")\n",
    "headers = 2*['text'] + 1*['data'] + 4*['post'] + 5*['subtf'] + 1*['post']\n",
    "headers = headers + (timelen + subredditlength -4)*['subtf'] + traitlen*['trait']\n",
    "\n",
    "# check multiindex\n",
    "columns = df.columns.values\n",
    "predictorsfile=open('columns.txt','w')\n",
    "for index in range(len(columns)):\n",
    "    predictorsfile.write(columns[index])\n",
    "    predictorsfile.write('\\n')\n",
    "predictorsfile.close()\n",
    "\n",
    "print(\"Length headers\", len(headers))\n",
    "print(\"Length columns\", len(columns))\n",
    "arrays = [headers] + [columns]\n",
    "df.columns=pd.MultiIndex.from_arrays(arrays)\n",
    "\n",
    "# reduce size of dataset\n",
    "print(\"Df with multiindex before reduction of dtypes (MB): \")\n",
    "print(df.memory_usage(index=True,deep=True).sum() / (1024**2))\n",
    "print(\"Reduce size of df...\")\n",
    "reduce_df_size()\n",
    "\n",
    "print(\"Df with multiindex (MB): \")\n",
    "print(df.memory_usage(index=True,deep=True).sum() / (1024**2))\n",
    "df.info(verbose=True)\n",
    "del headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create big five binary categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "# create binary representation of personality traits\n",
    "def bigfive_cat():\n",
    "    # change big five to binary representation\n",
    "    df['trait', 'big5_a'] = df['trait', 'agreeableness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['trait', 'big5_o'] = df['trait', 'openness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['trait', 'big5_c'] = df['trait', 'conscientiousness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['trait', 'big5_e'] = df['trait', 'extraversion'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['trait', 'big5_n'] = df['trait', 'neuroticism'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['trait', 'big5_a_multi'] = df['trait', 'agreeableness'].apply(lambda x: 0 if x<20 else(1 if x>19 and x<40 else(2 if x>39 and x<60 else(3 if x>59 and x<80 else 4))))\n",
    "    df['trait', 'big5_o_multi'] = df['trait', 'openness'].apply(lambda x: 0 if x<20 else(1 if x>19 and x<40 else(2 if x>39 and x<60 else(3 if x>59 and x<80 else 4))))\n",
    "    df['trait', 'big5_c_multi'] = df['trait', 'conscientiousness'].apply(lambda x: 0 if x<20 else(1 if x>19 and x<40 else(2 if x>39 and x<60 else(3 if x>59 and x<80 else 4))))\n",
    "    df['trait', 'big5_e_multi'] = df['trait', 'extraversion'].apply(lambda x: 0 if x<20 else(1 if x>19 and x<40 else(2 if x>39 and x<60 else(3 if x>59 and x<80 else 4))))\n",
    "    df['trait', 'big5_n_multi'] = df['trait', 'neuroticism'].apply(lambda x: 0 if x<20 else(1 if x>19 and x<40 else(2 if x>39 and x<60 else(3 if x>59 and x<80 else 4))))\n",
    "    \n",
    "bigfive_cat()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_df_size()\n",
    "\n",
    "# write optimized pickle\n",
    "filepath = \"aug_commentdf.pkl\"\n",
    "with open(filepath, \"wb\") as f:\n",
    "    pickled = pickle.dumps(df1, protocol=-1)\n",
    "    f.write(pickled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other features that are not mentioned in the paper\n",
    "def create_features():\n",
    "    # Total number of characters (including space)\n",
    "    print(\"\\tCharacter count per author...\")\n",
    "    df['x_feat', 'char_count'] = df['text', 'body'].str.len()\n",
    "    # Total number of stopwords\n",
    "    print(\"\\tNumber of stopwords per author...\")\n",
    "    stopwordList = stopwords.words('english')\n",
    "    df['x_feat', 'stopwords'] = df['text', 'body'].apply(lambda x: len([x for x in x.split() if x in stopwordList]))\n",
    "    # Total number of punctuation or special characters\n",
    "    print(\"\\tTotal number of punctuation per author...\")\n",
    "    df['x_feat', 'total_punc'] = df['text', 'body'].apply(lambda x: len([x for x in x.split() for j in x if j in string.punctuation]))\n",
    "    # Total number of numerics\n",
    "    print(\"\\tTotal number of numerics per author...\")\n",
    "    df['x_feat', 'total_num'] = df['text', 'body'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    # Total number of uppercase words\n",
    "    print(\"\\tTotal number of upper case words per author...\")\n",
    "    df['x_feat', 'total_uppercase'] = df['text', 'body'].apply(lambda x: len([x for x in x.split() if x.isupper()]))    \n",
    "\n",
    "# type token ratio\n",
    "def typetokenratio():\n",
    "    ratiolst = []\n",
    "    for comment in tqdm(df['text', 'body']):\n",
    "            lex = LexicalRichness(comment)\n",
    "            if lex.words == 0:\n",
    "                ratiolst.append(0)\n",
    "            else:\n",
    "                ratio = lex.ttr\n",
    "                ratiolst.append(ratio)\n",
    "    df['lin_feat', 'ttr'] = ratiolst\n",
    "    \n",
    "# POS tagger\n",
    "def tagging():\n",
    "    past = [] #VPA\n",
    "    presence = [] #VPR\n",
    "    adverbs = [] #RB\n",
    "    prepositions = [] #PREP\n",
    "    pronouns = [] #PR\n",
    "    for comment in tqdm(df['text', 'body']):\n",
    "            text = comment.split()\n",
    "            tags = nltk.pos_tag(text)\n",
    "            counts = Counter(tag for word,tag in tags)\n",
    "            total = sum(counts.values())\n",
    "            pron = counts['PRP'] + counts['PRP$']\n",
    "            verbspr = counts['VB'] + counts['VBG'] + counts['VBP'] + counts['VBZ'] + counts['MD']\n",
    "            verbspa = counts['VBD'] + counts['VBN']\n",
    "            preps = counts['IN'] + counts['TO']\n",
    "            counts['PR'] = pron\n",
    "            counts['PREP'] = preps\n",
    "            counts['VPR'] = verbspr #present tense\n",
    "            counts['VPA'] = verbspa #past tense\n",
    "            if total == 0:\n",
    "                allcounts = dict((word, float(count)/1) for word,count in counts.items())\n",
    "            else:\n",
    "                allcounts = dict((word, float(count)/total) for word,count in counts.items())\n",
    "            try:\n",
    "                past.append(allcounts['VPA'])\n",
    "            except KeyError:\n",
    "                past.append(0)\n",
    "            try:\n",
    "                presence.append(allcounts['VPR'])\n",
    "            except KeyError:\n",
    "                presence.append(0)\n",
    "            try:\n",
    "                adverbs.append(allcounts['RB'])\n",
    "            except KeyError:\n",
    "                adverbs.append(0)\n",
    "            try:\n",
    "                prepositions.append(allcounts['PREP'])\n",
    "            except KeyError:\n",
    "                prepositions.append(0)\n",
    "            try:\n",
    "                pronouns.append(allcounts['PR'])\n",
    "            except KeyError:\n",
    "                pronouns.append(0)\n",
    "    df['lin_feat', 'pasttense'] = past\n",
    "    df['lin_feat', 'presencetense'] = presence\n",
    "    df['lin_feat', 'adverbs'] = adverbs\n",
    "    df['lin_feat', 'prepositions'] = prepositions\n",
    "    df['lin_feat', 'pronouns'] = pronouns\n",
    "\n",
    "tr_initial = tracker.SummaryTracker()\n",
    "\n",
    "create_features()\n",
    "typetokenratio()\n",
    "tagging()\n",
    "\n",
    "\n",
    "print(tr_initial.print_diff())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_df_size()\n",
    "\n",
    "# write optimized pickle\n",
    "filepath = \"aug_commentdf_FE1.pkl\"\n",
    "with open(filepath, \"wb\") as f:\n",
    "    pickled = pickle.dumps(df1, protocol=-1)\n",
    "    f.write(pickled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentence tokens\n",
    "def senttokenize():\n",
    "    sentbody = []\n",
    "    for row in tqdm(df['text', 'doc_body']):\n",
    "        sentitem = []\n",
    "        for item in row:\n",
    "            sentences = sent_tokenize(item)\n",
    "            sentitem.append(sentences)\n",
    "        sentbody.append(sentitem)\n",
    "    df['text', 'senttokens'] = sentbody\n",
    "\n",
    "senttokenize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words per sentence\n",
    "def wordcounter():\n",
    "    lengthscore = []\n",
    "    for row in tqdm(df['text', 'senttokens']):\n",
    "        rowscore = []\n",
    "        for comment in row:\n",
    "            sentencescore = 0\n",
    "            for senttoken in comment:\n",
    "                length = len(senttoken.split())\n",
    "                sentencescore += length\n",
    "            if len(comment) > 1:\n",
    "                sentencescore = sentencescore/len(comment)\n",
    "        lengthscore.append(sentencescore)\n",
    "        arr = np.array(lengthscore)\n",
    "    df['lin_feat', 'words_per_sent'] = lengthscore\n",
    "\n",
    "wordcounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(('text', 'senttokens'), axis = 1, inplace = True)\n",
    "reduce_df_size()\n",
    "\n",
    "# write optimized pickle\n",
    "filepath = \"aug_commentdf_FE2.pkl\"\n",
    "with open(filepath, \"wb\") as f:\n",
    "    pickled = pickle.dumps(df1, protocol=-1)\n",
    "    f.write(pickled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove decontractions\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "df['text', 'decon_body'] = df['text', 'doc_body'].apply(lambda x:([decontracted(phrase) for phrase in x]))\n",
    "df.drop(('text', 'doc_body'), axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empath\n",
    "# create new categories with empath\n",
    "def new_cat():\n",
    "    empath = Empath()\n",
    "    social = empath.create_category(\"social\",[\"mate\",\"talk\",\"they\"])\n",
    "    humans = empath.create_category(\"humans\",[\"adult\",\"baby\",\"boy\"])\n",
    "    cognitive = empath.create_category(\"cognitive\",[\"cause\",\"know\",\"ought\"])\n",
    "    insight = empath.create_category(\"insight\",[\"think\",\"know\",\"consider\"])\n",
    "    causation = empath.create_category(\"causation\",[\"because\",\"effect\",\"hence\"])\n",
    "    discrepancy = empath.create_category(\"discrepancy\",[\"should\",\"would\",\"could\"])\n",
    "    tentative = empath.create_category(\"tentative\",[\"maybe\",\"perhaps\",\"guess\"])\n",
    "    certainty = empath.create_category(\"certainty\",[\"always\",\"never\", \"proof\"])\n",
    "    inhibition = empath.create_category(\"inhibition\",[\"block\",\"constrain\",\"stop\"])\n",
    "    inclusive = empath.create_category(\"inclusive\",[\"and\",\"with\",\"include\"])\n",
    "    exclusive = empath.create_category(\"exclusive\",[\"but\",\"without\",\"exclude\"])\n",
    "    perceptual = empath.create_category(\"perceptual\",[\"observing\",\"hear\",\"feeling\"])\n",
    "    see = empath.create_category(\"see\",[\"view\",\"saw\",\"seen\"])\n",
    "    feel = empath.create_category(\"feel\",[\"feels\",\"touch\",\"feeling\"])\n",
    "    biological = empath.create_category(\"biological\",[\"eat\",\"blood\",\"pain\"])\n",
    "    relativity = empath.create_category(\"relativity\",[\"area\",\"bend\",\"go\"])\n",
    "    motion = empath.create_category(\"motion\",[\"arrive\",\"car\",\"go\", \"walk\", \"fly\", \"move\", \"run\", \"leave\"])\n",
    "    space = empath.create_category(\"space\",[\"down\",\"in\",\"thin\"])\n",
    "    time = empath.create_category(\"time\",[\"end\",\"until\",\"season\"])\n",
    "    agreement = empath.create_category(\"agreement\", [\"agree\", \"ok\", \"yes\"])\n",
    "    fillers = empath.create_category(\"fillers\", [\"like\", \"Imean\", \"yaknow\"])\n",
    "    nonfluencies = empath.create_category(\"nonfluencies\", [\"umm\", \"hm\", \"er\"])\n",
    "    conjunctions = empath.create_category(\"conjunctions\", [\"and\", \"but\", \"whereas\"])\n",
    "    quantifiers = empath.create_category(\"quantifiers\", [\"few\", \"many\", \"much\"])\n",
    "    numbers = empath.create_category(\"numbers\", [\"two\", \"fourteen\", \"thousand\"])\n",
    "\n",
    "def apply_empath():\n",
    "    empath = Empath()\n",
    "    print(\"Create new empath categories...\")\n",
    "    new_cat()\n",
    "    print(\"Apply empath...\")\n",
    "    empathvalues = []\n",
    "    empathcategories = [\"swearing_terms\", \"social\", \"family\", \"friends\", \"humans\", \"emotional\", \"positive_emotion\", \n",
    "                        \"negative_emotion\", \"fear\", \"anger\", \"sadness\", \"cognitive\", \"insight\", \"causation\", \n",
    "                        \"discrepancy\", \"tentative\", \"certainty\", \"inhibition\", \"inclusive\", \"exclusive\", \n",
    "                        \"perceptual\", \"see\", \"hear\", \"feel\", \"biological\", \"body\", \"health\", \"sexual\", \"eat\", \n",
    "                        \"relativity\", \"space\", \"time\", \"work\", \"achievement\", \"leisure\", \"home\", \"money\", \n",
    "                        \"religion\", \"death\" ,\"agreement\", \"fillers\", \"nonfluencies\", \"conjunctions\", \"quantifiers\", \n",
    "                        \"numbers\"]\n",
    "    for sentence in tqdm(df['text', 'decon_body']):\n",
    "        empathvalues.append(empath.analyze(sentence, categories=empathcategories, normalize=True))\n",
    "    empathdf = pd.DataFrame(empathvalues)\n",
    "    empathdf['author'] = df.index\n",
    "    empathdf = empathdf.set_index('author')\n",
    "    headers = 40*['empath'] + 5*['lin_feat']\n",
    "    columns = empathdf.columns.values\n",
    "    print(len(headers))\n",
    "    print(len(columns))\n",
    "    arrays = [headers] + [columns]\n",
    "    empathdf.columns=pd.MultiIndex.from_arrays(arrays)\n",
    "    df = df.join(empathdf, rsuffix=\"_empath\")\n",
    "    del empathdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data for other wordlists\n",
    "concretenessdf = pd.read_csv('/home/sophia/ma_py/psych_lists/concreteness.csv')\n",
    "cdf = concretenessdf[['Conc.M']]\n",
    "cmatrix = cdf.to_numpy()\n",
    "concrete = concretenessdf['Word'].values.tolist()\n",
    "del concretenessdf\n",
    "\n",
    "happinessdf = pd.read_csv('/home/sophia/ma_py/psych_lists/happiness_ratings.csv')\n",
    "hdf = happinessdf[['happiness_average']]\n",
    "hmatrix = hdf.to_numpy()\n",
    "happiness = happinessdf['word'].values.tolist()\n",
    "del happinessdf\n",
    "\n",
    "cursedf = pd.read_csv('/home/sophia/ma_py/psych_lists/mean_good_curse.csv')\n",
    "cudf = cursedf[['mean_good_curse']]\n",
    "cumatrix = cudf.to_numpy()\n",
    "curse = cursedf['word'].values.tolist()\n",
    "del cursedf\n",
    "\n",
    "sensorydf = pd.read_csv('/home/sophia/ma_py/psych_lists/sensory_experience_ratings.csv')\n",
    "serdf = sensorydf[['Average SER']]\n",
    "sermatrix = serdf.to_numpy()\n",
    "ser = sensorydf['Word'].values.tolist()\n",
    "del sensorydf\n",
    "\n",
    "alldf = pd.read_csv('/home/sophia/ma_py/psych_lists/sensory_ratings_all.csv')\n",
    "newalldf = alldf[['Emotion', 'Polarity', 'Social', 'Moral', 'MotionSelf', 'Thought', 'Color', 'TasteSmell', 'Tactile', 'VisualForm', 'Auditory', 'Space', 'Quantity', 'Time', 'CNC', 'IMG', 'FAM']]\n",
    "newalldf = newalldf.fillna(0)\n",
    "allmatrix = newalldf.to_numpy()\n",
    "allsens = alldf['Word'].values.tolist()\n",
    "del alldf\n",
    "\n",
    "valarodomdf = pd.read_csv('/home/sophia/ma_py/psych_lists/valence_arousal_dominence.csv')\n",
    "vaddf = valarodomdf[['V.Mean.Sum', 'A.Mean.Sum', 'D.Mean.Sum']]\n",
    "vadmatrix = vaddf.to_numpy()\n",
    "vad = valarodomdf['Word'].values.tolist()\n",
    "del valarodomdf\n",
    "\n",
    "mrcdf = pd.read_csv('/home/sophia/ma_py/psych_lists/mrclists_c_p.csv', sep='\\t', names=['word', 'cmean', 'pmean'])\n",
    "cpdf = mrcdf[['cmean', 'pmean']]\n",
    "cpmatrix = cpdf.to_numpy()\n",
    "mrc = mrcdf['word'].values.tolist()\n",
    "del mrcdf\n",
    "\n",
    "# function for other wordlists\n",
    "\n",
    "def preprocess_counting():\n",
    "    inputtext = []\n",
    "    for row in tqdm(df['text', 'decon_body']):\n",
    "        text = ' '.join(row)\n",
    "        inputtext.append(text) \n",
    "    return inputtext\n",
    "\n",
    "def counter(inputtext, vocab):  \n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,1), vocabulary = vocab)\n",
    "    print(\"\\tVectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "def multiply(matrix, ratings):\n",
    "    # matrix multiplication \n",
    "    result = np.matmul(matrix, ratings)\n",
    "    # divide each score with the number of words in the list to normalize\n",
    "    if len(ratings) > 0:\n",
    "        result = result/(len(ratings))\n",
    "    return result\n",
    "\n",
    "def aggregator(inputtext, vocab, ratings, name):\n",
    "    print(\"\\tCount...\")\n",
    "    count = counter(inputtext, vocab)\n",
    "    print(\"\\tMultiply...\")\n",
    "    result = multiply(count, ratings)\n",
    "    num_rows, num_cols = result.shape\n",
    "    \n",
    "    if num_cols ==1:\n",
    "        df['psych', name] = result\n",
    "    else:\n",
    "        resultdf = pd.DataFrame(result, columns=name)\n",
    "        resultdf['author'] = df.index\n",
    "        resultdf = resultdf.set_index('author')\n",
    "        headers = (len(name))*['psych']\n",
    "        columns = resultdf.columns.values\n",
    "        print(len(headers))\n",
    "        print(len(columns))\n",
    "        arrays = [headers] + [columns]\n",
    "        resultdf.columns=pd.MultiIndex.from_arrays(arrays)\n",
    "        df = df.join(resultdf, rsuffix=\"_wordlist\")\n",
    "        del resultdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordlists created manually\n",
    "negations = [\"no\", \"not\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nowhere\", \"never\", \"nay\"]\n",
    "articles = [\"a\", \"an\", \"the\"]\n",
    "future = [\"will\", \"gonna\"]\n",
    "pers_pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\", \"you\", \"your\", \"yours\", \"yourself\", \"he\", \"him\", \"his\",\n",
    "                 \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"themself\", \"we\", \"us\",\n",
    "                 \"our\", \"ours\", \"ourselves\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\"]\n",
    "fp_sing = [\"i\", \"me\", \"my\", \"mine\", \"myself\"]\n",
    "fp_plural = [\"we\", \"us\", \"our\", \"ours\", \"ourselves\"]\n",
    "secondp = [\"you\", \"your\", \"yours\", \"yourself\"]\n",
    "tp_sing = [\"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"themself\"]\n",
    "tp_plural = [\"they\", \"them\", \"their\", \"theirs\", \"themselves\"]\n",
    "indef_pronouns = [\"another\", \"anybody\", \"anyone\", \"anything\", \"each\", \"either\", \"enough\", \"everybody\", \n",
    "                        \"everyone\", \"everything\", \"little\", \"much\", \"neither\", \"nobody\", \"no one\", \"nothing\", \n",
    "                        \"one\", \"other\", \"somebody\", \"someone\", \"something\", \"both\", \"few\", \"fewer\", \"many\", \n",
    "                        \"others\", \"several\", \"all\", \"any\", \"more\", \"most\", \"none\", \"some\", \"such\"]\n",
    "aux_verbs = [\"be\", \"am\", \"are\", \"is\", \"was\", \"were\", \"being\", \"can\", \"could\", \"do\", \"did\", \"does\", \"doing\", \n",
    "             \"have\", \"had\", \"has\", \"having\", \"may\", \"might\", \"must\", \"shall\", \"should\", \"will\", \"would\"]\n",
    "\n",
    "def list_counter(inputtext, vocab, name):\n",
    "    total = []\n",
    "    for row in tqdm(df['text', 'decon_body']):\n",
    "        total.append(len(row))\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,1), vocabulary = vocab)\n",
    "    print(\"\\tVectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    v = vectors.toarray()\n",
    "    averagev = v.sum(axis=1)\n",
    "    totalvector =  np.array(total)\n",
    "    score = np.divide(averagev, totalvector)\n",
    "    df['lin_feat', name] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wordlist_features():\n",
    "    global df\n",
    "    print(\"Empath...\")\n",
    "    apply_empath()\n",
    "    # create scores for each word list and add them to df\n",
    "    print(\"Preprocessing for wordlists...\")\n",
    "    inputtext = preprocess_counting()\n",
    "    print(\"\\nWordlist Concreteness: \\n\")\n",
    "    aggregator(inputtext, concrete, cmatrix, \"concreteness\")\n",
    "    print(\"\\nWordlist Happiness: \\n\")\n",
    "    aggregator(inputtext, happiness, hmatrix, \"happiness\")\n",
    "    print(\"\\nWordlist Good_Curse: \\n\")\n",
    "    aggregator(inputtext, curse, cumatrix, \"good_curse\")\n",
    "    print(\"\\n17 further wordlists: \\n\")\n",
    "    aggregator(inputtext, allsens, allmatrix, ['emotion', 'polarity', 'social', 'moral', 'motionself', 'thought', 'color', 'tastesmell', 'tactile', 'visualform', 'auditory', 'space', 'quantity', 'time', 'CNC', 'IMG', 'FAM'])\n",
    "    print(\"\\nWordlist SER: \\n\")\n",
    "    aggregator(inputtext, ser, sermatrix, \"SER\")\n",
    "    print(\"\\nWordlists Valence, Arousal, Dominance: \\n\")\n",
    "    aggregator(inputtext, vad, vadmatrix, ['valence', 'arousal', 'dominance'])\n",
    "    print(\"\\nWordlist Negation: \\n\")\n",
    "    list_counter(inputtext, negations, \"negations\")\n",
    "    print(\"\\nWordlist Articles: \\n\")\n",
    "    list_counter(inputtext, articles, \"articles\")\n",
    "    print(\"\\nWordlist Future: \\n\")\n",
    "    list_counter(inputtext, future, \"future\")\n",
    "    print(\"\\nWordlist personal pronouns: \\n\")\n",
    "    list_counter(inputtext, pers_pronouns, \"pers_pronouns\")\n",
    "    print(\"\\nWordlist first person singular pronouns: \\n\")\n",
    "    list_counter(inputtext, fp_sing, \"fp_sing\")\n",
    "    print(\"\\nWordlist first person plural pronouns: \\n\")\n",
    "    list_counter(inputtext, fp_plural, \"fp_plural\")\n",
    "    print(\"\\nWordlist second person pronouns: \\n\")\n",
    "    list_counter(inputtext, secondp, \"secondp\")\n",
    "    print(\"\\nWordlist third person singular pronouns: \\n\")\n",
    "    list_counter(inputtext, tp_sing, \"tp_sing\")\n",
    "    print(\"\\nWordlist third person plural pronouns: \\n\")\n",
    "    list_counter(inputtext, tp_plural, \"tp_plural\")\n",
    "    print(\"\\nWordlist indefinite pronouns: \\n\")\n",
    "    list_counter(inputtext, indef_pronouns, \"indef_pronouns\")\n",
    "    print(\"\\nWordlist auxiliary verbs: \\n\")\n",
    "    list_counter(inputtext, aux_verbs, \"aux_verbs\")\n",
    "    print(\"\\nWordlists from MRC (2): \\n\")\n",
    "    aggregator(inputtext, mrc, cpmatrix, [\"mrc_cmean\", \"mrc_pmean\"])\n",
    "\n",
    "extract_wordlist_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_df_size()\n",
    "\n",
    "# write optimized pickle\n",
    "filepath = \"aug_commentdf_FE3.pkl\"\n",
    "with open(filepath, \"wb\") as f:\n",
    "    pickled = pickle.dumps(df1, protocol=-1)\n",
    "    f.write(pickled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopwordlist to use\n",
    "def choose_stopwordlist(mode):\n",
    "    if mode == 'NLTK':\n",
    "        stopwordList = stopwords.words('english')\n",
    "    if mode == 'NLTK-neg':\n",
    "        stopwordList = stopwords.words('english')\n",
    "        stopwordList.remove('no')\n",
    "        stopwordList.remove('nor')\n",
    "        stopwordList.remove('not')\n",
    "    return stopwordList\n",
    "\n",
    "\n",
    "# lower words and remove special characters\n",
    "def lower_special():\n",
    "    newrow = []\n",
    "    for row in tqdm(df['text', 'decon_body']):\n",
    "        newcomment = []\n",
    "        for comment in row:\n",
    "            text_pre = \"\"\n",
    "            for character in comment:\n",
    "                if character.isalnum() or character.isspace():\n",
    "                    character = character.lower()\n",
    "                    text_pre += character\n",
    "                else:\n",
    "                    text_pre += \" \"\n",
    "            newcomment.append(text_pre)\n",
    "        newrow.append(newcomment)   \n",
    "    df['text', 'probody'] = newrow\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(stopwordList):\n",
    "    newprobody = []\n",
    "    for row in tqdm(df['text', 'probody']):\n",
    "        newrowprobody = []\n",
    "        for comment in row:\n",
    "            words = [word for word in comment.split() if (word not in stopwordList)]\n",
    "            newcomment = ' '.join(words)\n",
    "            newrowprobody.append(newcomment)\n",
    "        newprobody.append(newrowprobody)\n",
    "    df['text', 'probody'] = newprobody\n",
    "\n",
    "# change numbers to words and tokenize words\n",
    "\n",
    "import decimal\n",
    "def num_tokenize():    \n",
    "    newbody_complete = []\n",
    "    newprobody_complete = []\n",
    "    # num2words\n",
    "    for row in tqdm(df['text', 'probody']):\n",
    "        newbody = []\n",
    "        newprobody = []\n",
    "        for sentence in row:\n",
    "            # string to list\n",
    "            inputtext = sentence.split()\n",
    "            numlist = []\n",
    "            for i in range(len(inputtext)):\n",
    "                if inputtext[i].isnumeric():\n",
    "                    numlist.append(i)\n",
    "            for number in numlist:\n",
    "                # deleted: fractions, superscripts, extremely large numbers, 卌卌, 一\n",
    "                try:\n",
    "                    inputtext[number] = num2words(inputtext[number])\n",
    "                except decimal.InvalidOperation:\n",
    "                    inputtext[number] = \" \"\n",
    "                except OverflowError:\n",
    "                    inputtext[number] = \" \"\n",
    "\n",
    "            # list to string\n",
    "            inputtext = [word for word in inputtext if word.isalpha()]\n",
    "            celltext = ' '.join(inputtext)\n",
    "            newprobody.append(celltext)\n",
    "            # tokenize\n",
    "            words = word_tokenize(celltext)\n",
    "            newbody.append(words)\n",
    "        newbody_complete.append(newbody)\n",
    "        newprobody_complete.append(newprobody)\n",
    "    df['text', 'probody'] = newprobody_complete\n",
    "    df['text', 'tokens'] = newbody_complete\n",
    "\n",
    "# Porter Stemmer\n",
    "def stemming():\n",
    "    ps = PorterStemmer()\n",
    "    newtokens = []\n",
    "    for row in tqdm(df['text', 'tokens']):\n",
    "        newcomment = []\n",
    "        for comment in row:\n",
    "            words = [ps.stem(word) for word in comment]\n",
    "            newcomment.append(words)\n",
    "        newtokens.append(newcomment)\n",
    "    df['text', 'tokens'] = newtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose stopwordlist with or without negation\n",
    "stopwordList = choose_stopwordlist(mode='NLTK-neg')\n",
    "# lower, remove stopwords, num2words, tokenize\n",
    "print(\"Lower words and remove special characters...\")\n",
    "lower_special()\n",
    "print(\"Remove stopwords...\")\n",
    "remove_stopwords(stopwordList)\n",
    "print(\"Change numbers to words and tokenize words...\")\n",
    "num_tokenize()\n",
    "# porters stemmer\n",
    "print(\"Porters Stemmer...\")\n",
    "stemming()\n",
    "\n",
    "df.drop(('text', 'decon_body'), axis = 1, inplace = True)\n",
    "df.drop(('text', 'probody'), axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " words longer than six characters\n",
    "def charcounter():\n",
    "    charscore = []\n",
    "    for row in tqdm(df['text', 'tokens']):\n",
    "        for comment in row:\n",
    "            rowcharscore = 0\n",
    "            lencomment = len(comment)\n",
    "            if lencomment == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                number = 0\n",
    "                for token in comment:\n",
    "                    length = len(token)\n",
    "                    if length > 5:\n",
    "                        number+=1\n",
    "                score = number/lencomment\n",
    "            rowcharscore += score\n",
    "        rowcharscore = rowcharscore/len(row)\n",
    "        charscore.append(rowcharscore)\n",
    "    df['lin_feat', 'wordslongersix'] = charscore\n",
    "\n",
    "\n",
    "\n",
    "def ngram_preprocessing():\n",
    "    # convert input from list to string\n",
    "    ngrams = []\n",
    "    inputtext = []\n",
    "    valid = True\n",
    "    notvalid_lst =[]\n",
    "    for row in tqdm(df['text', 'tokens']):\n",
    "        valid_string = \"\"\n",
    "        for comment in row:\n",
    "            valid = True\n",
    "            text = ' '.join(comment)\n",
    "            i=0\n",
    "            for char in text:\n",
    "                if not(char in string.printable):\n",
    "                    valid = False\n",
    "                    notvalid_lst += [char]\n",
    "                    i+=1\n",
    "            if valid == True:\n",
    "                textspace = text + \" \"\n",
    "                valid_string += textspace\n",
    "#         print(valid_string, \"\\n\")\n",
    "        inputtext.append(valid_string)\n",
    "    if len(notvalid_lst) > 0:\n",
    "        print(\"\\nNumber of dismissed comments: \", i)\n",
    "    print(\"Length of inputtext: \", len(inputtext))\n",
    "    return inputtext\n",
    "    \n",
    "def ngrams(inputtext, n_min, n_max, ngramtype):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n_min,n_max), analyzer=ngramtype, max_features=(n_max-(n_min-1))*1000)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    print(\"Get feature names...\")\n",
    "    names = vectorizer.get_feature_names()\n",
    "    print(\"Length of feature names: \", len(names))\n",
    "    print(\"Create df...\")\n",
    "    ngramdf = pd.DataFrame(denselist, columns=names)\n",
    "    return ngramdf\n",
    "\n",
    "def merge_dfs(df1, df2):\n",
    "    ngramsdf = df1.join(df2, rsuffix=\"_char\")\n",
    "    ngramsdf['author_index'] = df.index\n",
    "    ngramsdf = ngramsdf.set_index('author_index')\n",
    "    headers = (len(df1.columns))*['wordngram'] + (len(df2.columns))*['charngram']\n",
    "    columns = ngramsdf.columns.values\n",
    "    print(\"Headers: \", len(headers))\n",
    "    print(\"Columns: \", len(columns))\n",
    "    arrays = [headers] + [columns]\n",
    "    ngramsdf.columns = pd.MultiIndex.from_arrays(arrays)\n",
    "    df = df.join(ngramsdf, rsuffix=\"_ngram\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for LDA\n",
    "def preprocess_lda():\n",
    "    global df\n",
    "    neglst = [\"no\", \"not\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nowhere\", \"never\", \"nay\"]\n",
    "    inputlst = []\n",
    "    for row in tqdm(df['x', 'tokens']):\n",
    "        rowlst = []\n",
    "        for comment in row:\n",
    "            rowlst.append([word for word in comment if (word not in neglst)])\n",
    "        inputlst.append(rowlst)\n",
    "        newlst = []\n",
    "        for smalllist in inputlst:\n",
    "            flat_list = list(itertools.chain(*smalllist))\n",
    "            newlst.append(flat_list)\n",
    "    return newlst\n",
    "\n",
    "\n",
    "def apply_lda(inputlst, number, name):\n",
    "    print(\"Start LDA...\")\n",
    "    dictionary = corpora.Dictionary(inputlst)\n",
    "    corpus = [dictionary.doc2bow(text) for text in inputlst]\n",
    "    ldamodel = gensim.models.LdaMulticore(corpus, num_topics=number, id2word = dictionary, chunksize=100, dtype = np.float32, workers=14)\n",
    "#     for idx, topic in ldamodel.print_topics(-1):\n",
    "#     print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "#     print(\"\\n\")  \n",
    "    topics_list = []\n",
    "    for document in corpus:\n",
    "        topics = ldamodel.get_document_topics(document, minimum_probability=0.0)\n",
    "        onlytopics = [x[1] for x in topics]\n",
    "        topics_list.append(onlytopics)\n",
    "    ldadf = pd.DataFrame(topics_list)\n",
    "    ldadf['author'] = df.index\n",
    "    ldadf = ldadf.set_index('author')\n",
    "    columnname = 'lda' + str(number)\n",
    "    headers = number*[columnname]\n",
    "    columns = ldadf.columns.values\n",
    "    arrays = [headers] + [columns]\n",
    "    ldadf.columns=pd.MultiIndex.from_arrays(arrays)\n",
    "    return ldadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charcounter()\n",
    "\n",
    "print(\"Ngrams...\")\n",
    "print(\"Preprocessing for ngrams: \")\n",
    "inputtext = ngram_preprocessing(df)\n",
    "print(\"Create word ngrams...\")\n",
    "wordngramsdf = ngrams(inputtext, 1, 3, \"word\")\n",
    "print(\"Create char ngrams...\")\n",
    "charngramsdf = ngrams(inputtext, 2, 3, \"char\")\n",
    "print(\"Merge df...\")\n",
    "merge_dfs(wordngramsdf, charngramsdf)\n",
    "\n",
    "print(\"\\n\\nCreate user features (LDA)...\\n\")\n",
    "print(\"Preprocessing for LDA...\")\n",
    "inputlst = preprocess_lda()\n",
    "print(\"LDA with fifty topics: \")\n",
    "lda50df = apply_lda(inputlst, 50, \"ldafifty\")\n",
    "print(\"LDA with onehundred topics: \")\n",
    "lda100df = apply_lda(inputlst, 100, \"ldahundred\")\n",
    "df = df.join(lda50df, rsuffix=\"_lda50\")\n",
    "df = df.join(lda100df, rsuffix=\"_lda100\")\n",
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_df_size()\n",
    "\n",
    "# write optimized pickle\n",
    "filepath = \"aug_commentdf_FE4.pkl\"\n",
    "with open(filepath, \"wb\") as f:\n",
    "    pickled = pickle.dumps(df1, protocol=-1)\n",
    "    f.write(pickled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naming\n",
    "Big Five Labels: b5feat\n",
    "\n",
    "Big Five Labels + Augmentation: b5feat_aug\n",
    "\n",
    "Big Five + MBTI Labels: b5mbtifeat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
