{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict continuous Big Five scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "import random\n",
    "random.seed(32)\n",
    "import sklearn\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, VarianceThreshold, mutual_info_regression, RFE, SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in normal dataset\n",
    "df = pd.read_pickle(\"b5feat.pkl\")\n",
    "df.name = 'allfeatures_df'\n",
    "df = df[df['trait', 'openness'].notna()]\n",
    "df = df[df['trait', 'conscientiousness'].notna()]\n",
    "df = df[df['trait', 'extraversion'].notna()]\n",
    "df = df[df['trait', 'agreeableness'].notna()]\n",
    "df = df[df['trait', 'neuroticism'].notna()]\n",
    "df.name = 'allfeatures_df'\n",
    "df.trait.info()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable depending on which trait to focus on\n",
    "def trait(df, trait_name):\n",
    "    featuredf = df.drop(['trait'], axis=1, level=0)\n",
    "    try:\n",
    "        featuredf.drop(['text'], axis=1, level=0, inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        featuredf.drop(['data'], axis=1, level=0, inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    feature_cols = featuredf.columns.tolist()\n",
    "    \n",
    "    x = df[feature_cols]     \n",
    "    y = df['trait', trait_name]\n",
    "    return x,y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for nested stratified cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names of the features\n",
    "def get_names(x, pipeline):\n",
    "    features = pipeline.named_steps['feature_selection']\n",
    "    names = x.columns[features.get_support(indices=True)]\n",
    "    return names\n",
    "\n",
    "def get_pvalues(pipeline, x):\n",
    "#     x_indices = np.arange(x.shape[-1])\n",
    "#     selector = SelectKBest(f_classif, k=30)\n",
    "#     selector.fit(x_train, y_train)\n",
    "#     scores = -np.log10(selector.pvalues_)\n",
    "    features = pipeline.named_steps['feature_selection']\n",
    "    pvalues = features.pvalues_\n",
    "#     pvalues /= pvalues.max()\n",
    "    dfpvalues = pd.DataFrame(features.pvalues_)\n",
    "    dfscores = pd.DataFrame(features.scores_)\n",
    "    dfcolumns = pd.DataFrame(x.columns)\n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores, dfpvalues],axis=1)\n",
    "    featureScores.columns = ['specs','score', 'pvalue']\n",
    "    featureScores.sort_values(by='pvalue')\n",
    "\n",
    "    plt.figure(figsize = (7, 7))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(pvalues, bins=20)\n",
    "    plt.title('All p-values')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    smallpvalues = pvalues[pvalues<0.1]\n",
    "    plt.hist(smallpvalues, bins=10)\n",
    "    plt.title('Small p-values')\n",
    "    \n",
    "    plt.suptitle(\"Histograms of the p-values\")\n",
    "    plt.subplots_adjust(left=0.1, \n",
    "                    bottom=0.1,  \n",
    "                    right=0.9,  \n",
    "                    top=0.9,  \n",
    "                    wspace=0.4,  \n",
    "                    hspace=0.4) \n",
    "    plt.show()\n",
    "    \n",
    "    return featureScores\n",
    "\n",
    "def get_classifier(classifier):\n",
    "    if classifier == 'linear':\n",
    "        return LinearRegression(n_jobs=-1)\n",
    "    elif classifier == 'rfc_reg':\n",
    "        return RandomForestRegressor(n_jobs=-1)\n",
    "    elif classifier == 'boost_reg':\n",
    "        return GradientBoostingRegressor(random_state=0)\n",
    "    elif classifier == 'mlp_reg':\n",
    "        return MLPRegressor(random_state=1, max_iter=1000)\n",
    "\n",
    "    \n",
    "def get_featureselection(fs, classifier, n_feat):\n",
    "    if fs == 'anova':\n",
    "        return SelectKBest(f_regression, k=n_feat)\n",
    "    if fs == 'mutual':\n",
    "        return SelectKBest(mutual_info_regression, k=n_feat)\n",
    "    if fs == 'sequential_forward':\n",
    "        return SequentialFeatureSelector(get_classifier(classifier), n_features_to_select=n_feat, direction='forward', n_jobs=-1)\n",
    "    if fs == 'sequential_backward':\n",
    "        return SequentialFeatureSelector(get_classifier(classifier), n_features_to_select=n_feat, direction='backward', n_jobs=-1)\n",
    "\n",
    "\n",
    "    \n",
    "def create_pipeline_cv(classifier, fs, dim, n_feat):\n",
    "    if dim:\n",
    "        pipeline = Pipeline([\n",
    "              ('variance_threshold', VarianceThreshold()),\n",
    "              ('scaler', StandardScaler()),\n",
    "              ('pca', PCA(n_components=100)),\n",
    "              ('feature_selection',  get_featureselection(fs, classifier, n_feat)),\n",
    "              ('classification', get_classifier(classifier))\n",
    "            ])\n",
    "    else:\n",
    "        pipeline = Pipeline([\n",
    "              ('variance_threshold', VarianceThreshold()),\n",
    "              ('scaler', StandardScaler()),\n",
    "              ('feature_selection',  get_featureselection(fs, classifier, n_feat)),\n",
    "              ('classification', get_classifier(classifier))\n",
    "            ])\n",
    "    return pipeline\n",
    "\n",
    "def get_params(classifier):\n",
    "    if classifier == 'linear':\n",
    "        params = {}\n",
    "    elif classifier == 'rfc_reg':\n",
    "        params = {'classification__n_estimators': [50, 100, 200], \n",
    "                  'classification__max_depth': [3, 5, 10],\n",
    "                  'classification__max_features': ['auto', 'sqrt', 'log2']}\n",
    "    elif classifier == 'boost_reg':\n",
    "        params = {'classification__n_estimators': [50, 100, 200],\n",
    "                  'classification__learning_rate': [10**x for x in range(-3,3)],\n",
    "                  'classification__max_depth': [3, 5, 10],\n",
    "                  'classification__max_features': ['auto', 'sqrt', 'log2']}\n",
    "    elif classifier == 'mlp_reg':\n",
    "        params = {'classification__hidden_layer_sizes': [(50,), (100,), (200,), (500,)],\n",
    "                 'classification__activation': ['logistic', 'tanh', 'relu']}\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def define_outputname(traits, df, option, fs, dim, n_feat, train=False):\n",
    "    if train:\n",
    "        if len(traits) ==1:\n",
    "            outputname =  \"train_\"  +str(option) +\"_\" +str(fs) +\"_PCA\" +str(dim) +\"_\"+str(n_feat)\n",
    "        else:\n",
    "            outputname =  \"train_\"  +str(option) +\"_\" +str(fs) +\"_PCA\" +str(dim) +\"_\"+str(n_feat)\n",
    " \n",
    "    else:\n",
    "        if len(traits) ==1:\n",
    "            outputname = str(option) +\"_\" +str(fs) +\"_PCA\" +str(dim) +\"_\"+str(n_feat)\n",
    "        else:\n",
    "            outputname = str(option) +\"_\"   +str(fs) +\"_PCA\" +str(dim) +\"_\"+str(n_feat)\n",
    "\n",
    "    return outputname\n",
    "\n",
    "\n",
    "def save_predictors(names, predictors_fold1, predictors_fold2, predictors_fold3, predictors_fold4, predictors_fold5, j):\n",
    "    if j==1:\n",
    "        predictors_fold1.append(list(names))\n",
    "    elif j==2:\n",
    "        predictors_fold2.append(list(names))\n",
    "    elif j==3:\n",
    "        predictors_fold3.append(list(names))\n",
    "    elif j==4:\n",
    "        predictors_fold4.append(list(names))\n",
    "    elif j==5:\n",
    "        predictors_fold5.append(list(names))\n",
    "    return predictors_fold1, predictors_fold2, predictors_fold3, predictors_fold4, predictors_fold5\n",
    "\n",
    "def save_coefficients(coefficients, coef_fold1, coef_fold2, coef_fold3, coef_fold4, coef_fold5, j):\n",
    "    if j==1:\n",
    "        coef_fold1.append(list(coefficients))\n",
    "    elif j==2:\n",
    "        coef_fold2.append(list(coefficients))\n",
    "    elif j==3:\n",
    "        coef_fold3.append(list(coefficients))\n",
    "    elif j==4:\n",
    "        coef_fold4.append(list(coefficients))\n",
    "    elif j==5:\n",
    "        coef_fold5.append(list(coefficients))\n",
    "    return coef_fold1, coef_fold2, coef_fold3, coef_fold4, coef_fold5\n",
    "\n",
    "def save_params_folds(foldparams, params_fold1, params_fold2, params_fold3, params_fold4, params_fold5, j):\n",
    "    if j==1:\n",
    "        params_fold1.append(foldparams)\n",
    "    elif j==2:\n",
    "        params_fold2.append(foldparams)\n",
    "    elif j==3:\n",
    "        params_fold3.append(foldparams)\n",
    "    elif j==4:\n",
    "        params_fold4.append(foldparams)\n",
    "    elif j==5:\n",
    "        params_fold5.append(foldparams)\n",
    "    return params_fold1, params_fold2, params_fold3, params_fold4, params_fold5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress(df, traits, clf_lst, fs, dim, n_feat,train=False):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        print(\"Current time: \", str(datetime.datetime.now()))\n",
    "        tstart=time()\n",
    "        for option in clf_lst:\n",
    "            print(\"Classifier: \", option, \"\\n\")\n",
    "            outputname = define_outputname(traits, df, option, fs, dim, n_feat)\n",
    "            output = {'Traits': traits}\n",
    "            \n",
    "            # empty lists to save data in csv\n",
    "            predictors_fold1, predictors_fold2, predictors_fold3, predictors_fold4, predictors_fold5 = [],[],[],[],[]\n",
    "            rsquared_traits, mse_traits = [],[]\n",
    "            params_fold1, params_fold2, params_fold3, params_fold4, params_fold5 = [],[],[],[],[]\n",
    "            if option == 'linear':\n",
    "                coef_fold1, coef_fold2, coef_fold3, coef_fold4, coef_fold5  = [],[],[],[],[]\n",
    "\n",
    "    \n",
    "            for trait_name in traits:\n",
    "                print(\"\\nTrait to predict: \", trait_name, \"(\", option, \")\\n\")\n",
    "                x,y = trait(df, trait_name)\n",
    "                cv_outer = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "                cv_outer_lst = cv_outer.split(x)\n",
    "                \n",
    "                # empty lists for saving        \n",
    "                rsquared_lst, mse_lst, ytrue_lst, ypred_lst = [],[],[],[]\n",
    "\n",
    "                plt.figure(figsize=(7, 7))\n",
    "                p = Path('/home/sophia/ma_py/Big5-NLP/results/regression/')\n",
    "                j=1\n",
    "                for train_idx, val_idx in cv_outer_lst:\n",
    "                    train_data, val_data = x.iloc[train_idx], x.iloc[val_idx]\n",
    "                    train_target, val_target = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "                    # create pipeline\n",
    "                    clf = create_pipeline_cv(option, fs, dim, n_feat)\n",
    "            \n",
    "                    # create inner loop\n",
    "                    cv_inner = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "                    params = get_params(option)\n",
    "                    gd_search = GridSearchCV(clf, params, scoring = 'r2', n_jobs=-1, cv=cv_inner).fit(train_data, train_target)\n",
    "                    best_model = gd_search.best_estimator_\n",
    "                    clfnew = best_model.fit(train_data, train_target)\n",
    "                    \n",
    "                    foldparams = gd_search.best_params_\n",
    "                    params_fold1, params_fold2, params_fold3, params_fold4, params_fold5 = save_params_folds(foldparams, params_fold1, params_fold2, params_fold3, params_fold4, params_fold5, j)\n",
    "                    \n",
    "                    if dim == False:\n",
    "                        names = get_names(train_data, best_model)\n",
    "                        predictors_fold1, predictors_fold2, predictors_fold3, predictors_fold4, predictors_fold5 = save_predictors(names, predictors_fold1, predictors_fold2, predictors_fold3, predictors_fold4, predictors_fold5, j)\n",
    "                    \n",
    "                    if train: \n",
    "                        y_pred = clfnew.predict(train_data)\n",
    "                        score = clfnew.score(train_data, train_target)\n",
    "                        mse = mean_squared_error(train_target, y_pred)\n",
    "                        ytrue_lst.append(train_target)\n",
    "                        \n",
    "                    else:\n",
    "                        y_pred = clfnew.predict(val_data)\n",
    "                        score = clfnew.score(val_data, val_target)\n",
    "                        mse = mean_squared_error(val_target, y_pred)\n",
    "                        ytrue_lst.append(val_target)\n",
    "                        \n",
    "                    ypred_lst.append(y_pred)\n",
    "                        \n",
    "                    if option == 'linear':\n",
    "                        coefficients = clfnew.named_steps['classification'].coef_\n",
    "                        coef_fold1, coef_fold2, coef_fold3, coef_fold4, coef_fold5 = save_coefficients(coefficients, coef_fold1, coef_fold2, coef_fold3, coef_fold4, coef_fold5, j)\n",
    "                    rsquared_lst.append(score)\n",
    "                    mse_lst.append(mse)\n",
    "\n",
    "\n",
    "                    j+=1\n",
    "                    \n",
    "                    \n",
    "                  # Average results\n",
    "                r_avg = np.mean(rsquared_lst)\n",
    "                rsquared_traits.append(round(r_avg, 4))\n",
    "                mse_avg = np.mean(mse_lst)\n",
    "                mse_traits.append(round(mse_avg, 4))\n",
    "                print(\"Average score (R squared): \", r_avg, \"\\nAverage MSE: \", mse_avg)\n",
    "\n",
    "                all_ytrue = np.concatenate(ytrue_lst)\n",
    "                global all_ypred\n",
    "                all_ypred = np.concatenate(ypred_lst)\n",
    "                i=0\n",
    "                for value in all_ytrue:\n",
    "                    if math.isinf(value):\n",
    "                        i+=1\n",
    "                print(\"Infinite values in y true: \", i)\n",
    "                j=0\n",
    "                for value in all_ypred:\n",
    "                    if math.isinf(value):\n",
    "                        j+=1\n",
    "                print(\"Infinite values in y pred\", j)\n",
    "                \n",
    "                # plot\n",
    "                title = 'Regression plot for trait ' + trait_name \n",
    "                plot = sns.regplot(x=all_ytrue, y=all_ypred, ci=None, scatter_kws={\"color\": \"blue\", \"alpha\":0.1}, line_kws={\"color\": \"red\"})\n",
    "                plt.figure(figsize=(7, 7))\n",
    "                plt.ylabel('Predicted scores')\n",
    "                plt.xlabel('True scores')\n",
    "#                 plt.axes().set_aspect('equal', 'datalim')\n",
    "                plt.title(title)\n",
    "                plt.savefig(Path(p, outputname + \"_\" + trait_name + '_linearplot.png'))\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "            print(\"Total r squared: \", np.mean(rsquared_traits), \"Total MSE: \", np.mean(mse_traits))\n",
    "            output.update({'rsquared': rsquared_traits, 'MSE': mse_traits})\n",
    "            if dim==False:\n",
    "                output.update({'predictors_fold1': predictors_fold1, 'predictors_fold2': predictors_fold2, 'predictors_fold3': predictors_fold3, 'predictors_fold4': predictors_fold4, 'predictors_fold5': predictors_fold5})\n",
    "            if option=='linear':\n",
    "                output.update({'coef_fold1': coef_fold1, 'coef_fold2': coef_fold2, 'coef_fold3': coef_fold3, 'coef_fold4': coef_fold4, 'coef_fold5': coef_fold5})\n",
    "            outputdf = pd.DataFrame(output)\n",
    "            outputdf.to_csv(Path(p, outputname + '.csv'), index=False)\n",
    "            print(\"Time for entire process: %0.2fs\" % (time() - tstart))\n",
    "\n",
    "            \n",
    "\n",
    "big5_traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']\n",
    "regres = ['linear', 'rfc_reg', 'boost_reg', 'mlp_reg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions on continuous traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regress(df, big5_traits, regres, 'anova', dim=False, n_feat=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regress(df, big5_traits, ['mlp_reg'], 'anova', dim=False, n_feat=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
