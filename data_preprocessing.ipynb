{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing:\n",
    "\n",
    "### Features (as in paper):\n",
    "1. 11.140 ngram features: tf and tf-idf weighted word and character ngrams stemmed with Porter's stemmer\n",
    "2. type-token ratio\n",
    "3. ratio of comments in English\n",
    "4. ratio of British english vs. American English words\n",
    "5. 93 features from LIWC \n",
    "6. 26 PSYCH features (Preotiuc: Paraphrase Database and MRC Psycholinguistics Database)\n",
    "\n",
    "### Columns (from the description of the dataset):\n",
    "1. 'global':[7,10], #subreddits_commented, subreddits_commented_mbti, num_comments\n",
    "2. 'liwc':[10,103], #liwc\n",
    "3. 'word':[103,3938], #top1000 word ngram (1,2,3) per dimension based on chi2\n",
    "4. 'char':[3938,7243], #top1000 char ngrams (2,3) per dimension based on chi2\n",
    "5. 'sub':[7243,12228], #number of comments in each subreddit\n",
    "6. 'ent':[12228,12229], #entropy\n",
    "7. 'subtf':[12229,17214], #tf-idf on subreddits\n",
    "8. 'subcat':[17214,17249], #manually crafted subreddit categories\n",
    "9. 'lda50':[17249,17299], #50 LDA topics\n",
    "10. 'posts':[17299,17319], #posts statistics\n",
    "11. 'lda100':[17319,17419], #100 LDA topics\n",
    "12. 'psy':[17419,17443], #psycholinguistic features\n",
    "13. 'en':[17443,17444], #ratio of english comments\n",
    "14. 'ttr':[17444,17445], #type token ratio\n",
    "15. 'meaning':[17445,17447], #additional pyscholinguistic features\n",
    "16. 'time_diffs':[17447,17453], #commenting time diffs\n",
    "17. 'month':[17453,17465], #monthly distribution\n",
    "18. 'hour':[17465,17489], #hourly distribution\n",
    "19. 'day_of_week':[17489,17496], #daily distribution\n",
    "20. 'word_an':[17496,21496], #word ngrams selected by F-score\n",
    "21. 'word_an_tf':[21496,25496], #tf-idf ngrams selected by F-score\n",
    "22. 'char_an':[25496,29496], #char ngrams selected by F-score\n",
    "23. 'char_an_tf':[29496,33496], #tf-idf char ngrams selected by F-score\n",
    "24. 'brit_amer':[33496,33499], #british vs american english ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sophia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import bigrams, ngrams\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from collections import Counter\n",
    "from num2words import num2words \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import random\n",
    "random.seed(32)\n",
    "\n",
    "# close nltk download window to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author search:  True\n",
      "Author search:  True\n",
      "Author search:  True\n",
      "Length of dataframe:  975\n",
      "NaN in df?  True\n",
      "Sum of NaN in agreeableness 0\n",
      "Sum of NaN in openness 0\n",
      "Sum of NaN in conscientiousness 0\n",
      "Sum of NaN in extraversion 0\n",
      "Sum of NaN in neuroticism 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>...</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_quoteless</th>\n",
       "      <th>lang</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>906</td>\n",
       "      <td>-BigSexy-</td>\n",
       "      <td>Catechumen</td>\n",
       "      <td>Oooh i see</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1510236798</td>\n",
       "      <td>t5_2qra3</td>\n",
       "      <td>t3_7bqplg</td>\n",
       "      <td>t1_dpkf5mq</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>OrthodoxChristianity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>39.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145</td>\n",
       "      <td>-BlitzN9ne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>**Quality** material right here</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1549708109</td>\n",
       "      <td>t5_3cx36</td>\n",
       "      <td>t3_aopk72</td>\n",
       "      <td>t3_aopk72</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>en</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>367</td>\n",
       "      <td>-CrestiaBell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EA Indubitably</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1512615144</td>\n",
       "      <td>t5_2qh03</td>\n",
       "      <td>t3_7hzusb</td>\n",
       "      <td>t1_dqvr3u0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>gaming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>182</td>\n",
       "      <td>-CrestiaBell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>That's because we had to watch that cartoon in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1475867279</td>\n",
       "      <td>t5_2qh03</td>\n",
       "      <td>t3_56c8z3</td>\n",
       "      <td>t1_d8i510p</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>gaming</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>en</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>245</td>\n",
       "      <td>-CrestiaBell</td>\n",
       "      <td>I bet you thought my account would be here c:</td>\n",
       "      <td>[You will protect these smiles..](http://i.img...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1505862626</td>\n",
       "      <td>t5_2qh22</td>\n",
       "      <td>t3_716at9</td>\n",
       "      <td>t3_716at9</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>anime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>en</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        author                              author_flair_text  \\\n",
       "0    906     -BigSexy-                                     Catechumen   \n",
       "1    145    -BlitzN9ne                                            NaN   \n",
       "2    367  -CrestiaBell                                            NaN   \n",
       "3    182  -CrestiaBell                                            NaN   \n",
       "4    245  -CrestiaBell  I bet you thought my account would be here c:   \n",
       "\n",
       "                                                body  downs  created_utc  \\\n",
       "0                                         Oooh i see    NaN   1510236798   \n",
       "1                    **Quality** material right here    NaN   1549708109   \n",
       "2                                     EA Indubitably    NaN   1512615144   \n",
       "3  That's because we had to watch that cartoon in...    NaN   1475867279   \n",
       "4  [You will protect these smiles..](http://i.img...    NaN   1505862626   \n",
       "\n",
       "  subreddit_id    link_id   parent_id  score  ...             subreddit  ups  \\\n",
       "0     t5_2qra3  t3_7bqplg  t1_dpkf5mq    1.0  ...  OrthodoxChristianity  NaN   \n",
       "1     t5_3cx36  t3_aopk72   t3_aopk72   13.0  ...  UnethicalLifeProTips  NaN   \n",
       "2     t5_2qh03  t3_7hzusb  t1_dqvr3u0   15.0  ...                gaming  NaN   \n",
       "3     t5_2qh03  t3_56c8z3  t1_d8i510p   22.0  ...                gaming  NaN   \n",
       "4     t5_2qh22  t3_716at9   t3_716at9   22.0  ...                 anime  NaN   \n",
       "\n",
       "  word_count word_count_quoteless  lang  agreeableness  openness  \\\n",
       "0          3                    3    en           39.0      92.0   \n",
       "1          4                    4    en           50.0      85.0   \n",
       "2          2                    2    en           50.0      85.0   \n",
       "3         55                   55    en           50.0      85.0   \n",
       "4          5                    5    en           50.0      85.0   \n",
       "\n",
       "  conscientiousness  extraversion  neuroticism  \n",
       "0               1.0          18.0          4.0  \n",
       "1              15.0          50.0         30.0  \n",
       "2              50.0          85.0         50.0  \n",
       "3              50.0          85.0         50.0  \n",
       "4              50.0          85.0         50.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandora = pd.read_csv('/home/sophia/ma_py/pandora_bigfive1000.csv')\n",
    "#provide identifier\n",
    "# lst = []\n",
    "# for i in range(len(pandora)):\n",
    "#     lst.append(pandora.author[i] + str(i))\n",
    "# pandora['ident'] = lst\n",
    "\n",
    "# multiindex\n",
    "# pandora = pandora.set_index(['author', 'ident']).sort_index()\n",
    "\n",
    "authors = pd.read_csv('/home/sophia/ma_py/author_profiles.csv')\n",
    "# find missing data in big five traits\n",
    "authorslst = authors['author'].tolist()\n",
    "print(\"Author search: \", 'DarthHedonist' in authorslst)\n",
    "print(\"Author search: \", 'FonsoTheWhitesican' in authorslst)\n",
    "print(\"Author search: \", 'chaosking121' in authorslst)\n",
    "\n",
    "bigfive = authors[['author','agreeableness','openness','conscientiousness','extraversion','neuroticism']]\n",
    "bigfive = bigfive.dropna()\n",
    "# print(bigfive[bigfive['author'] == \"DarthHedonist\"])\n",
    "\n",
    "# pandoradf = pd.merge(pandora, bigfive, how='left', on='author')\n",
    "pandoradf = pandora.merge(bigfive, how='left', on=['author'])\n",
    "# pandoradf = pandoradf.dropna()\n",
    "pandoradf = pandoradf.sort_values(by='author')\n",
    "pandoradf = pandoradf[pandoradf['agreeableness'].notna()]\n",
    "pandoradf = pandoradf.reset_index()\n",
    "\n",
    "print(\"Length of dataframe: \", len(pandoradf))\n",
    "print(\"NaN in df? \", pandoradf.isnull().any().any())\n",
    "print(\"Sum of NaN in agreeableness\", pandoradf['agreeableness'].isnull().values.sum())\n",
    "print(\"Sum of NaN in openness\", pandoradf['openness'].isnull().values.sum())\n",
    "print(\"Sum of NaN in conscientiousness\", pandoradf['conscientiousness'].isnull().values.sum())\n",
    "print(\"Sum of NaN in extraversion\", pandoradf['extraversion'].isnull().values.sum())\n",
    "print(\"Sum of NaN in neuroticism\", pandoradf['neuroticism'].isnull().values.sum())\n",
    "# nan_values = pandoradf[pandoradf['neuroticism'].isna()]\n",
    "# nan_values\n",
    "pandoradf.head()\n",
    "# pandoradf[pandoradf.isnull().any(axis=1)]\n",
    "\n",
    "# number of entries does not fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust representations of some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change language to numeric representation\n",
    "def adjust(df):\n",
    "    # change lang to numerical representation\n",
    "    language = df['lang'].values.tolist()\n",
    "    language = set(language)\n",
    "    language\n",
    "    df['language']= np.select([df.lang == 'en', df.lang == 'es', df.lang == 'nl'], \n",
    "                            [0, 1, 2], \n",
    "                            default=3)\n",
    "    # print(gramsdf['language'])\n",
    "    df = df.drop(columns=['lang'])\n",
    "\n",
    "    # change big five to binary representation\n",
    "    df['agree'] = df['agreeableness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['openn'] = df['openness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['consc'] = df['conscientiousness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['extra'] = df['extraversion'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['neuro'] = df['neuroticism'].apply(lambda x: 0 if x<50 else 1)\n",
    "    return df\n",
    "# newpandoradf = adjust(pandoradf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_stopwordlist(df, mode):\n",
    "    if mode == 'NLTK':\n",
    "        stopwordList = stopwords.words('english')\n",
    "    if mode == 'NLTK-neg':\n",
    "        stopwordList = stopwords.words('english')\n",
    "        stopwordList.remove('no')\n",
    "        stopwordList.remove('nor')\n",
    "        stopwordList.remove('not')\n",
    "    return stopwordList\n",
    "\n",
    "# stopwordList = choose_stopwordlist(pandoradf, mode='NLTK-neg')\n",
    "\n",
    "# print(stopwordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "1. lower \n",
    "2. tokenize\n",
    "3. numbers to words\n",
    "4. delete special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# featuredf['probody'] = featuredf['body'].apply(lambda x:(decontracted(''.join(x))))\n",
    "# print(featuredf.iloc[5]['probody'])\n",
    "\n",
    "def senttokenize(df):\n",
    "    sentbody = []\n",
    "    for row in df['body']:\n",
    "        sentences = sent_tokenize(row)\n",
    "        sentbody.append(sentences)\n",
    "    df['senttokens'] = sentbody\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_stop_num_token(workdf, stopwordList):\n",
    "    # lower, remove special characters, remove stopwords\n",
    "    workdf['probody'] = workdf['probody'].apply(lambda x: ' '.join([x.lower() for x in x.split() if x.isalnum()]))\n",
    "    workdf['probody'] = workdf['probody'].apply(lambda x: ' '.join([x for x in x.split() if (x not in stopwordList)]))\n",
    "    newbody = []\n",
    "    newprobody = []\n",
    "    # num2words\n",
    "    for sentence in tqdm(workdf['probody']):\n",
    "        # string to list\n",
    "        inputtext = sentence.split()\n",
    "        numlist = []\n",
    "        for i in range(len(inputtext)):\n",
    "            if inputtext[i].isnumeric():\n",
    "                numlist.append(i)\n",
    "        for number in numlist:\n",
    "            inputtext[number] = num2words(inputtext[number])\n",
    "        \n",
    "        # list to string\n",
    "        celltext = ' '.join(inputtext)\n",
    "        newprobody.append(celltext)\n",
    "        # tokenize\n",
    "        words = word_tokenize(celltext)\n",
    "        newbody.append(words)\n",
    "    workdf['probody'] = newprobody\n",
    "    workdf['tokens'] = newbody\n",
    "    return workdf\n",
    "\n",
    "# preprocesseddf = preprocessing(featuredf)\n",
    "# print(preprocesseddf.iloc[2]['body'])\n",
    "# preprocesseddf.head()\n",
    "# preprocesseddf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "# Porter Stemmer\n",
    "def stemming(df):\n",
    "    ps = PorterStemmer()\n",
    "    df['tokens'] = df['tokens'].progress_apply(lambda x:([ps.stem(word) for word in x]))\n",
    "    return df\n",
    "\n",
    "# stemmeddf = stemming(preprocesseddf)\n",
    "# print(stemmeddf.iloc[1]['tokens'])\n",
    "# stemmeddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gramsdf.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordering(df):\n",
    "    lst = []\n",
    "    for i in range(len(df)):\n",
    "        lst.append(df.author[i] + str(i))\n",
    "    df['ident'] = lst\n",
    "    \n",
    "    cols_tomove = ['index', 'author', 'ident', 'body', 'probody', 'tokens', 'senttokens', 'agreeableness', 'openness', 'conscientiousness', 'extraversion', 'neuroticism', 'agree', 'openn', 'consc', 'extra', 'neuro', 'language']\n",
    "    orderdf  = df[cols_tomove + [col for col in df.columns if col not in cols_tomove]]\n",
    "#     orderdf.info(verbose=True)\n",
    "    return orderdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce4c9c0de1b4f2ba73199cc4fe93c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/975 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b388cd10834b77811f1a45e3c05502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/975 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          -BigSexy-0\n",
      "1         -BlitzN9ne1\n",
      "2       -CrestiaBell2\n",
      "3       -CrestiaBell3\n",
      "4       -CrestiaBell4\n",
      "            ...      \n",
      "970    zugzwang_03970\n",
      "971    zugzwang_03971\n",
      "972    zugzwang_03972\n",
      "973    zugzwang_03973\n",
      "974    zugzwang_03974\n",
      "Name: ident, Length: 975, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    # adjust some column representations\n",
    "    df = adjust(df)\n",
    "    # choose stopwordlist with or without negation\n",
    "    stopwordList = choose_stopwordlist(df, mode='NLTK-neg')\n",
    "    # decontract abbreviations (e.g., n't to not)\n",
    "    df['probody'] = df['body'].apply(lambda x:(decontracted(''.join(x))))\n",
    "    # create sentence tokens\n",
    "    df = senttokenize(df)\n",
    "    # lower, remove stopwords, num2words, tokenize\n",
    "    df = low_stop_num_token(df, stopwordList)\n",
    "    # porters stemmer\n",
    "    df = stemming(df)\n",
    "    df = ordering(df)\n",
    "    return df\n",
    "\n",
    "predf = preprocess(pandoradf)\n",
    "print(predf.ident)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 975 entries, 0 to 974\n",
      "Data columns (total 32 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   index                 975 non-null    int64  \n",
      " 1   author                975 non-null    object \n",
      " 2   ident                 975 non-null    object \n",
      " 3   body                  975 non-null    object \n",
      " 4   probody               975 non-null    object \n",
      " 5   tokens                975 non-null    object \n",
      " 6   senttokens            975 non-null    object \n",
      " 7   agreeableness         975 non-null    float64\n",
      " 8   openness              975 non-null    float64\n",
      " 9   conscientiousness     975 non-null    float64\n",
      " 10  extraversion          975 non-null    float64\n",
      " 11  neuroticism           975 non-null    float64\n",
      " 12  agree                 975 non-null    int64  \n",
      " 13  openn                 975 non-null    int64  \n",
      " 14  consc                 975 non-null    int64  \n",
      " 15  extra                 975 non-null    int64  \n",
      " 16  neuro                 975 non-null    int64  \n",
      " 17  language              975 non-null    int64  \n",
      " 18  author_flair_text     419 non-null    object \n",
      " 19  downs                 179 non-null    float64\n",
      " 20  created_utc           975 non-null    int64  \n",
      " 21  subreddit_id          975 non-null    object \n",
      " 22  link_id               975 non-null    object \n",
      " 23  parent_id             975 non-null    object \n",
      " 24  score                 975 non-null    float64\n",
      " 25  controversiality      975 non-null    int64  \n",
      " 26  gilded                975 non-null    int64  \n",
      " 27  id                    975 non-null    object \n",
      " 28  subreddit             975 non-null    object \n",
      " 29  ups                   381 non-null    float64\n",
      " 30  word_count            975 non-null    int64  \n",
      " 31  word_count_quoteless  975 non-null    int64  \n",
      "dtypes: float64(8), int64(12), object(12)\n",
      "memory usage: 243.9+ KB\n"
     ]
    }
   ],
   "source": [
    "predf.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               oooh see\n",
       "1                                         material right\n",
       "2                                         ea indubitably\n",
       "3      watch cartoon school martin luther king time t...\n",
       "4                                                protect\n",
       "                             ...                        \n",
       "970    keep extra clothing items soon get suit jacket...\n",
       "971    someone stumbled onto sub 5min thanks sticky g...\n",
       "972    institutions accommodate religious serious med...\n",
       "973    toss dirty laundry pile week way worn cheap pe...\n",
       "974            auto correct hates french thanks catching\n",
       "Name: probody, Length: 975, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predf['probody']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predf.to_pickle(\"preprocessed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
