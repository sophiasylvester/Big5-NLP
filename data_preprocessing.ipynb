{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing:\n",
    "\n",
    "TODO: \n",
    "- change pipeline such that three columns emerge: raw_body, pro_body and tokens\n",
    "- character ngrams \n",
    "\n",
    "Steps: \n",
    "1. Import data\n",
    "2. Extract features\n",
    "3. Preprocessing\n",
    "4. EMPATH\n",
    "5. Paraphrase database with personality scores \n",
    "\n",
    "Features:\n",
    "1. 11.140 ngram features: tf and tf-idf weighted word and character ngrams stemmed with Porter's stemmer\n",
    "2. type-token ratio\n",
    "3. ratio of comments in English\n",
    "4. ratio of British english vs. American English words\n",
    "5. 93 features from LIWC \n",
    "6. 26 PSYCH features (Preotiuc: Paraphrase Database and NRC Psycholinguistics Database)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import bigrams, ngrams\n",
    "import string\n",
    "from string import punctuation\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from num2words import num2words \n",
    "import pandas as pd\n",
    "from empath import Empath\n",
    "import random\n",
    "random.seed(32)\n",
    "\n",
    "# close nltk download window to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>...</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_quoteless</th>\n",
       "      <th>lang</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>Xaielao</td>\n",
       "      <td>Pharah</td>\n",
       "      <td>It seems to me that the least played character...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.463691e+09</td>\n",
       "      <td>t5_2u5kl</td>\n",
       "      <td>t3_4k2vck</td>\n",
       "      <td>t3_4k2vck</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Overwatch</td>\n",
       "      <td>2.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>en</td>\n",
       "      <td>78.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>BadgerKid96</td>\n",
       "      <td>19</td>\n",
       "      <td>Close.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.469684e+09</td>\n",
       "      <td>t5_2rjli</td>\n",
       "      <td>t3_4ur4p7</td>\n",
       "      <td>t1_d5sbnrs</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>teenagers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>en</td>\n",
       "      <td>77.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>Ambedo_1</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>ahh gotcha. thanks for replying. i think i do ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.479464e+09</td>\n",
       "      <td>t5_2qowo</td>\n",
       "      <td>t3_54j3ww</td>\n",
       "      <td>t1_da5gu9x</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>intj</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>en</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>WhatINeverSaid</td>\n",
       "      <td>[ISFJ]</td>\n",
       "      <td>No it isn't too much information. I would say ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.429376e+09</td>\n",
       "      <td>t5_2s90r</td>\n",
       "      <td>t3_32vycz</td>\n",
       "      <td>t1_cqfnjda</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mbti</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>en</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>mdhh99</td>\n",
       "      <td>http://smile.amazon.com/gp/registry/wishlist/2...</td>\n",
       "      <td>What type of skate? The trick to ice skate is ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.436839e+09</td>\n",
       "      <td>t5_2tx47</td>\n",
       "      <td>t3_3d6q1c</td>\n",
       "      <td>t1_ct2e6qm</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Random_Acts_Of_Amazon</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>en</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                  author_flair_text  \\\n",
       "932         Xaielao                                             Pharah   \n",
       "944     BadgerKid96                                                 19   \n",
       "962        Ambedo_1                                               INTJ   \n",
       "968  WhatINeverSaid                                             [ISFJ]   \n",
       "988          mdhh99  http://smile.amazon.com/gp/registry/wishlist/2...   \n",
       "\n",
       "                                                  body  downs   created_utc  \\\n",
       "932  It seems to me that the least played character...    0.0  1.463691e+09   \n",
       "944                                             Close.    0.0  1.469684e+09   \n",
       "962  ahh gotcha. thanks for replying. i think i do ...    0.0  1.479464e+09   \n",
       "968  No it isn't too much information. I would say ...    0.0  1.429376e+09   \n",
       "988  What type of skate? The trick to ice skate is ...    0.0  1.436839e+09   \n",
       "\n",
       "    subreddit_id    link_id   parent_id  score  controversiality  ...  \\\n",
       "932     t5_2u5kl  t3_4k2vck   t3_4k2vck    2.0               0.0  ...   \n",
       "944     t5_2rjli  t3_4ur4p7  t1_d5sbnrs    1.0               0.0  ...   \n",
       "962     t5_2qowo  t3_54j3ww  t1_da5gu9x    1.0               0.0  ...   \n",
       "968     t5_2s90r  t3_32vycz  t1_cqfnjda    1.0               0.0  ...   \n",
       "988     t5_2tx47  t3_3d6q1c  t1_ct2e6qm    2.0               0.0  ...   \n",
       "\n",
       "                 subreddit  ups word_count  word_count_quoteless  lang  \\\n",
       "932              Overwatch  2.0       98.0                  96.0    en   \n",
       "944              teenagers  1.0        1.0                   1.0    en   \n",
       "962                   intj  0.0      120.0                 120.0    en   \n",
       "968                   mbti  1.0       42.0                  42.0    en   \n",
       "988  Random_Acts_Of_Amazon  2.0       27.0                  27.0    en   \n",
       "\n",
       "     agreeableness openness  conscientiousness  extraversion  neuroticism  \n",
       "932           78.0     57.0               38.0          31.0         10.0  \n",
       "944           77.0     73.0               73.0           1.0         98.0  \n",
       "962           11.0      6.0               61.0           1.0         45.0  \n",
       "968           34.0     10.0               54.0          33.0         46.0  \n",
       "988            8.0      9.0               14.0          14.0         29.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandora = pd.read_csv('/home/sophia/ma_py/pandora_bigfive1000.csv')\n",
    "\n",
    "# # Total number of characters (including space)\n",
    "# data['char_count'] = data['body'].str.len()\n",
    "\n",
    "# # Total number of stopwords\n",
    "# data['stopwords'] = data['body'].apply(lambda x: len([x for x in x.split() if x in stopwordList]))\n",
    "\n",
    "# # Total number of punctuation or special characters\n",
    "# data['total_punc'] = data['body'].apply(lambda x: len([x for x in x.split() for j in x if j in string.punctuation]))\n",
    "\n",
    "# # Total number of numerics\n",
    "# data['total_num'] = data['body'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "# # Total number of uppercase words\n",
    "# data['total_uppercase'] = data['body'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "\n",
    "\n",
    "authors = pd.read_csv('/home/sophia/ma_py/author_profiles.csv')\n",
    "\n",
    "bigfive = authors[['author', 'agreeableness','openness','conscientiousness','extraversion','neuroticism']]\n",
    "bigfive = bigfive.dropna()\n",
    "\n",
    "pandoradf = pd.merge(pandora, bigfive, on='author', how='outer')\n",
    "pandoradf = pandoradf.dropna()\n",
    "pandoradf.tail()\n",
    "\n",
    "# pandoradf['probody'] = pandoradf['body']\n",
    "# pandoradf['tokens'] = pandoradf['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "def choose_stopwordlist(df, mode):\n",
    "    if mode == 'NLTK':\n",
    "        stopwordList = stopwords.words('english')\n",
    "    if mode == 'NLTK-neg':\n",
    "        stopwordList = stopwords.words('english')\n",
    "        stopwordList.remove('no')\n",
    "        stopwordList.remove('nor')\n",
    "        stopwordList.remove('not')\n",
    "    return stopwordList\n",
    "\n",
    "stopwordList = choose_stopwordlist(pandoradf, mode='NLTK-neg')\n",
    "\n",
    "print(stopwordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>...</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>total_punc</th>\n",
       "      <th>total_num</th>\n",
       "      <th>total_uppercase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sabata11792</td>\n",
       "      <td>Twilight Sparkle</td>\n",
       "      <td>That's subtle enough to just look like a coinc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.447260e+09</td>\n",
       "      <td>t5_2s8bl</td>\n",
       "      <td>t3_3sdrrj</td>\n",
       "      <td>t1_cwwel0w</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Shadow_Of_</td>\n",
       "      <td>Communist fag</td>\n",
       "      <td>Downturned nose, dirty skin, tattoos, small ch...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.493736e+09</td>\n",
       "      <td>t5_35j1r</td>\n",
       "      <td>t3_68roag</td>\n",
       "      <td>t3_68roag</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>xenomouse</td>\n",
       "      <td>INFJ/atelerix</td>\n",
       "      <td>Yes, if I was a man they'd call it a man cave....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.506875e+09</td>\n",
       "      <td>t5_2r39a</td>\n",
       "      <td>t3_73lfuz</td>\n",
       "      <td>t3_73lfuz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>207</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>eiznekk</td>\n",
       "      <td>Eiznek 1736-2104-7526 [Fighting:Machoke,Throh,...</td>\n",
       "      <td>Added you back! Thank you :D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.440447e+09</td>\n",
       "      <td>t5_2yt52</td>\n",
       "      <td>t3_3i8rel</td>\n",
       "      <td>t1_cue9ixy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>vitrael2</td>\n",
       "      <td>135x5</td>\n",
       "      <td>I squatted 225x14 a couple weeks ago and I mad...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.424723e+09</td>\n",
       "      <td>t5_34op9</td>\n",
       "      <td>t3_2wwmlc</td>\n",
       "      <td>t3_2wwmlc</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                  author_flair_text  \\\n",
       "1   Sabata11792                                   Twilight Sparkle   \n",
       "21   Shadow_Of_                                      Communist fag   \n",
       "34    xenomouse                                      INFJ/atelerix   \n",
       "37      eiznekk  Eiznek 1736-2104-7526 [Fighting:Machoke,Throh,...   \n",
       "62     vitrael2                                              135x5   \n",
       "\n",
       "                                                 body  downs   created_utc  \\\n",
       "1   That's subtle enough to just look like a coinc...    0.0  1.447260e+09   \n",
       "21  Downturned nose, dirty skin, tattoos, small ch...    0.0  1.493736e+09   \n",
       "34  Yes, if I was a man they'd call it a man cave....    0.0  1.506875e+09   \n",
       "37                       Added you back! Thank you :D    0.0  1.440447e+09   \n",
       "62  I squatted 225x14 a couple weeks ago and I mad...    0.0  1.424723e+09   \n",
       "\n",
       "   subreddit_id    link_id   parent_id  score  controversiality  ...  \\\n",
       "1      t5_2s8bl  t3_3sdrrj  t1_cwwel0w    4.0               0.0  ...   \n",
       "21     t5_35j1r  t3_68roag   t3_68roag    1.0               0.0  ...   \n",
       "34     t5_2r39a  t3_73lfuz   t3_73lfuz    2.0               0.0  ...   \n",
       "37     t5_2yt52  t3_3i8rel  t1_cue9ixy    1.0               0.0  ...   \n",
       "62     t5_34op9  t3_2wwmlc   t3_2wwmlc    8.0               0.0  ...   \n",
       "\n",
       "    agreeableness openness conscientiousness  extraversion  neuroticism  \\\n",
       "1             8.0     11.0              74.0           1.0         25.0   \n",
       "21           76.0     47.0               1.0           4.0         75.0   \n",
       "34           26.0     93.0              49.0          70.0         16.0   \n",
       "37           70.0     64.0               5.0           5.0         95.0   \n",
       "62           26.0     98.0              75.0          93.0         29.0   \n",
       "\n",
       "    char_count stopwords  total_punc  total_num  total_uppercase  \n",
       "1           53         3           2          0                0  \n",
       "21          95         0           8          0                0  \n",
       "34         207        16          16          0                4  \n",
       "37          28         2           2          0                1  \n",
       "62         108         5           2          0                2  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_features(workdata):\n",
    "\n",
    "    # Total number of characters (including space)\n",
    "    workdata['char_count'] = workdata['body'].str.len()\n",
    "\n",
    "    # Total number of stopwords\n",
    "    workdata['stopwords'] = workdata['body'].apply(lambda x: len([x for x in x.split() if x in stopwordList]))\n",
    "\n",
    "    # Total number of punctuation or special characters\n",
    "    workdata['total_punc'] = workdata['body'].apply(lambda x: len([x for x in x.split() for j in x if j in string.punctuation]))\n",
    "\n",
    "    # Total number of numerics\n",
    "    workdata['total_num'] = workdata['body'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "    # Total number of uppercase words\n",
    "    workdata['total_uppercase'] = workdata['body'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "    \n",
    "    return workdata\n",
    "\n",
    "featuredf = create_features(pandoradf)\n",
    "featuredf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "1. lower \n",
    "2. tokenize\n",
    "3. numbers to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man', 'call', 'man', 'guess', 'mouse', 'agree', 'not', 'safe', 'space', 'per', 'everything', 'want', 'not', 'mess']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 87 entries, 1 to 988\n",
      "Data columns (total 30 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   author                87 non-null     object \n",
      " 1   author_flair_text     87 non-null     object \n",
      " 2   body                  87 non-null     object \n",
      " 3   downs                 87 non-null     float64\n",
      " 4   created_utc           87 non-null     float64\n",
      " 5   subreddit_id          87 non-null     object \n",
      " 6   link_id               87 non-null     object \n",
      " 7   parent_id             87 non-null     object \n",
      " 8   score                 87 non-null     float64\n",
      " 9   controversiality      87 non-null     float64\n",
      " 10  gilded                87 non-null     float64\n",
      " 11  id                    87 non-null     object \n",
      " 12  subreddit             87 non-null     object \n",
      " 13  ups                   87 non-null     float64\n",
      " 14  word_count            87 non-null     float64\n",
      " 15  word_count_quoteless  87 non-null     float64\n",
      " 16  lang                  87 non-null     object \n",
      " 17  agreeableness         87 non-null     float64\n",
      " 18  openness              87 non-null     float64\n",
      " 19  conscientiousness     87 non-null     float64\n",
      " 20  extraversion          87 non-null     float64\n",
      " 21  neuroticism           87 non-null     float64\n",
      " 22  char_count            87 non-null     int64  \n",
      " 23  stopwords             87 non-null     int64  \n",
      " 24  total_punc            87 non-null     int64  \n",
      " 25  total_num             87 non-null     int64  \n",
      " 26  total_uppercase       87 non-null     int64  \n",
      " 27  probody               87 non-null     object \n",
      " 28  tokens                87 non-null     object \n",
      " 29  empath                87 non-null     object \n",
      "dtypes: float64(13), int64(5), object(12)\n",
      "memory usage: 23.1+ KB\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(workdf):\n",
    "    # lower, remove special characters, remove stopwords\n",
    "    workdf['probody'] = workdf['body'].apply(lambda x: ' '.join([x.lower() for x in x.split() if x.isalnum()]))\n",
    "    workdf['probody'] = workdf['probody'].apply(lambda x: ' '.join([x for x in x.split() if (x not in stopwordList)]))\n",
    "    newbody = []\n",
    "    # num2words\n",
    "    for sentence in workdf['probody']:\n",
    "        # string to list\n",
    "        inputtext = sentence.split()\n",
    "        numlist = []\n",
    "        for i in range(len(inputtext)):\n",
    "            if inputtext[i].isnumeric():\n",
    "                numlist.append(i)\n",
    "        for number in numlist:\n",
    "            inputtext[number] = num2words(inputtext[number])\n",
    "        \n",
    "        # list to string\n",
    "        celltext = ' '.join(inputtext)\n",
    "        # tokenize\n",
    "        celltext = word_tokenize(celltext)\n",
    "        newbody.append(celltext)   \n",
    "    workdf['tokens'] = newbody\n",
    "    return workdf\n",
    "\n",
    "preprocesseddf = preprocessing(featuredf)\n",
    "print(preprocesseddf.iloc[2]['tokens'])\n",
    "preprocesseddf.head()\n",
    "preprocesseddf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['downturn', 'dirti', 'small', 'small', 'obviou', 'would', 'not']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>...</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>total_punc</th>\n",
       "      <th>total_num</th>\n",
       "      <th>total_uppercase</th>\n",
       "      <th>probody</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sabata11792</td>\n",
       "      <td>Twilight Sparkle</td>\n",
       "      <td>That's subtle enough to just look like a coinc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.447260e+09</td>\n",
       "      <td>t5_2s8bl</td>\n",
       "      <td>t3_3sdrrj</td>\n",
       "      <td>t1_cwwel0w</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>subtle enough look like</td>\n",
       "      <td>[subtl, enough, look, like]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Shadow_Of_</td>\n",
       "      <td>Communist fag</td>\n",
       "      <td>Downturned nose, dirty skin, tattoos, small ch...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.493736e+09</td>\n",
       "      <td>t5_35j1r</td>\n",
       "      <td>t3_68roag</td>\n",
       "      <td>t3_68roag</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>downturned dirty small small obvious would not</td>\n",
       "      <td>[downturn, dirti, small, small, obviou, would,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>xenomouse</td>\n",
       "      <td>INFJ/atelerix</td>\n",
       "      <td>Yes, if I was a man they'd call it a man cave....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.506875e+09</td>\n",
       "      <td>t5_2r39a</td>\n",
       "      <td>t3_73lfuz</td>\n",
       "      <td>t3_73lfuz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>207</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>man call man guess mouse agree not safe space ...</td>\n",
       "      <td>[man, call, man, guess, mou, agr, not, safe, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>eiznekk</td>\n",
       "      <td>Eiznek 1736-2104-7526 [Fighting:Machoke,Throh,...</td>\n",
       "      <td>Added you back! Thank you :D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.440447e+09</td>\n",
       "      <td>t5_2yt52</td>\n",
       "      <td>t3_3i8rel</td>\n",
       "      <td>t1_cue9ixy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>added thank</td>\n",
       "      <td>[ad, thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>vitrael2</td>\n",
       "      <td>135x5</td>\n",
       "      <td>I squatted 225x14 a couple weeks ago and I mad...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.424723e+09</td>\n",
       "      <td>t5_34op9</td>\n",
       "      <td>t3_2wwmlc</td>\n",
       "      <td>t3_2wwmlc</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>squatted 225x14 couple weeks ago made sad card...</td>\n",
       "      <td>[squat, 225x14, coupl, week, ago, made, sad, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                  author_flair_text  \\\n",
       "1   Sabata11792                                   Twilight Sparkle   \n",
       "21   Shadow_Of_                                      Communist fag   \n",
       "34    xenomouse                                      INFJ/atelerix   \n",
       "37      eiznekk  Eiznek 1736-2104-7526 [Fighting:Machoke,Throh,...   \n",
       "62     vitrael2                                              135x5   \n",
       "\n",
       "                                                 body  downs   created_utc  \\\n",
       "1   That's subtle enough to just look like a coinc...    0.0  1.447260e+09   \n",
       "21  Downturned nose, dirty skin, tattoos, small ch...    0.0  1.493736e+09   \n",
       "34  Yes, if I was a man they'd call it a man cave....    0.0  1.506875e+09   \n",
       "37                       Added you back! Thank you :D    0.0  1.440447e+09   \n",
       "62  I squatted 225x14 a couple weeks ago and I mad...    0.0  1.424723e+09   \n",
       "\n",
       "   subreddit_id    link_id   parent_id  score  controversiality  ...  \\\n",
       "1      t5_2s8bl  t3_3sdrrj  t1_cwwel0w    4.0               0.0  ...   \n",
       "21     t5_35j1r  t3_68roag   t3_68roag    1.0               0.0  ...   \n",
       "34     t5_2r39a  t3_73lfuz   t3_73lfuz    2.0               0.0  ...   \n",
       "37     t5_2yt52  t3_3i8rel  t1_cue9ixy    1.0               0.0  ...   \n",
       "62     t5_34op9  t3_2wwmlc   t3_2wwmlc    8.0               0.0  ...   \n",
       "\n",
       "    conscientiousness extraversion neuroticism  char_count  stopwords  \\\n",
       "1                74.0          1.0        25.0          53          3   \n",
       "21                1.0          4.0        75.0          95          0   \n",
       "34               49.0         70.0        16.0         207         16   \n",
       "37                5.0          5.0        95.0          28          2   \n",
       "62               75.0         93.0        29.0         108          5   \n",
       "\n",
       "    total_punc total_num  total_uppercase  \\\n",
       "1            2         0                0   \n",
       "21           8         0                0   \n",
       "34          16         0                4   \n",
       "37           2         0                1   \n",
       "62           2         0                2   \n",
       "\n",
       "                                              probody  \\\n",
       "1                             subtle enough look like   \n",
       "21     downturned dirty small small obvious would not   \n",
       "34  man call man guess mouse agree not safe space ...   \n",
       "37                                        added thank   \n",
       "62  squatted 225x14 couple weeks ago made sad card...   \n",
       "\n",
       "                                               tokens  \n",
       "1                         [subtl, enough, look, like]  \n",
       "21  [downturn, dirti, small, small, obviou, would,...  \n",
       "34  [man, call, man, guess, mou, agr, not, safe, s...  \n",
       "37                                        [ad, thank]  \n",
       "62  [squat, 225x14, coupl, week, ago, made, sad, c...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "\n",
    "def stemming(df):\n",
    "    ps = PorterStemmer()\n",
    "    df['tokens'] = df['tokens'].apply(lambda x:([ps.stem(word) for word in x]))\n",
    "    return df\n",
    "\n",
    "stemmeddf = stemming(preprocesseddf)\n",
    "print(stemmeddf.iloc[1]['tokens'])\n",
    "stemmeddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>...</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>total_punc</th>\n",
       "      <th>total_num</th>\n",
       "      <th>total_uppercase</th>\n",
       "      <th>probody</th>\n",
       "      <th>tokens</th>\n",
       "      <th>empath</th>\n",
       "      <th>wordngrams</th>\n",
       "      <th>charngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sabata11792</td>\n",
       "      <td>Twilight Sparkle</td>\n",
       "      <td>That's subtle enough to just look like a coinc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.447260e+09</td>\n",
       "      <td>t5_2s8bl</td>\n",
       "      <td>t3_3sdrrj</td>\n",
       "      <td>t1_cwwel0w</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>subtle enough look like</td>\n",
       "      <td>[subtle, enough, look, like]</td>\n",
       "      <td>[{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...</td>\n",
       "      <td>(0, {'225x14': 0.0, '225x14 couple': 0.0, '225...</td>\n",
       "      <td>(0, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Shadow_Of_</td>\n",
       "      <td>Communist fag</td>\n",
       "      <td>Downturned nose, dirty skin, tattoos, small ch...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.493736e+09</td>\n",
       "      <td>t5_35j1r</td>\n",
       "      <td>t3_68roag</td>\n",
       "      <td>t3_68roag</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>downturned dirty small small obvious would not</td>\n",
       "      <td>[downturned, dirty, small, small, obvious, wou...</td>\n",
       "      <td>[{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...</td>\n",
       "      <td>(1, {'225x14': 0.0, '225x14 couple': 0.0, '225...</td>\n",
       "      <td>(1, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>xenomouse</td>\n",
       "      <td>INFJ/atelerix</td>\n",
       "      <td>Yes, if I was a man they'd call it a man cave....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.506875e+09</td>\n",
       "      <td>t5_2r39a</td>\n",
       "      <td>t3_73lfuz</td>\n",
       "      <td>t3_73lfuz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>man call man guess mouse agree not safe space ...</td>\n",
       "      <td>[man, call, man, guess, mouse, agree, not, saf...</td>\n",
       "      <td>[{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...</td>\n",
       "      <td>(2, {'225x14': 0.0, '225x14 couple': 0.0, '225...</td>\n",
       "      <td>(2, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>eiznekk</td>\n",
       "      <td>Eiznek 1736-2104-7526 [Fighting:Machoke,Throh,...</td>\n",
       "      <td>Added you back! Thank you :D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.440447e+09</td>\n",
       "      <td>t5_2yt52</td>\n",
       "      <td>t3_3i8rel</td>\n",
       "      <td>t1_cue9ixy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>added thank</td>\n",
       "      <td>[added, thank]</td>\n",
       "      <td>[{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...</td>\n",
       "      <td>(3, {'225x14': 0.0, '225x14 couple': 0.0, '225...</td>\n",
       "      <td>(3, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>vitrael2</td>\n",
       "      <td>135x5</td>\n",
       "      <td>I squatted 225x14 a couple weeks ago and I mad...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.424723e+09</td>\n",
       "      <td>t5_34op9</td>\n",
       "      <td>t3_2wwmlc</td>\n",
       "      <td>t3_2wwmlc</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>squatted 225x14 couple weeks ago made sad card...</td>\n",
       "      <td>[squatted, 225x14, couple, weeks, ago, made, s...</td>\n",
       "      <td>[{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...</td>\n",
       "      <td>(4, {'225x14': 0.18609151074643016, '225x14 co...</td>\n",
       "      <td>(4, {' 2': 0.1099928376810328, ' 22': 0.120178...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                  author_flair_text  \\\n",
       "1   Sabata11792                                   Twilight Sparkle   \n",
       "21   Shadow_Of_                                      Communist fag   \n",
       "34    xenomouse                                      INFJ/atelerix   \n",
       "37      eiznekk  Eiznek 1736-2104-7526 [Fighting:Machoke,Throh,...   \n",
       "62     vitrael2                                              135x5   \n",
       "\n",
       "                                                 body  downs   created_utc  \\\n",
       "1   That's subtle enough to just look like a coinc...    0.0  1.447260e+09   \n",
       "21  Downturned nose, dirty skin, tattoos, small ch...    0.0  1.493736e+09   \n",
       "34  Yes, if I was a man they'd call it a man cave....    0.0  1.506875e+09   \n",
       "37                       Added you back! Thank you :D    0.0  1.440447e+09   \n",
       "62  I squatted 225x14 a couple weeks ago and I mad...    0.0  1.424723e+09   \n",
       "\n",
       "   subreddit_id    link_id   parent_id  score  controversiality  ...  \\\n",
       "1      t5_2s8bl  t3_3sdrrj  t1_cwwel0w    4.0               0.0  ...   \n",
       "21     t5_35j1r  t3_68roag   t3_68roag    1.0               0.0  ...   \n",
       "34     t5_2r39a  t3_73lfuz   t3_73lfuz    2.0               0.0  ...   \n",
       "37     t5_2yt52  t3_3i8rel  t1_cue9ixy    1.0               0.0  ...   \n",
       "62     t5_34op9  t3_2wwmlc   t3_2wwmlc    8.0               0.0  ...   \n",
       "\n",
       "    char_count stopwords total_punc  total_num  total_uppercase  \\\n",
       "1           53         3          2          0                0   \n",
       "21          95         0          8          0                0   \n",
       "34         207        16         16          0                4   \n",
       "37          28         2          2          0                1   \n",
       "62         108         5          2          0                2   \n",
       "\n",
       "                                              probody  \\\n",
       "1                             subtle enough look like   \n",
       "21     downturned dirty small small obvious would not   \n",
       "34  man call man guess mouse agree not safe space ...   \n",
       "37                                        added thank   \n",
       "62  squatted 225x14 couple weeks ago made sad card...   \n",
       "\n",
       "                                               tokens  \\\n",
       "1                        [subtle, enough, look, like]   \n",
       "21  [downturned, dirty, small, small, obvious, wou...   \n",
       "34  [man, call, man, guess, mouse, agree, not, saf...   \n",
       "37                                     [added, thank]   \n",
       "62  [squatted, 225x14, couple, weeks, ago, made, s...   \n",
       "\n",
       "                                               empath  \\\n",
       "1   [{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...   \n",
       "21  [{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...   \n",
       "34  [{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...   \n",
       "37  [{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...   \n",
       "62  [{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...   \n",
       "\n",
       "                                           wordngrams  \\\n",
       "1   (0, {'225x14': 0.0, '225x14 couple': 0.0, '225...   \n",
       "21  (1, {'225x14': 0.0, '225x14 couple': 0.0, '225...   \n",
       "34  (2, {'225x14': 0.0, '225x14 couple': 0.0, '225...   \n",
       "37  (3, {'225x14': 0.0, '225x14 couple': 0.0, '225...   \n",
       "62  (4, {'225x14': 0.18609151074643016, '225x14 co...   \n",
       "\n",
       "                                           charngrams  \n",
       "1   (0, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...  \n",
       "21  (1, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...  \n",
       "34  (2, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...  \n",
       "37  (3, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...  \n",
       "62  (4, {' 2': 0.1099928376810328, ' 22': 0.120178...  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngrams(df, n_min, n_max, ngramtype):\n",
    "    # convert input from list to string\n",
    "    ngrams = []\n",
    "    inputtext = []\n",
    "    for sentence in df['tokens']:\n",
    "        text = ' '.join(sentence)\n",
    "        inputtext.append(text)\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n_min,n_max), analyzer=ngramtype) \n",
    "    vectors = vectorizer.fit_transform(inputtext)\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    names = vectorizer.get_feature_names()\n",
    "    ngramdf = pd.DataFrame(denselist, columns=names)\n",
    "    ngramdict = ngramdf.to_dict('index')\n",
    "    dict_items = list(ngramdict.items())    \n",
    "    return dict_items\n",
    "\n",
    "stemmeddf['wordngrams'] = ngrams(stemmeddf, 1, 3, 'word')\n",
    "stemmeddf['charngrams'] = ngrams(stemmeddf, 2, 3, 'char')\n",
    "stemmeddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>...</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>total_punc</th>\n",
       "      <th>total_num</th>\n",
       "      <th>total_uppercase</th>\n",
       "      <th>probody</th>\n",
       "      <th>tokens</th>\n",
       "      <th>empath</th>\n",
       "      <th>wordngrams</th>\n",
       "      <th>charngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sabata11792</td>\n",
       "      <td>Twilight Sparkle</td>\n",
       "      <td>That's subtle enough to just look like a coinc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.447260e+09</td>\n",
       "      <td>t5_2s8bl</td>\n",
       "      <td>t3_3sdrrj</td>\n",
       "      <td>t1_cwwel0w</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>subtle enough look like</td>\n",
       "      <td>[subtle, enough, look, like]</td>\n",
       "      <td>[{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...</td>\n",
       "      <td>(0, {'225x14': 0.0, '225x14 couple': 0.0, '225...</td>\n",
       "      <td>(0, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Shadow_Of_</td>\n",
       "      <td>Communist fag</td>\n",
       "      <td>Downturned nose, dirty skin, tattoos, small ch...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.493736e+09</td>\n",
       "      <td>t5_35j1r</td>\n",
       "      <td>t3_68roag</td>\n",
       "      <td>t3_68roag</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>downturned dirty small small obvious would not</td>\n",
       "      <td>[downturned, dirty, small, small, obvious, wou...</td>\n",
       "      <td>[{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...</td>\n",
       "      <td>(1, {'225x14': 0.0, '225x14 couple': 0.0, '225...</td>\n",
       "      <td>(1, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>xenomouse</td>\n",
       "      <td>INFJ/atelerix</td>\n",
       "      <td>Yes, if I was a man they'd call it a man cave....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.506875e+09</td>\n",
       "      <td>t5_2r39a</td>\n",
       "      <td>t3_73lfuz</td>\n",
       "      <td>t3_73lfuz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>man call man guess mouse agree not safe space ...</td>\n",
       "      <td>[man, call, man, guess, mouse, agree, not, saf...</td>\n",
       "      <td>[{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...</td>\n",
       "      <td>(2, {'225x14': 0.0, '225x14 couple': 0.0, '225...</td>\n",
       "      <td>(2, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>eiznekk</td>\n",
       "      <td>Eiznek 1736-2104-7526 [Fighting:Machoke,Throh,...</td>\n",
       "      <td>Added you back! Thank you :D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.440447e+09</td>\n",
       "      <td>t5_2yt52</td>\n",
       "      <td>t3_3i8rel</td>\n",
       "      <td>t1_cue9ixy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>added thank</td>\n",
       "      <td>[added, thank]</td>\n",
       "      <td>[{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...</td>\n",
       "      <td>(3, {'225x14': 0.0, '225x14 couple': 0.0, '225...</td>\n",
       "      <td>(3, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>vitrael2</td>\n",
       "      <td>135x5</td>\n",
       "      <td>I squatted 225x14 a couple weeks ago and I mad...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.424723e+09</td>\n",
       "      <td>t5_34op9</td>\n",
       "      <td>t3_2wwmlc</td>\n",
       "      <td>t3_2wwmlc</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>squatted 225x14 couple weeks ago made sad card...</td>\n",
       "      <td>[squatted, 225x14, couple, weeks, ago, made, s...</td>\n",
       "      <td>[{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...</td>\n",
       "      <td>(4, {'225x14': 0.18609151074643016, '225x14 co...</td>\n",
       "      <td>(4, {' 2': 0.1099928376810328, ' 22': 0.120178...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                  author_flair_text  \\\n",
       "1   Sabata11792                                   Twilight Sparkle   \n",
       "21   Shadow_Of_                                      Communist fag   \n",
       "34    xenomouse                                      INFJ/atelerix   \n",
       "37      eiznekk  Eiznek 1736-2104-7526 [Fighting:Machoke,Throh,...   \n",
       "62     vitrael2                                              135x5   \n",
       "\n",
       "                                                 body  downs   created_utc  \\\n",
       "1   That's subtle enough to just look like a coinc...    0.0  1.447260e+09   \n",
       "21  Downturned nose, dirty skin, tattoos, small ch...    0.0  1.493736e+09   \n",
       "34  Yes, if I was a man they'd call it a man cave....    0.0  1.506875e+09   \n",
       "37                       Added you back! Thank you :D    0.0  1.440447e+09   \n",
       "62  I squatted 225x14 a couple weeks ago and I mad...    0.0  1.424723e+09   \n",
       "\n",
       "   subreddit_id    link_id   parent_id  score  controversiality  ...  \\\n",
       "1      t5_2s8bl  t3_3sdrrj  t1_cwwel0w    4.0               0.0  ...   \n",
       "21     t5_35j1r  t3_68roag   t3_68roag    1.0               0.0  ...   \n",
       "34     t5_2r39a  t3_73lfuz   t3_73lfuz    2.0               0.0  ...   \n",
       "37     t5_2yt52  t3_3i8rel  t1_cue9ixy    1.0               0.0  ...   \n",
       "62     t5_34op9  t3_2wwmlc   t3_2wwmlc    8.0               0.0  ...   \n",
       "\n",
       "    char_count stopwords total_punc  total_num  total_uppercase  \\\n",
       "1           53         3          2          0                0   \n",
       "21          95         0          8          0                0   \n",
       "34         207        16         16          0                4   \n",
       "37          28         2          2          0                1   \n",
       "62         108         5          2          0                2   \n",
       "\n",
       "                                              probody  \\\n",
       "1                             subtle enough look like   \n",
       "21     downturned dirty small small obvious would not   \n",
       "34  man call man guess mouse agree not safe space ...   \n",
       "37                                        added thank   \n",
       "62  squatted 225x14 couple weeks ago made sad card...   \n",
       "\n",
       "                                               tokens  \\\n",
       "1                        [subtle, enough, look, like]   \n",
       "21  [downturned, dirty, small, small, obvious, wou...   \n",
       "34  [man, call, man, guess, mouse, agree, not, saf...   \n",
       "37                                     [added, thank]   \n",
       "62  [squatted, 225x14, couple, weeks, ago, made, s...   \n",
       "\n",
       "                                               empath  \\\n",
       "1   [{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...   \n",
       "21  [{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...   \n",
       "34  [{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...   \n",
       "37  [{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...   \n",
       "62  [{'help': 0.0, 'office': 0.0, 'dance': 0.0, 'm...   \n",
       "\n",
       "                                           wordngrams  \\\n",
       "1   (0, {'225x14': 0.0, '225x14 couple': 0.0, '225...   \n",
       "21  (1, {'225x14': 0.0, '225x14 couple': 0.0, '225...   \n",
       "34  (2, {'225x14': 0.0, '225x14 couple': 0.0, '225...   \n",
       "37  (3, {'225x14': 0.0, '225x14 couple': 0.0, '225...   \n",
       "62  (4, {'225x14': 0.18609151074643016, '225x14 co...   \n",
       "\n",
       "                                           charngrams  \n",
       "1   (0, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...  \n",
       "21  (1, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...  \n",
       "34  (2, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...  \n",
       "37  (3, {' 2': 0.0, ' 22': 0.0, ' 2s': 0.0, ' a': ...  \n",
       "62  (4, {' 2': 0.1099928376810328, ' 22': 0.120178...  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_empath(df):\n",
    "\n",
    "    empath = Empath()\n",
    "    df['empath'] = df['tokens'].apply(lambda x:([empath.analyze(sentence, normalize=True) for sentence in x]))\n",
    "    \n",
    "#     empathvalues = []\n",
    "\n",
    "#     for sentence in comments['body']:\n",
    "#         empathvalues.append(empath.analyze(sentence, normalize=True))\n",
    "#     comments['empath'] = empathvalues\n",
    "    \n",
    "    return df\n",
    "\n",
    "empathdf = apply_empath(stemmeddf)\n",
    "empathdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Idilismiguzel/NLP-with-Python/blob/master/Text-Classification.ipynb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(empathdf.wordngrams, empathdf.extraversion, test_size=0.25, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'tuple'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-479902bb1a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlogisticRegr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogisticRegr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[1;32m   1345\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    737\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    738\u001b[0m         \"\"\"\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ma_py/.venv/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logisticRegr.predict(x_test)\n",
    "\n",
    "score = logisticRegr.score(x_test, y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
