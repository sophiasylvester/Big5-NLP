{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing:\n",
    "\n",
    "### Features (as in paper):\n",
    "1. 11.140 ngram features: tf and tf-idf weighted word and character ngrams stemmed with Porter's stemmer\n",
    "2. type-token ratio\n",
    "3. ratio of comments in English\n",
    "4. ratio of British english vs. American English words\n",
    "5. 93 features from LIWC \n",
    "6. 26 PSYCH features (Preotiuc: Paraphrase Database and MRC Psycholinguistics Database)\n",
    "\n",
    "### Columns (from the description of the dataset):\n",
    "1. 'global':[7,10], #subreddits_commented, subreddits_commented_mbti, num_comments\n",
    "2. 'liwc':[10,103], #liwc\n",
    "3. 'word':[103,3938], #top1000 word ngram (1,2,3) per dimension based on chi2\n",
    "4. 'char':[3938,7243], #top1000 char ngrams (2,3) per dimension based on chi2\n",
    "5. 'sub':[7243,12228], #number of comments in each subreddit\n",
    "6. 'ent':[12228,12229], #entropy\n",
    "7. 'subtf':[12229,17214], #tf-idf on subreddits\n",
    "8. 'subcat':[17214,17249], #manually crafted subreddit categories\n",
    "9. 'lda50':[17249,17299], #50 LDA topics\n",
    "10. 'posts':[17299,17319], #posts statistics\n",
    "11. 'lda100':[17319,17419], #100 LDA topics\n",
    "12. 'psy':[17419,17443], #psycholinguistic features\n",
    "13. 'en':[17443,17444], #ratio of english comments\n",
    "14. 'ttr':[17444,17445], #type token ratio\n",
    "15. 'meaning':[17445,17447], #additional pyscholinguistic features\n",
    "16. 'time_diffs':[17447,17453], #commenting time diffs\n",
    "17. 'month':[17453,17465], #monthly distribution\n",
    "18. 'hour':[17465,17489], #hourly distribution\n",
    "19. 'day_of_week':[17489,17496], #daily distribution\n",
    "20. 'word_an':[17496,21496], #word ngrams selected by F-score\n",
    "21. 'word_an_tf':[21496,25496], #tf-idf ngrams selected by F-score\n",
    "22. 'char_an':[25496,29496], #char ngrams selected by F-score\n",
    "23. 'char_an_tf':[29496,33496], #tf-idf char ngrams selected by F-score\n",
    "24. 'brit_amer':[33496,33499], #british vs american english ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sophia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import bigrams, ngrams\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from collections import Counter\n",
    "from num2words import num2words \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "random.seed(32)\n",
    "\n",
    "# close nltk download window to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>downs</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>score</th>\n",
       "      <th>...</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>ups</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_quoteless</th>\n",
       "      <th>lang</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>932</td>\n",
       "      <td>Xaielao</td>\n",
       "      <td>Pharah</td>\n",
       "      <td>It seems to me that the least played character...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.463691e+09</td>\n",
       "      <td>t5_2u5kl</td>\n",
       "      <td>t3_4k2vck</td>\n",
       "      <td>t3_4k2vck</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Overwatch</td>\n",
       "      <td>2.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>en</td>\n",
       "      <td>78.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>944</td>\n",
       "      <td>BadgerKid96</td>\n",
       "      <td>19</td>\n",
       "      <td>Close.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.469684e+09</td>\n",
       "      <td>t5_2rjli</td>\n",
       "      <td>t3_4ur4p7</td>\n",
       "      <td>t1_d5sbnrs</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>teenagers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>en</td>\n",
       "      <td>77.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>962</td>\n",
       "      <td>Ambedo_1</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>ahh gotcha. thanks for replying. i think i do ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.479464e+09</td>\n",
       "      <td>t5_2qowo</td>\n",
       "      <td>t3_54j3ww</td>\n",
       "      <td>t1_da5gu9x</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>intj</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>en</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>968</td>\n",
       "      <td>WhatINeverSaid</td>\n",
       "      <td>[ISFJ]</td>\n",
       "      <td>No it isn't too much information. I would say ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.429376e+09</td>\n",
       "      <td>t5_2s90r</td>\n",
       "      <td>t3_32vycz</td>\n",
       "      <td>t1_cqfnjda</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>mbti</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>en</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>988</td>\n",
       "      <td>mdhh99</td>\n",
       "      <td>http://smile.amazon.com/gp/registry/wishlist/2...</td>\n",
       "      <td>What type of skate? The trick to ice skate is ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.436839e+09</td>\n",
       "      <td>t5_2tx47</td>\n",
       "      <td>t3_3d6q1c</td>\n",
       "      <td>t1_ct2e6qm</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Random_Acts_Of_Amazon</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>en</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index          author                                  author_flair_text  \\\n",
       "82    932         Xaielao                                             Pharah   \n",
       "83    944     BadgerKid96                                                 19   \n",
       "84    962        Ambedo_1                                               INTJ   \n",
       "85    968  WhatINeverSaid                                             [ISFJ]   \n",
       "86    988          mdhh99  http://smile.amazon.com/gp/registry/wishlist/2...   \n",
       "\n",
       "                                                 body  downs   created_utc  \\\n",
       "82  It seems to me that the least played character...    0.0  1.463691e+09   \n",
       "83                                             Close.    0.0  1.469684e+09   \n",
       "84  ahh gotcha. thanks for replying. i think i do ...    0.0  1.479464e+09   \n",
       "85  No it isn't too much information. I would say ...    0.0  1.429376e+09   \n",
       "86  What type of skate? The trick to ice skate is ...    0.0  1.436839e+09   \n",
       "\n",
       "   subreddit_id    link_id   parent_id  score  ...              subreddit  \\\n",
       "82     t5_2u5kl  t3_4k2vck   t3_4k2vck    2.0  ...              Overwatch   \n",
       "83     t5_2rjli  t3_4ur4p7  t1_d5sbnrs    1.0  ...              teenagers   \n",
       "84     t5_2qowo  t3_54j3ww  t1_da5gu9x    1.0  ...                   intj   \n",
       "85     t5_2s90r  t3_32vycz  t1_cqfnjda    1.0  ...                   mbti   \n",
       "86     t5_2tx47  t3_3d6q1c  t1_ct2e6qm    2.0  ...  Random_Acts_Of_Amazon   \n",
       "\n",
       "    ups word_count word_count_quoteless  lang  agreeableness  openness  \\\n",
       "82  2.0       98.0                 96.0    en           78.0      57.0   \n",
       "83  1.0        1.0                  1.0    en           77.0      73.0   \n",
       "84  0.0      120.0                120.0    en           11.0       6.0   \n",
       "85  1.0       42.0                 42.0    en           34.0      10.0   \n",
       "86  2.0       27.0                 27.0    en            8.0       9.0   \n",
       "\n",
       "   conscientiousness  extraversion  neuroticism  \n",
       "82              38.0          31.0         10.0  \n",
       "83              73.0           1.0         98.0  \n",
       "84              61.0           1.0         45.0  \n",
       "85              54.0          33.0         46.0  \n",
       "86              14.0          14.0         29.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandora = pd.read_csv('/home/sophia/ma_py/pandora_bigfive1000.csv')\n",
    "\n",
    "authors = pd.read_csv('/home/sophia/ma_py/author_profiles.csv')\n",
    "\n",
    "bigfive = authors[['author', 'agreeableness','openness','conscientiousness','extraversion','neuroticism']]\n",
    "bigfive = bigfive.dropna()\n",
    "\n",
    "pandoradf = pd.merge(pandora, bigfive, on='author', how='outer')\n",
    "pandoradf = pandoradf.dropna()\n",
    "pandoradf = pandoradf.reset_index()\n",
    "pandoradf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust representations of some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change language to numeric representation\n",
    "def adjust(df):\n",
    "    # change lang to numerical representation\n",
    "    language = df['lang'].values.tolist()\n",
    "    language = set(language)\n",
    "    language\n",
    "    df['language']= np.select([df.lang == 'en', df.lang == 'es', df.lang == 'nl'], \n",
    "                            [0, 1, 2], \n",
    "                            default=3)\n",
    "    # print(gramsdf['language'])\n",
    "    df = df.drop(columns=['lang'])\n",
    "\n",
    "    # change big five to binary representation\n",
    "    df['agree'] = df['agreeableness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['openn'] = df['openness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['consc'] = df['conscientiousness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['extra'] = df['extraversion'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['neuro'] = df['neuroticism'].apply(lambda x: 0 if x<50 else 1)\n",
    "    return df\n",
    "# newpandoradf = adjust(pandoradf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_stopwordlist(df, mode):\n",
    "    if mode == 'NLTK':\n",
    "        stopwordList = stopwords.words('english')\n",
    "    if mode == 'NLTK-neg':\n",
    "        stopwordList = stopwords.words('english')\n",
    "        stopwordList.remove('no')\n",
    "        stopwordList.remove('nor')\n",
    "        stopwordList.remove('not')\n",
    "    return stopwordList\n",
    "\n",
    "# stopwordList = choose_stopwordlist(pandoradf, mode='NLTK-neg')\n",
    "\n",
    "# print(stopwordList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "1. lower \n",
    "2. tokenize\n",
    "3. numbers to words\n",
    "4. delete special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not like this description by the way, too vague and disorganized and short also wth I thought you were TYPE_MENTION there is no way in hell you are TYPE_MENTION\n"
     ]
    }
   ],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "featuredf['probody'] = featuredf['body'].apply(lambda x:(decontracted(''.join(x))))\n",
    "print(featuredf.iloc[5]['probody'])\n",
    "\n",
    "def senttokenize(df):\n",
    "    sentbody = []\n",
    "    for row in df['body']:\n",
    "        sentences = sent_tokenize(row)\n",
    "        sentbody.append(sentences)\n",
    "    df['senttokens'] = sentbody\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_stop_num_token(workdf):\n",
    "    # lower, remove special characters, remove stopwords\n",
    "    workdf['probody'] = workdf['probody'].apply(lambda x: ' '.join([x.lower() for x in x.split() if x.isalnum()]))\n",
    "    workdf['probody'] = workdf['probody'].apply(lambda x: ' '.join([x for x in x.split() if (x not in stopwordList)]))\n",
    "    newbody = []\n",
    "    newprobody = []\n",
    "    # num2words\n",
    "    for sentence in tqdm(workdf['probody']):\n",
    "        # string to list\n",
    "        inputtext = sentence.split()\n",
    "        numlist = []\n",
    "        for i in range(len(inputtext)):\n",
    "            if inputtext[i].isnumeric():\n",
    "                numlist.append(i)\n",
    "        for number in numlist:\n",
    "            inputtext[number] = num2words(inputtext[number])\n",
    "        \n",
    "        # list to string\n",
    "        celltext = ' '.join(inputtext)\n",
    "        newprobody.append(celltext)\n",
    "        # tokenize\n",
    "        words = word_tokenize(celltext)\n",
    "        newbody.append(words)\n",
    "    workdf['probody'] = newprobody\n",
    "    workdf['tokens'] = newbody\n",
    "    return workdf\n",
    "\n",
    "# preprocesseddf = preprocessing(featuredf)\n",
    "# print(preprocesseddf.iloc[2]['body'])\n",
    "# preprocesseddf.head()\n",
    "# preprocesseddf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "# Porter Stemmer\n",
    "def stemming(df):\n",
    "    ps = PorterStemmer()\n",
    "    df['tokens'] = df['tokens'].progress_apply(lambda x:([ps.stem(word) for word in x]))\n",
    "    return df\n",
    "\n",
    "# stemmeddf = stemming(preprocesseddf)\n",
    "# print(stemmeddf.iloc[1]['tokens'])\n",
    "# stemmeddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gramsdf.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordering(df):\n",
    "    cols_tomove = ['index', 'author', 'body', 'probody', 'tokens', 'senttokens', 'agreeableness', 'openness', 'conscientiousness', 'extraversion', 'neuroticism', 'agree', 'openn', 'consc', 'extra', 'neuro', 'language']\n",
    "    orderdf  = df[cols_tomove + [col for col in df.columns if col not in cols_tomove]]\n",
    "#     orderdf.info(verbose=True)\n",
    "    return orderdf\n",
    "\n",
    "# orderdf = ordering(gramsdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7559e9ec083d468bbe25b56f30a66fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56176f4b3d54367ba02162c524ae178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    # adjust some column representations\n",
    "    df = adjust(df)\n",
    "    # choose stopwordlist with or without negation\n",
    "    stopwordList = choose_stopwordlist(df, mode='NLTK-neg')\n",
    "    # decontract abbreviations (e.g., n't to not)\n",
    "    df['probody'] = df['body'].apply(lambda x:(decontracted(''.join(x))))\n",
    "    # create sentence tokens\n",
    "    df = senttokenize(df)\n",
    "    # lower, remove stopwords, num2words, tokenize\n",
    "    df = low_stop_num_token(df)\n",
    "    # porters stemmer\n",
    "    df = stemming(df)\n",
    "    df = ordering(df)\n",
    "    return df\n",
    "\n",
    "predf = preprocess(pandoradf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87 entries, 0 to 86\n",
      "Data columns (total 31 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   index                 87 non-null     int64  \n",
      " 1   author                87 non-null     object \n",
      " 2   body                  87 non-null     object \n",
      " 3   probody               87 non-null     object \n",
      " 4   tokens                87 non-null     object \n",
      " 5   senttokens            87 non-null     object \n",
      " 6   agreeableness         87 non-null     float64\n",
      " 7   openness              87 non-null     float64\n",
      " 8   conscientiousness     87 non-null     float64\n",
      " 9   extraversion          87 non-null     float64\n",
      " 10  neuroticism           87 non-null     float64\n",
      " 11  agree                 87 non-null     int64  \n",
      " 12  openn                 87 non-null     int64  \n",
      " 13  consc                 87 non-null     int64  \n",
      " 14  extra                 87 non-null     int64  \n",
      " 15  neuro                 87 non-null     int64  \n",
      " 16  language              87 non-null     int64  \n",
      " 17  author_flair_text     87 non-null     object \n",
      " 18  downs                 87 non-null     float64\n",
      " 19  created_utc           87 non-null     float64\n",
      " 20  subreddit_id          87 non-null     object \n",
      " 21  link_id               87 non-null     object \n",
      " 22  parent_id             87 non-null     object \n",
      " 23  score                 87 non-null     float64\n",
      " 24  controversiality      87 non-null     float64\n",
      " 25  gilded                87 non-null     float64\n",
      " 26  id                    87 non-null     object \n",
      " 27  subreddit             87 non-null     object \n",
      " 28  ups                   87 non-null     float64\n",
      " 29  word_count            87 non-null     float64\n",
      " 30  word_count_quoteless  87 non-null     float64\n",
      "dtypes: float64(13), int64(7), object(11)\n",
      "memory usage: 21.2+ KB\n"
     ]
    }
   ],
   "source": [
    "predf.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               subtle enough look like\n",
       "1        downturned dirty small small obvious would not\n",
       "2     man would call man guess mouse agree not safe ...\n",
       "3                                           added thank\n",
       "4     squatted 225x14 couple weeks ago made sad card...\n",
       "                            ...                        \n",
       "82    seems least played characters also characters ...\n",
       "83                                                     \n",
       "84    ahh thanks think care people not societal thin...\n",
       "85    no not much would say probably not fact behavi...\n",
       "86    type trick ice skate lean would imagine would ...\n",
       "Name: probody, Length: 87, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predf['probody']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "predf.to_pickle(\"preprocessed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
