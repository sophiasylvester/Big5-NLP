{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All functions for personality prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sophia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/sophia/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import bigrams, ngrams\n",
    "\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "from empath import Empath\n",
    "\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from lexicalrichness import LexicalRichness\n",
    "import textblob\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import datetime\n",
    "import random\n",
    "random.seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset with comments\n",
    "df = pd.read_csv('/home/sophia/ma_py/pandora_bigfive.csv')\n",
    "\n",
    "# Import dataset authors and delete not needed columns\n",
    "authors = pd.read_csv('/home/sophia/ma_py/author_profiles.csv')\n",
    "bigfive = authors[['author','agreeableness','openness','conscientiousness','extraversion','neuroticism']]\n",
    "bigfive = bigfive[bigfive['agreeableness'].notna()]\n",
    "del authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# change language to numeric representation\n",
    "def numeric_lang(df):\n",
    "    # change lang to numerical representation\n",
    "    language = df['lang'].values.tolist()\n",
    "    language = set(language)\n",
    "    language\n",
    "    df['language']= np.select([df.lang == 'en', df.lang == 'es', df.lang == 'nl'], \n",
    "                            [0, 1, 2], \n",
    "                            default=3)\n",
    "    # print(gramsdf['language'])\n",
    "    df = df.drop(columns=['lang'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# create time columns from UTC\n",
    "def create_timecolumns(df):\n",
    "    readable = []\n",
    "    weekday = []\n",
    "    month = []\n",
    "    year = []\n",
    "    for row in tqdm(df['created_utc']):\n",
    "        item = datetime.datetime.fromtimestamp(row)\n",
    "        weekday_item = item.strftime('%A')\n",
    "        readable_item = datetime.datetime.fromtimestamp(row).isoformat()\n",
    "        month.append(str(readable_item[5:7]))\n",
    "        year.append(str(readable_item[0:4]))\n",
    "        readable.append(readable_item)\n",
    "        weekday.append(weekday_item.lower())\n",
    "    df['time'] = readable\n",
    "    df['weekday'] = weekday\n",
    "    df['month'] = month\n",
    "    df['year'] = year\n",
    "    return df\n",
    "\n",
    "# count occurences in time columns to get time distribution\n",
    "def timecounter(lst, vocablst):\n",
    "    if vocablst == 'weekday':\n",
    "        vocab = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    elif vocablst == 'month':\n",
    "        vocab = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    elif vocablst == 'year':\n",
    "        vocab = ['2015', '2016', '2017', '2018', '2019']\n",
    "    else:\n",
    "        print(\"No valid input: vocab list\")\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", vocabulary=vocab)\n",
    "    vectors = vectorizer.fit_transform(lst)\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "# create a list of all subreddits in the dataset\n",
    "lst = df['subreddit'].tolist()\n",
    "lst = [item.lower() for item in lst]\n",
    "subredditset = set(lst)\n",
    "subredditlist = list(subredditset)\n",
    "\n",
    "# count occurences of subreddits \n",
    "def subredditcounter(lst, subredditlst):\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", vocabulary=subredditlist)\n",
    "    vectors = vectorizer.fit_transform(lst)\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "# aggregate dataset to get one row per author and create new columns for time and subreddit\n",
    "def create_groupdf(df): \n",
    "    print(\"\\tCreate numeric language representation...\")\n",
    "    df = numeric_lang(df)\n",
    "    print(\"\\tCreate time columns...\")\n",
    "    df = create_timecolumns(df)\n",
    "    # create dictionary for aggregation function\n",
    "    d = {'lang': ['nunique'] , 'controversiality': ['mean'], 'gilded': ['mean'], \n",
    "         'body': (' '. join), 'doc_body': (lambda x : list(x)),\n",
    "         'utc': (lambda x : list(x)), 'subreddit': (' '. join), 'num_subreddit': ['nunique'],\n",
    "         'weekday': (' '. join), 'month': (' '. join), 'year': (' '. join)}\n",
    "    # 'ยง'. join(x)\n",
    " \n",
    "    # new ungrouped columns\n",
    "    print(\"\\tCreate new ungrouped columns...\")\n",
    "    df['body'] = df['body'].apply(lambda x: str(x))\n",
    "    df['doc_body'] = df['body']\n",
    "    df['num_subreddit'] = df['subreddit']\n",
    "    df['lang'] = df['language'].apply(lambda x: str(x))\n",
    "    df['utc'] = df['created_utc'].apply(lambda x: str(x))\n",
    "#     df['subreddit'] = df['subreddit'].apply(lambda x: [x.lower()])\n",
    "    df['subreddit'] = df['subreddit'].apply(lambda x: ''.join(x.lower()))\n",
    "    \n",
    "    # create df groupd by author + transform\n",
    "    print(\"\\tGroup df by author...\")\n",
    "    groupdf = df.groupby(['author']).agg(d)\n",
    "    groupdf = groupdf.reset_index()\n",
    "    groupdf.columns = groupdf.columns.droplevel(1)\n",
    "    return groupdf\n",
    "    \n",
    "def create_new_columns(df):    \n",
    "    # body\n",
    "#     print(\"\\tCreate doc_body...\")\n",
    "# #     df['doc_body'] =  df['doc_body'].apply(lambda x: [x.split(\"ยง\") for x in x])\n",
    "#     # created_utc\n",
    "#     print(\"\\tCreate utc list...\")\n",
    "#     df['all_utc'] = df['utc_lst'].apply(lambda x: x.split())\n",
    "    # controversiality\n",
    "    print(\"\\tCreate controversiality column...\")\n",
    "    df['controversiality'] = df['controversiality'].fillna(0)\n",
    "    # gilded\n",
    "    print(\"\\tCreate mean_gilded...\")\n",
    "    df['gilded'] = df['gilded'].fillna(0)\n",
    "    # number of comments per subreddit\n",
    "    print(\"\\tCreate subreddit_dist...\")\n",
    "    subreddit_predist = subredditcounter(df['subreddit'], subredditlist)\n",
    "    subreddit_predist = subreddit_predist.tolist()\n",
    "    df['subreddit_dist'] = subreddit_predist\n",
    "    # time\n",
    "    print(\"\\tCreate weekday_dist...\")\n",
    "    weekday = timecounter(df['weekday'], 'weekday')\n",
    "    weekday = weekday.tolist()\n",
    "    df['weekday_dist'] = weekday\n",
    "    print(\"\\tCreate month_dist...\")\n",
    "    month = timecounter(df['month'], 'month')\n",
    "    month = month.tolist()\n",
    "    df['month_dist'] = month\n",
    "    print(\"\\tCreate year_dist...\")\n",
    "    year = timecounter(df['year'], 'year')\n",
    "    year = year.tolist()\n",
    "    df['year_dist'] = year\n",
    "    \n",
    "    print(\"\\tCreate new aggregated df...\")\n",
    "    newdf = df[['author', 'body', 'doc_body', 'utc', 'controversiality', \n",
    "                'gilded', 'num_subreddit', 'subreddit_dist', 'weekday_dist', \n",
    "                'month_dist', 'year_dist', 'lang']]\n",
    "    print(\"\\tSort new aggregated df...\")\n",
    "    newdf = newdf.sort_values(by='author')\n",
    "    print(\"\\tDrop duplicates in new aggregated df...\")\n",
    "    newdf = newdf.drop_duplicates(subset=['author'])\n",
    "    return newdf\n",
    "\n",
    "# test = create_groupdf(df)\n",
    "# del df\n",
    "# newtest = create_new_columns(test)\n",
    "# newtest.head()\n",
    "\n",
    "# get one column for each feature in the distributions of time and subreddit\n",
    "weekday = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "month = ['january', 'february', 'march', 'april', 'may', 'june', 'juli', 'august', 'september', 'october', 'november', 'december']\n",
    "year = ['2015', '2016', '2017', '2018', '2019']\n",
    "\n",
    "def onecolumnperdatapoint(df, column, namelist):\n",
    "    for i in tqdm(range(len(namelist))):\n",
    "        df[namelist[i]] = df[column].apply(lambda x:[x[i]])\n",
    "        df[namelist[i]] = [item[0] for item in df[namelist[i]]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create comment df (name: pandora)...\n",
      "Create new df grouped by author...\n",
      "\tCreate numeric language representation...\n",
      "\tCreate time columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb05b5af786342e6a4aba3ec9f9479e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3103208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCreate new ungrouped columns...\n",
      "\tGroup df by author...\n",
      "Create new columns with features...\n",
      "\tCreate controversiality column...\n",
      "\tCreate mean_gilded...\n",
      "\tCreate subreddit_dist...\n",
      "\tCreate weekday_dist...\n",
      "\tCreate month_dist...\n",
      "\tCreate year_dist...\n",
      "\tCreate new aggregated df...\n",
      "\tSort new aggregated df...\n",
      "\tDrop duplicates in new aggregated df...\n",
      "Distribute the weekday_dist to several columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f1e90908904cbda6d3be2979ba0ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribute the month_dist to several columns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51610fe9bf343e3b705d0d84c3985f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribute the year_dist to several columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e4f853624f46e690a2f6a05193e72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribute the subreddit_dist to several columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77fb9e2b9c741c7a7ca3689a95a14f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16063 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop dist columns...\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "# Wrapper for commentdf\n",
    "def create_commentdf(df):\n",
    "    print(\"Create new df grouped by author...\")\n",
    "    groupdf = create_groupdf(df)\n",
    "    print(\"Create new columns with features...\")\n",
    "    pandora = create_new_columns(groupdf)\n",
    "    print(\"Distribute the weekday_dist to several columns...\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'weekday_dist', weekday)\n",
    "    print(\"Distribute the month_dist to several columns\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'month_dist', month)\n",
    "    print(\"Distribute the year_dist to several columns...\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'year_dist', year)\n",
    "    print(\"Distribute the subreddit_dist to several columns...\")\n",
    "    pandora = onecolumnperdatapoint(pandora, 'subreddit_dist', subredditlist)\n",
    "    print(\"Drop dist columns...\")\n",
    "    pandora.drop(['weekday_dist', 'month_dist', 'year_dist', 'subreddit_dist'], axis=1, inplace=True)\n",
    "    return pandora\n",
    "\n",
    "# create commentdf\n",
    "print(\"Create comment df (name: pandora)...\")\n",
    "pandora = create_commentdf(df)\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Sort pandora df\")\n",
    "# pandora= pandora.sort_values(by='author')\n",
    "# print(\"Sort big five df\")\n",
    "# bigfive= bigfive.sort_values(by='author')\n",
    "# # pandora = pandora.set_index('author')\n",
    "# # bigfive = bigfive.set_index('author')\n",
    "# pandoradf = pandora.join(bigfive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_joining(pandoradf, bigfive):\n",
    "#     for i in range(100):\n",
    "#         number = random.randint(0, 1605)\n",
    "#         print(pandoradf.iloc[number]['neuroticism'] == bigfive.iloc[number]['neuroticism'], \n",
    "#         pandoradf.iloc[number]['neuroticism'], bigfive.iloc[number]['neuroticism'])\n",
    "\n",
    "# test_joining(pandoradf, bigfive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_sorted(pandora, bigfive):\n",
    "#     for i in range(100):\n",
    "#         number = random.randint(0, 1606)\n",
    "#         print(pandora.iloc[number]['author'] == bigfive.iloc[number]['author'], \n",
    "#         pandora.iloc[number]['author'], bigfive.iloc[number]['author'])\n",
    "\n",
    "# test_sorted(pandora, bigfive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigfive.index.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort pandora df\n",
      "Sort big five df\n",
      "Join commentdf and authordf\n",
      "Create binary representations for each personality trait\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>doc_body</th>\n",
       "      <th>utc</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>gilded</th>\n",
       "      <th>num_subreddit</th>\n",
       "      <th>lang</th>\n",
       "      <th>monday</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>wednesday</th>\n",
       "      <th>...</th>\n",
       "      <th>finlandconspiracy</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agree</th>\n",
       "      <th>openn</th>\n",
       "      <th>consc</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-Areopagan-</th>\n",
       "      <td>Your first and second question is the same que...</td>\n",
       "      <td>[Your first and second question is the same qu...</td>\n",
       "      <td>[1513882848, 1513744846, 1522253427, 151370438...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-BigSexy-</th>\n",
       "      <td>I've been asked to cum everywhere with my ex j...</td>\n",
       "      <td>[I've been asked to cum everywhere with my ex ...</td>\n",
       "      <td>[1507650565, 1516397088, 1502590403, 151682490...</td>\n",
       "      <td>0.020737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>444</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-BlitzN9ne</th>\n",
       "      <td>I'm currently in the middle of making a Payday...</td>\n",
       "      <td>[I'm currently in the middle of making a Payda...</td>\n",
       "      <td>[1422166355, 1423504286, 1449881503, 145521567...</td>\n",
       "      <td>0.014159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>393</td>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-CrestiaBell</th>\n",
       "      <td>First and foremost I extend my condolences to ...</td>\n",
       "      <td>[First and foremost I extend my condolences to...</td>\n",
       "      <td>[1462304635, 1528773104, 1513663029, 148131600...</td>\n",
       "      <td>0.017687</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>1258</td>\n",
       "      <td>0</td>\n",
       "      <td>1210</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-dyad-</th>\n",
       "      <td>I failed both...I'm great at reading people ir...</td>\n",
       "      <td>[I failed both...I'm great at reading people i...</td>\n",
       "      <td>[1475875524, 1473096864, 1505168466, 150318014...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zugzwang_03</th>\n",
       "      <td>You know that giggly group of women going to t...</td>\n",
       "      <td>[You know that giggly group of women going to ...</td>\n",
       "      <td>[1466099531, 1469625145, 1455352713, 150837533...</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>146</td>\n",
       "      <td>3</td>\n",
       "      <td>2409</td>\n",
       "      <td>0</td>\n",
       "      <td>2317</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuluthrone</th>\n",
       "      <td>I saw some speculate that the \"download\" would...</td>\n",
       "      <td>[I saw some speculate that the \"download\" woul...</td>\n",
       "      <td>[1438979642, 1451195516, 1468833505, 151331960...</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zwelg</th>\n",
       "      <td>I am actually pretty pleased about my score:Ag...</td>\n",
       "      <td>[I am actually pretty pleased about my score:A...</td>\n",
       "      <td>[1508185843]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zymmaster</th>\n",
       "      <td>Respectfully disagree. Offense had plenty of i...</td>\n",
       "      <td>[Respectfully disagree. Offense had plenty of ...</td>\n",
       "      <td>[1455228093, 1476665332, 1468599441, 146004506...</td>\n",
       "      <td>0.010444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>394</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyzee</th>\n",
       "      <td>Tarzaned can't be compared in this situation. ...</td>\n",
       "      <td>[Tarzaned can't be compared in this situation....</td>\n",
       "      <td>[1494445453, 1487154474, 1521448885, 142322120...</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1606 rows ร 16099 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           body  \\\n",
       "author                                                            \n",
       "-Areopagan-   Your first and second question is the same que...   \n",
       "-BigSexy-     I've been asked to cum everywhere with my ex j...   \n",
       "-BlitzN9ne    I'm currently in the middle of making a Payday...   \n",
       "-CrestiaBell  First and foremost I extend my condolences to ...   \n",
       "-dyad-        I failed both...I'm great at reading people ir...   \n",
       "...                                                         ...   \n",
       "zugzwang_03   You know that giggly group of women going to t...   \n",
       "zuluthrone    I saw some speculate that the \"download\" would...   \n",
       "zwelg         I am actually pretty pleased about my score:Ag...   \n",
       "zymmaster     Respectfully disagree. Offense had plenty of i...   \n",
       "zyzee         Tarzaned can't be compared in this situation. ...   \n",
       "\n",
       "                                                       doc_body  \\\n",
       "author                                                            \n",
       "-Areopagan-   [Your first and second question is the same qu...   \n",
       "-BigSexy-     [I've been asked to cum everywhere with my ex ...   \n",
       "-BlitzN9ne    [I'm currently in the middle of making a Payda...   \n",
       "-CrestiaBell  [First and foremost I extend my condolences to...   \n",
       "-dyad-        [I failed both...I'm great at reading people i...   \n",
       "...                                                         ...   \n",
       "zugzwang_03   [You know that giggly group of women going to ...   \n",
       "zuluthrone    [I saw some speculate that the \"download\" woul...   \n",
       "zwelg         [I am actually pretty pleased about my score:A...   \n",
       "zymmaster     [Respectfully disagree. Offense had plenty of ...   \n",
       "zyzee         [Tarzaned can't be compared in this situation....   \n",
       "\n",
       "                                                            utc  \\\n",
       "author                                                            \n",
       "-Areopagan-   [1513882848, 1513744846, 1522253427, 151370438...   \n",
       "-BigSexy-     [1507650565, 1516397088, 1502590403, 151682490...   \n",
       "-BlitzN9ne    [1422166355, 1423504286, 1449881503, 145521567...   \n",
       "-CrestiaBell  [1462304635, 1528773104, 1513663029, 148131600...   \n",
       "-dyad-        [1475875524, 1473096864, 1505168466, 150318014...   \n",
       "...                                                         ...   \n",
       "zugzwang_03   [1466099531, 1469625145, 1455352713, 150837533...   \n",
       "zuluthrone    [1438979642, 1451195516, 1468833505, 151331960...   \n",
       "zwelg                                              [1508185843]   \n",
       "zymmaster     [1455228093, 1476665332, 1468599441, 146004506...   \n",
       "zyzee         [1494445453, 1487154474, 1521448885, 142322120...   \n",
       "\n",
       "              controversiality    gilded  num_subreddit  lang  monday  \\\n",
       "author                                                                  \n",
       "-Areopagan-           0.000000  0.000000              1     1       0   \n",
       "-BigSexy-             0.020737  0.000000            147     4     359   \n",
       "-BlitzN9ne            0.014159  0.000000            116     4     393   \n",
       "-CrestiaBell          0.017687  0.000866            149     4    1258   \n",
       "-dyad-                0.000000  0.000000              5     2      25   \n",
       "...                        ...       ...            ...   ...     ...   \n",
       "zugzwang_03           0.011709  0.000291            146     3    2409   \n",
       "zuluthrone            0.018458  0.000000             46     3     119   \n",
       "zwelg                 0.000000  0.000000              1     1       1   \n",
       "zymmaster             0.010444  0.000000             99     3     300   \n",
       "zyzee                 0.010989  0.000000             13     3      11   \n",
       "\n",
       "              tuesday  wednesday  ...  finlandconspiracy  agreeableness  \\\n",
       "author                            ...                                     \n",
       "-Areopagan-         0          2  ...                  0            0.0   \n",
       "-BigSexy-           0        444  ...                  0           39.0   \n",
       "-BlitzN9ne          0        343  ...                  0           50.0   \n",
       "-CrestiaBell        0       1210  ...                  0           50.0   \n",
       "-dyad-              0         40  ...                  0           60.0   \n",
       "...               ...        ...  ...                ...            ...   \n",
       "zugzwang_03         0       2317  ...                  0           10.0   \n",
       "zuluthrone          0        142  ...                  0           17.0   \n",
       "zwelg               0          0  ...                  0           39.0   \n",
       "zymmaster           0        394  ...                  0           28.0   \n",
       "zyzee               0         19  ...                  0           88.0   \n",
       "\n",
       "              openness  conscientiousness  extraversion  neuroticism  agree  \\\n",
       "author                                                                        \n",
       "-Areopagan-       99.0               96.0          60.0          1.0      0   \n",
       "-BigSexy-         92.0                1.0          18.0          4.0      0   \n",
       "-BlitzN9ne        85.0               15.0          50.0         30.0      1   \n",
       "-CrestiaBell      85.0               50.0          85.0         50.0      1   \n",
       "-dyad-            67.0               45.0          10.0         47.0      1   \n",
       "...                ...                ...           ...          ...    ...   \n",
       "zugzwang_03       41.0               86.0          83.0         18.0      0   \n",
       "zuluthrone        96.0               28.0          95.0         34.0      0   \n",
       "zwelg             89.0               91.0          80.0          3.0      0   \n",
       "zymmaster         47.0               62.0          21.0         49.0      0   \n",
       "zyzee             78.0               31.0          75.0         10.0      1   \n",
       "\n",
       "              openn  consc  extra  \n",
       "author                             \n",
       "-Areopagan-       1      1      1  \n",
       "-BigSexy-         1      0      0  \n",
       "-BlitzN9ne        1      0      1  \n",
       "-CrestiaBell      1      1      1  \n",
       "-dyad-            1      0      0  \n",
       "...             ...    ...    ...  \n",
       "zugzwang_03       0      1      1  \n",
       "zuluthrone        1      0      1  \n",
       "zwelg             1      1      1  \n",
       "zymmaster         0      1      0  \n",
       "zyzee             1      0      1  \n",
       "\n",
       "[1606 rows x 16099 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge commentdf and authordf\n",
    "print(\"Sort pandora df\")\n",
    "pandora= pandora.sort_values(by='author')\n",
    "print(\"Sort big five df\")\n",
    "bigfive= bigfive.sort_values(by='author')\n",
    "if pandora.index.name != 'author':\n",
    "    pandora = pandora.set_index('author')\n",
    "if bigfive.index.name != 'author':\n",
    "    bigfive = bigfive.set_index('author')\n",
    "print(\"Join commentdf and authordf\")\n",
    "pandoradf = pandora.join(bigfive)\n",
    "# pandoradf = pandoradf.reset_index()\n",
    "\n",
    "# create binary representation of personality traits\n",
    "def bigfive_cat(df):\n",
    "    # change big five to binary representation\n",
    "    df['agree'] = df['agreeableness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['openn'] = df['openness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['consc'] = df['conscientiousness'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['extra'] = df['extraversion'].apply(lambda x: 0 if x<50 else 1)\n",
    "    df['neuro'] = df['neuroticism'].apply(lambda x: 0 if x<50 else 1)\n",
    "    return df\n",
    "\n",
    "print(\"Create binary representations for each personality trait\")\n",
    "pandoradf = bigfive_cat(pandoradf)\n",
    "pandoradf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# define stopwordlist to use\n",
    "def choose_stopwordlist(df, mode):\n",
    "    if mode == 'NLTK':\n",
    "        stopwordList = stopwords.words('english')\n",
    "    if mode == 'NLTK-neg':\n",
    "        stopwordList = stopwords.words('english')\n",
    "        stopwordList.remove('no')\n",
    "        stopwordList.remove('nor')\n",
    "        stopwordList.remove('not')\n",
    "    return stopwordList\n",
    "\n",
    "# remove decontractions\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# create sentence tokens\n",
    "def senttokenize(df):\n",
    "    sentbody = []\n",
    "    for row in tqdm(df['doc_body']):\n",
    "        sentitem = []\n",
    "        for item in row:\n",
    "            sentences = sent_tokenize(item)\n",
    "            sentitem.append(sentences)\n",
    "        sentbody.append(sentitem)\n",
    "    df['senttokens'] = sentbody\n",
    "    return df\n",
    "\n",
    "# lower words and remove special characters\n",
    "def lower_special(df):\n",
    "    newrow = []\n",
    "    for row in tqdm(df['probody']):\n",
    "        newcomment = []\n",
    "        for comment in row:\n",
    "            text_pre = \"\"\n",
    "            for character in comment:\n",
    "                if character.isalnum() or character.isspace():\n",
    "                    character = character.lower()\n",
    "                    text_pre += character\n",
    "                else:\n",
    "                    text_pre += \" \"\n",
    "            newcomment.append(text_pre)\n",
    "        newrow.append(newcomment)   \n",
    "    df['probody'] = newrow\n",
    "    return df\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stopwords(df, stopwordList):\n",
    "    newprobody = []\n",
    "    for row in tqdm(df['probody']):\n",
    "        newrowprobody = []\n",
    "        for comment in row:\n",
    "            words = [word for word in comment.split() if (word not in stopwordList)]\n",
    "            newcomment = ' '.join(words)\n",
    "            newrowprobody.append(newcomment)\n",
    "        newprobody.append(newrowprobody)\n",
    "    df['probody'] = newprobody\n",
    "    return df\n",
    "\n",
    "# change numbers to words and tokenize words\n",
    "\n",
    "import decimal\n",
    "def num_tokenize(df):    \n",
    "    newbody_complete = []\n",
    "    newprobody_complete = []\n",
    "    # num2words\n",
    "    for row in tqdm(df['probody']):\n",
    "        newbody = []\n",
    "        newprobody = []\n",
    "        for sentence in row:\n",
    "            # string to list\n",
    "            inputtext = sentence.split()\n",
    "            numlist = []\n",
    "            for i in range(len(inputtext)):\n",
    "                if inputtext[i].isnumeric():\n",
    "                    numlist.append(i)\n",
    "            for number in numlist:\n",
    "                # deleted: fractions, superscripts, extremely large numbers, ๅๅ, ไธ\n",
    "                try:\n",
    "                    inputtext[number] = num2words(inputtext[number])\n",
    "                except decimal.InvalidOperation:\n",
    "                    inputtext[number] = \" \"\n",
    "                except OverflowError:\n",
    "                    inputtext[number] = \" \"\n",
    "\n",
    "            # list to string\n",
    "            inputtext = [word for word in inputtext if word.isalpha()]\n",
    "            celltext = ' '.join(inputtext)\n",
    "            newprobody.append(celltext)\n",
    "            # tokenize\n",
    "            words = word_tokenize(celltext)\n",
    "            newbody.append(words)\n",
    "        newbody_complete.append(newbody)\n",
    "        newprobody_complete.append(newprobody)\n",
    "    df['probody'] = newprobody_complete\n",
    "    df['tokens'] = newbody_complete\n",
    "    return df\n",
    "\n",
    "# Porter Stemmer\n",
    "def stemming(df):\n",
    "    ps = PorterStemmer()\n",
    "    for row in tqdm(df['tokens']):\n",
    "        for comment in row:\n",
    "            words = [ps.stem(word) for word in comment]\n",
    "            comment = ' '.join(words)\n",
    "    return df\n",
    "\n",
    "# bring columns of dataframe in correct order\n",
    "def ordering(df):\n",
    "    cols_tomove = ['body', 'doc_body', 'probody', 'tokens', 'senttokens', 'agreeableness', 'openness', 'conscientiousness', 'extraversion', 'neuroticism', 'agree', 'openn', 'consc', 'extra', 'neuro']\n",
    "    orderdf  = df[cols_tomove + [col for col in df.columns if col not in cols_tomove]]\n",
    "    return orderdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decontract...\n",
      "Tokenize Sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe5a56a3d7740f897c8b7a5ca767df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower words and remove special characters...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd285bbdafe0464897a75a31dde0a8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fc03d9c19f4b0db79f7188870743c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change numbers to words and tokenize words...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0005179f7b44b46b75701b8f1b1c39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ยฝ\n",
      "ยฝ\n",
      "ยฒ\n",
      "โ\n",
      "โ\n",
      "โ\n",
      "โ\n",
      "ยฒ\n",
      "ยน\n",
      "ยฒ\n",
      "โด\n",
      "ยน\n",
      "โถ\n",
      "โถ\n",
      "1ยฝ\n",
      "9ยฝ\n",
      "ไธ\n",
      "ยฝ\n",
      "ยฝ\n",
      "โ\n",
      "ยฒ\n",
      "ยฒ\n",
      "โก\n",
      "โข\n",
      "ยฒ\n",
      "ยฒ\n",
      "ไธ\n",
      "ไบ\n",
      "ไธ\n",
      "ๅ\n",
      "ไบ\n",
      "ๅญ\n",
      "ไธ\n",
      "ๅซ\n",
      "ไน\n",
      "2ยฝ\n",
      "2ยฝ\n",
      "ๅฃน\n",
      "ยฝ\n",
      "ๅ็พ้ถๅ\n",
      "ๅญ\n",
      "ๅฃน่ดฐๅ่ไผ้ๆๆ็ๆพ\n",
      "99999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999\n",
      "99999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999\n",
      "2ยฝ\n",
      "3ยฝ\n",
      "2ยฝ\n",
      "1ยฝ\n",
      "1ยฝ\n",
      "2ยฝ\n",
      "ยฝ\n",
      "999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999\n",
      "999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999\n",
      "999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9ยฝ\n",
      "ยฝ\n",
      "ยฝ\n",
      "โจ\n",
      "ยน\n",
      "ยน\n",
      "ยฒ\n",
      "ยณ\n",
      "ยน\n",
      "ยน\n",
      "ยน\n",
      "ยน\n",
      "ยน\n",
      "ยน\n",
      "ยน\n",
      "โฐ\n",
      "โฐ\n",
      "ยน\n",
      "ยน\n",
      "1ยฝ\n",
      "2ยฝ\n",
      "ยฝ\n",
      "ยผ\n",
      "ยผ\n",
      "ยผ\n",
      "ยพ\n",
      "ยน\n",
      "123456789101112131415161718192020212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000\n",
      "ยฒ\n",
      "ยณ\n",
      "ยฒ\n",
      "ยฒ\n",
      "ยพ\n",
      "ยณ\n",
      "ยฝ\n",
      "ยฒ\n",
      "ยณ\n",
      "2ยฝ\n",
      "โ\n",
      "6ยฝ\n",
      "3238462643383279502884197169399375105820974944592307816406286208998628034825342117067982148086513282306647093844609550582231725359408128481117450284102701938521105559644622948954930381964428810975665933446128475648233786783165271201909145648566923460348610454326648213393607260249141273724587006606315588174881520920962829254091715364367892590360011330530548820466521384146951941511609433057270365759591953092186117381932611793105118548074462379962749567351885752724891227938183011949129833673362440656643086021394946395224737190702179860943702770539217176293176752384674818467669405132000568127145263560827785771342757789609173637178721468440901224953430146549585371050792279689258923542019956112129021960864034418159813629774771309960518707211349999998372978049951059731732816096318595024459455346908302642522308253344685035261931188171010003137838752886587533208381420617177669147303598253490428755468731159562863882353787593751957781857780532171226806613001927876611195909216420198938095257201065485863278865936153381827968230301952035301852968995773622599413891249721775283479131515574857242454150695950829533116861727855889075098381754637464939319255060400927701671139009848824012858361603563707660104710181942955596198946767837449448255379774726847104047534646208046684259069491293313677028989152104752162056966024058038150193511253382430035587640247496473263914199272604269922796782354781636009341721641219924586315030286182974555706749838505494588586926995690927210797509302955321165344987202755960236480665499119881834797753566369807426542527862551818417574672890977772793800081647060016145249192173217214772350141441973568548161361157352552133475741849468438523323907394143334547762416862518983569485562099219222184272550254256887671790494601653466804988627232791786085784383827967976681454100953883786360950680064225125205117392984896084128488626945604241965285022210661186306744278622039194945047123713786960956364371917287467764657573962413890865832645995813390478027590099465764078951269468398352595709825822620522489407726719478268482601476990902640136394437455305068203496252451749399651431429809190659250937221696461515709858387410597885959772975498930161753928468138268683868942774155991855925245953959431049972524680845987273644695848653836736222626099124608051243884390451244136549762780797715691435997700129616089441694868555848406353422072225828488648158456028506016842739452267467678895252138522549954666727823986456596116354886230577456498035593634568174324112515076069479451096596094025228879710893145669136867228748940560101503308617928680920874760917824938589009714909675985261365549781893129784821682998948722658804857564014270477555132379641451523746234364542858444795265867821051141354735739523113427166102135969536231442952484937187110145765403590279934403742007310578539062198387447808478489683321445713868751943506430218453191048481005370614680674919278191197939952061419663428754440643745123718192179998391015919561814675142691239748940907186494231961567945208095146550225231603881930142093762137855956638937787083039069792077346722182562599661501421503068038447734549202605414665925201497442850732518666002132434088190710486331734649651453905796268561005508106658796998163574736384052571459102897064140110971206280439039759515677157700420337869936007230558763176359421873125147120532928191826186125867321579198414848829164470609575270695722091756711672291098169091528017350671274858322287183520935396572512108357915136988209144421006751033467110314126711136990865851639831501970165151168517143765761835155650884909989859982387345528331635507647918535893226185489632132933089857064204675259070915481416549859461637180270981994309924488957571282890592323326097299712084433573265489382391193259746366730583604142813883032038249037589852437441702913276561809377344403070746921120191302033038019762110110044929321516084244485963766983895228684783123552658213144957685726243344189303968642624341077322697802807318915441101044682325271620105265227211166039666557309254711055785376346682065310989652691862056476931257058635662018558100729360659876486117910453348850346113657686753249441668039626579787718556084552965412665408530614344431858676975145661406800700237877659134401712749470420562230538994561314071127000407854733269939081454664645880797270826683063432858785698305235808933065757406795457163775254202114955761581400250126228594130216471550979259230990796547376125517656751357517829666454779174501129961489030463994713296210734043751895735961458901938971311179042978285647503203198691514028708085990480109412147221317947647772622414254854540332157185306142288137585043063321751829798662237172159160771669254748738986654949450114654062843366393790039769265672146385306736096571209180763832716641627488880078692560290228472104031721186082041900042296617119637792133757511495950156604963186294726547364252308177036751590673502350728354056704038674351362222477158915049530984448933309634087807693259939780541934144737744184263129860809988868741326047215695162396586457302163159819319516735381297416772947867242292465436680098067692823828068996400482435403701416314965897940924323789690706977942236250822168895738379862300159377647165122893578601588161755782973523344604281512627203734314653197777416031990665541876397929334419521541341899485444734567383162499341913181480927777103863877343177207545654532207770921201905166096280490926360197598828161332316663652861932668633606273567630354477628035045077723554710585954870279081435624014517180624643626794561275318134078330336254232783944975382437205835311477119926063813346776879695970309833913077109870408591337464144282277263465947047458784778720192771528073176790770715721344473060570073349243693113835049316312840425121925651798069411352801314701304781643788518529092854520116583934196562134914341595625865865570552690496520985803385072242648293972858478316305777756068887644624824685792603953527734803048029005876075825104747091643961362676044925627420420832085661190625454337213153595845068772460290161876679524061634252257719542916299193064553779914037340432875262888963995879475729174642635745525407909145135711136941091193932519107602082520261879853188770584297259167781314969900901921169717372784768472686084900337702424291651300500516832336435038951702989392233451722013812806965011784408745196012122859937162313017114448464090389064495444006198690754851602632750529834918740786680881833851022833450850486082503930213321971551843063545500766828294930413776552793975175461395398468339363830474611996653858153842056853386218672523340283087112328278921250771262946322956398989893582116745627010218356462201349671518819097303811980049734072396103685406643193950979019069963955245300545058068550195673022921913933918568034490398205955100226353536192041994745538593810234395544959778377902374216172711172364343543947822181852862408514006660443325888569867054315470696574745855033232334210730154594051655379068662733379958511562578432298827372319898757141595781119635833005940873068121602876496286744604774649159950549737425626901049037781986835938146574126804925648798556145372347867330390468838343634655379498641927056387293174872332083760112302991136793862708943879936201629515413371424892830722012690147546684765357616477379467520049075715552781965362132392640616013635815590742202020318727760527721900556148425551879253034351398442532234157623361064250639049750086562710953591946589751413103482276930624743536325691607815478181152843667957061108615331504452127473924544945423682886061340841486377670096120715124914043027253860764823634143346235189757664521641376796903149501910857598442391986291642193994907236234646844117394032659184044378051333894525742399508296591228508555821572503107125701266830240292952522011872676756220415420516184163484756516999811614101002996078386909291603028840026910414079288621507842451670908700069928212066041837180653556725253256753286129104248776182582976515795984703562226293486003415872298053498965022629174878820273420922224533985626476691490556284250391275771028402799806636582548892648802545661017296702664076559042909945681506526530537182941270336931378517860904070866711496558343434769338578171138645587367812301458768712660348913909562009939361031029161615288138437909904231747336394804575931493140529763475748119356709110137751721008031559024853090669203767192203322909433467685142214477379393751703443661991040337511173547191855046449026365512816228824462575916333039107225383742182140883508657391771509682887478265699599574490661758344137522397096834080053559849175417381883999446974867626551658276584835884531427756879002909517028352971634456212964043523117600665101241200659755851276178583829204197484423608007193045761893234922927965019875187212726750798125547095890455635792122103334669749923563025494780249011419521238281530911407907386025152274299581807247162591668545133312394804947079119153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5ยผ\n",
      "ยผ\n",
      "ยฝ\n",
      "ยพ\n",
      "ไธ\n",
      "โ\n",
      "2ยฝ\n",
      "ยผ\n",
      "ๅซ\n",
      "1ยฝ\n",
      "2ยฝ\n",
      "1ยฝ\n",
      "ยฝ\n",
      "1ยฝ\n",
      "2ยฝ\n",
      "1ยฝ\n",
      "1ยฝ\n",
      "1ยฝ\n",
      "41ยฝ\n",
      "ยฝ\n",
      "ยฝ\n",
      "ยฝ\n",
      "ยฝ\n",
      "ยผ\n",
      "ยฝ\n",
      "ยผ\n",
      "ยฝ\n",
      "ยผ\n",
      "1ยฝ\n",
      "ยฝ\n",
      "ยผ\n",
      "โ\n",
      "ยณ\n",
      "ยฒ\n",
      "ยฒ\n",
      "ยฒ\n",
      "ยฒ\n",
      "ยฒ\n",
      "ยฒ\n",
      "ยฒ\n",
      "ยฒ\n",
      "8ยฝ\n",
      "4ยฝ\n",
      "ยผ\n",
      "2ยฒ\n",
      "10ยนยฒ\n",
      "ยฒ\n",
      "ยฒ\n",
      "ๅๅ\n",
      "ๅ\n",
      "ๅ\n",
      "1ยฝ\n",
      "ยผ\n",
      "Porters Stemmer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486789c8f8f44902a59d027da5a1efc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order df...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>doc_body</th>\n",
       "      <th>probody</th>\n",
       "      <th>tokens</th>\n",
       "      <th>senttokens</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>...</th>\n",
       "      <th>horror</th>\n",
       "      <th>xboxahoy</th>\n",
       "      <th>animatedbooksummaries</th>\n",
       "      <th>kendricklamar</th>\n",
       "      <th>bigmouth</th>\n",
       "      <th>science</th>\n",
       "      <th>tanlines</th>\n",
       "      <th>hungryartists</th>\n",
       "      <th>dirtyconfession</th>\n",
       "      <th>finlandconspiracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-Areopagan-</th>\n",
       "      <td>Your first and second question is the same que...</td>\n",
       "      <td>[Your first and second question is the same qu...</td>\n",
       "      <td>[first second question question try make incis...</td>\n",
       "      <td>[[first, second, question, question, try, make...</td>\n",
       "      <td>[[Your first and second question is the same q...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-BigSexy-</th>\n",
       "      <td>I've been asked to cum everywhere with my ex j...</td>\n",
       "      <td>[I've been asked to cum everywhere with my ex ...</td>\n",
       "      <td>[asked cum everywhere ex experiment preferred ...</td>\n",
       "      <td>[[asked, cum, everywhere, ex, experiment, pref...</td>\n",
       "      <td>[[I've been asked to cum everywhere with my ex...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-BlitzN9ne</th>\n",
       "      <td>I'm currently in the middle of making a Payday...</td>\n",
       "      <td>[I'm currently in the middle of making a Payda...</td>\n",
       "      <td>[currently middle making payday two inspired m...</td>\n",
       "      <td>[[currently, middle, making, payday, two, insp...</td>\n",
       "      <td>[[I'm currently in the middle of making a Payd...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-CrestiaBell</th>\n",
       "      <td>First and foremost I extend my condolences to ...</td>\n",
       "      <td>[First and foremost I extend my condolences to...</td>\n",
       "      <td>[first foremost extend condolences family espe...</td>\n",
       "      <td>[[first, foremost, extend, condolences, family...</td>\n",
       "      <td>[[First and foremost I extend my condolences t...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-dyad-</th>\n",
       "      <td>I failed both...I'm great at reading people ir...</td>\n",
       "      <td>[I failed both...I'm great at reading people i...</td>\n",
       "      <td>[failed great reading people irl swear haha, i...</td>\n",
       "      <td>[[failed, great, reading, people, irl, swear, ...</td>\n",
       "      <td>[[I failed both...I'm great at reading people ...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zugzwang_03</th>\n",
       "      <td>You know that giggly group of women going to t...</td>\n",
       "      <td>[You know that giggly group of women going to ...</td>\n",
       "      <td>[know giggly group women going bar go dance sa...</td>\n",
       "      <td>[[know, giggly, group, women, going, bar, go, ...</td>\n",
       "      <td>[[You know that giggly group of women going to...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuluthrone</th>\n",
       "      <td>I saw some speculate that the \"download\" would...</td>\n",
       "      <td>[I saw some speculate that the \"download\" woul...</td>\n",
       "      <td>[saw speculate download would backup rather st...</td>\n",
       "      <td>[[saw, speculate, download, would, backup, rat...</td>\n",
       "      <td>[[I saw some speculate that the \"download\" wou...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zwelg</th>\n",
       "      <td>I am actually pretty pleased about my score:Ag...</td>\n",
       "      <td>[I am actually pretty pleased about my score:A...</td>\n",
       "      <td>[actually pretty pleased score agreeableness m...</td>\n",
       "      <td>[[actually, pretty, pleased, score, agreeablen...</td>\n",
       "      <td>[[I am actually pretty pleased about my score:...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zymmaster</th>\n",
       "      <td>Respectfully disagree. Offense had plenty of i...</td>\n",
       "      <td>[Respectfully disagree. Offense had plenty of ...</td>\n",
       "      <td>[respectfully disagree offense plenty issues s...</td>\n",
       "      <td>[[respectfully, disagree, offense, plenty, iss...</td>\n",
       "      <td>[[Respectfully disagree., Offense had plenty o...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyzee</th>\n",
       "      <td>Tarzaned can't be compared in this situation. ...</td>\n",
       "      <td>[Tarzaned can't be compared in this situation....</td>\n",
       "      <td>[tarzaned not compared situation not pro playe...</td>\n",
       "      <td>[[tarzaned, not, compared, situation, not, pro...</td>\n",
       "      <td>[[Tarzaned can't be compared in this situation...</td>\n",
       "      <td>88.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1606 rows ร 16102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           body  \\\n",
       "author                                                            \n",
       "-Areopagan-   Your first and second question is the same que...   \n",
       "-BigSexy-     I've been asked to cum everywhere with my ex j...   \n",
       "-BlitzN9ne    I'm currently in the middle of making a Payday...   \n",
       "-CrestiaBell  First and foremost I extend my condolences to ...   \n",
       "-dyad-        I failed both...I'm great at reading people ir...   \n",
       "...                                                         ...   \n",
       "zugzwang_03   You know that giggly group of women going to t...   \n",
       "zuluthrone    I saw some speculate that the \"download\" would...   \n",
       "zwelg         I am actually pretty pleased about my score:Ag...   \n",
       "zymmaster     Respectfully disagree. Offense had plenty of i...   \n",
       "zyzee         Tarzaned can't be compared in this situation. ...   \n",
       "\n",
       "                                                       doc_body  \\\n",
       "author                                                            \n",
       "-Areopagan-   [Your first and second question is the same qu...   \n",
       "-BigSexy-     [I've been asked to cum everywhere with my ex ...   \n",
       "-BlitzN9ne    [I'm currently in the middle of making a Payda...   \n",
       "-CrestiaBell  [First and foremost I extend my condolences to...   \n",
       "-dyad-        [I failed both...I'm great at reading people i...   \n",
       "...                                                         ...   \n",
       "zugzwang_03   [You know that giggly group of women going to ...   \n",
       "zuluthrone    [I saw some speculate that the \"download\" woul...   \n",
       "zwelg         [I am actually pretty pleased about my score:A...   \n",
       "zymmaster     [Respectfully disagree. Offense had plenty of ...   \n",
       "zyzee         [Tarzaned can't be compared in this situation....   \n",
       "\n",
       "                                                        probody  \\\n",
       "author                                                            \n",
       "-Areopagan-   [first second question question try make incis...   \n",
       "-BigSexy-     [asked cum everywhere ex experiment preferred ...   \n",
       "-BlitzN9ne    [currently middle making payday two inspired m...   \n",
       "-CrestiaBell  [first foremost extend condolences family espe...   \n",
       "-dyad-        [failed great reading people irl swear haha, i...   \n",
       "...                                                         ...   \n",
       "zugzwang_03   [know giggly group women going bar go dance sa...   \n",
       "zuluthrone    [saw speculate download would backup rather st...   \n",
       "zwelg         [actually pretty pleased score agreeableness m...   \n",
       "zymmaster     [respectfully disagree offense plenty issues s...   \n",
       "zyzee         [tarzaned not compared situation not pro playe...   \n",
       "\n",
       "                                                         tokens  \\\n",
       "author                                                            \n",
       "-Areopagan-   [[first, second, question, question, try, make...   \n",
       "-BigSexy-     [[asked, cum, everywhere, ex, experiment, pref...   \n",
       "-BlitzN9ne    [[currently, middle, making, payday, two, insp...   \n",
       "-CrestiaBell  [[first, foremost, extend, condolences, family...   \n",
       "-dyad-        [[failed, great, reading, people, irl, swear, ...   \n",
       "...                                                         ...   \n",
       "zugzwang_03   [[know, giggly, group, women, going, bar, go, ...   \n",
       "zuluthrone    [[saw, speculate, download, would, backup, rat...   \n",
       "zwelg         [[actually, pretty, pleased, score, agreeablen...   \n",
       "zymmaster     [[respectfully, disagree, offense, plenty, iss...   \n",
       "zyzee         [[tarzaned, not, compared, situation, not, pro...   \n",
       "\n",
       "                                                     senttokens  \\\n",
       "author                                                            \n",
       "-Areopagan-   [[Your first and second question is the same q...   \n",
       "-BigSexy-     [[I've been asked to cum everywhere with my ex...   \n",
       "-BlitzN9ne    [[I'm currently in the middle of making a Payd...   \n",
       "-CrestiaBell  [[First and foremost I extend my condolences t...   \n",
       "-dyad-        [[I failed both...I'm great at reading people ...   \n",
       "...                                                         ...   \n",
       "zugzwang_03   [[You know that giggly group of women going to...   \n",
       "zuluthrone    [[I saw some speculate that the \"download\" wou...   \n",
       "zwelg         [[I am actually pretty pleased about my score:...   \n",
       "zymmaster     [[Respectfully disagree., Offense had plenty o...   \n",
       "zyzee         [[Tarzaned can't be compared in this situation...   \n",
       "\n",
       "              agreeableness  openness  conscientiousness  extraversion  \\\n",
       "author                                                                   \n",
       "-Areopagan-             0.0      99.0               96.0          60.0   \n",
       "-BigSexy-              39.0      92.0                1.0          18.0   \n",
       "-BlitzN9ne             50.0      85.0               15.0          50.0   \n",
       "-CrestiaBell           50.0      85.0               50.0          85.0   \n",
       "-dyad-                 60.0      67.0               45.0          10.0   \n",
       "...                     ...       ...                ...           ...   \n",
       "zugzwang_03            10.0      41.0               86.0          83.0   \n",
       "zuluthrone             17.0      96.0               28.0          95.0   \n",
       "zwelg                  39.0      89.0               91.0          80.0   \n",
       "zymmaster              28.0      47.0               62.0          21.0   \n",
       "zyzee                  88.0      78.0               31.0          75.0   \n",
       "\n",
       "              neuroticism  ...  horror  xboxahoy  animatedbooksummaries  \\\n",
       "author                     ...                                            \n",
       "-Areopagan-           1.0  ...       0         0                      0   \n",
       "-BigSexy-             4.0  ...       0         0                      0   \n",
       "-BlitzN9ne           30.0  ...       0         0                      0   \n",
       "-CrestiaBell         50.0  ...       0         0                      0   \n",
       "-dyad-               47.0  ...       0         0                      0   \n",
       "...                   ...  ...     ...       ...                    ...   \n",
       "zugzwang_03          18.0  ...       0         0                      0   \n",
       "zuluthrone           34.0  ...       0         0                      0   \n",
       "zwelg                 3.0  ...       0         0                      0   \n",
       "zymmaster            49.0  ...       0         0                      0   \n",
       "zyzee                10.0  ...       0         0                      0   \n",
       "\n",
       "              kendricklamar  bigmouth science  tanlines  hungryartists  \\\n",
       "author                                                                   \n",
       "-Areopagan-               0         0       0         0              0   \n",
       "-BigSexy-                 0         0       3         0              0   \n",
       "-BlitzN9ne                0         0       0         0              0   \n",
       "-CrestiaBell              1         0       8         0              0   \n",
       "-dyad-                    0         0       0         0              0   \n",
       "...                     ...       ...     ...       ...            ...   \n",
       "zugzwang_03               0         0      13         0              0   \n",
       "zuluthrone                0         0       8         0              0   \n",
       "zwelg                     0         0       0         0              0   \n",
       "zymmaster                 0         0       0         0              0   \n",
       "zyzee                     0         0       0         0              0   \n",
       "\n",
       "              dirtyconfession  finlandconspiracy  \n",
       "author                                            \n",
       "-Areopagan-                 0                  0  \n",
       "-BigSexy-                   0                  0  \n",
       "-BlitzN9ne                  0                  0  \n",
       "-CrestiaBell                0                  0  \n",
       "-dyad-                      0                  0  \n",
       "...                       ...                ...  \n",
       "zugzwang_03                 0                  0  \n",
       "zuluthrone                  0                  0  \n",
       "zwelg                       0                  0  \n",
       "zymmaster                   0                  0  \n",
       "zyzee                       0                  0  \n",
       "\n",
       "[1606 rows x 16102 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrapper\n",
    "\n",
    "def preprocess(df):\n",
    "    # adjust some column representations\n",
    "    df = bigfive_cat(df)\n",
    "    # choose stopwordlist with or without negation\n",
    "    stopwordList = choose_stopwordlist(df, mode='NLTK-neg')\n",
    "    # decontract abbreviations (e.g., n't to not)\n",
    "    print(\"Decontract...\")\n",
    "    df['probody'] = df['doc_body'].apply(lambda x:([decontracted(x) for x in x]))\n",
    "    # create sentence tokens\n",
    "    print(\"Tokenize Sentences...\")\n",
    "    df = senttokenize(df)\n",
    "    # lower, remove stopwords, num2words, tokenize\n",
    "    print(\"Lower words and remove special characters...\")\n",
    "    df = lower_special(df)\n",
    "    print(\"Remove stopwords...\")\n",
    "    df = remove_stopwords(df, stopwordList)\n",
    "    print(\"Change numbers to words and tokenize words...\")\n",
    "    df = num_tokenize(df)\n",
    "    # porters stemmer\n",
    "    print(\"Porters Stemmer...\")\n",
    "    df = stemming(df)\n",
    "    print(\"Order df...\")\n",
    "    df = ordering(df)\n",
    "    print(\"Done!\")\n",
    "    return df\n",
    "\n",
    "# apply preprocessing\n",
    "predf = preprocess(pandoradf)\n",
    "\n",
    "predf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body',\n",
       " 'doc_body',\n",
       " 'probody',\n",
       " 'tokens',\n",
       " 'senttokens',\n",
       " 'agreeableness',\n",
       " 'openness',\n",
       " 'conscientiousness',\n",
       " 'extraversion',\n",
       " 'neuroticism',\n",
       " 'agree',\n",
       " 'openn',\n",
       " 'consc',\n",
       " 'extra',\n",
       " 'neuro',\n",
       " 'utc',\n",
       " 'controversiality',\n",
       " 'gilded',\n",
       " 'num_subreddit',\n",
       " 'lang',\n",
       " 'monday',\n",
       " 'tuesday',\n",
       " 'wednesday',\n",
       " 'thursday',\n",
       " 'friday',\n",
       " 'saturday',\n",
       " 'sunday',\n",
       " 'january',\n",
       " 'february',\n",
       " 'march',\n",
       " 'april',\n",
       " 'may',\n",
       " 'june',\n",
       " 'juli',\n",
       " 'august',\n",
       " 'september',\n",
       " 'october',\n",
       " 'november',\n",
       " 'december',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " 'osugame',\n",
       " 'navyblazer',\n",
       " 'modelldiskussion',\n",
       " 'mcbc',\n",
       " 'mwo',\n",
       " 'hfy',\n",
       " 'shoegaze',\n",
       " 'thecteam',\n",
       " 'snhu',\n",
       " 'noggler',\n",
       " 'shittybuildapc',\n",
       " 'sexypizza',\n",
       " 'womenshealth',\n",
       " 'watchersonthewall',\n",
       " 'ytmnd',\n",
       " 'quotes',\n",
       " 'carnivorediet',\n",
       " 'ancientaliens',\n",
       " 'tydides',\n",
       " 'javascripthelp',\n",
       " 'u_vunha',\n",
       " 'futbol',\n",
       " 'gopro',\n",
       " 'bethesdasoftworks',\n",
       " 'downriver',\n",
       " 'swingdancing',\n",
       " 'rainmeterhelp',\n",
       " 'blink182',\n",
       " 'linux',\n",
       " 'theydidthemath',\n",
       " 'accutane',\n",
       " 'nfl_draft',\n",
       " 'workingadultsguild',\n",
       " 'rengarmains',\n",
       " 'loldominion',\n",
       " 'kickstarter',\n",
       " 'gaymersgonewild',\n",
       " 'slipknot',\n",
       " 'emo',\n",
       " 'letschat',\n",
       " 'gangweed',\n",
       " 'shestillsucking',\n",
       " 'fox',\n",
       " 'oshkosh',\n",
       " 'stressoutloud',\n",
       " 'justengaged',\n",
       " 'fightme',\n",
       " 'casualknitting',\n",
       " 'eastertoad',\n",
       " 'automate',\n",
       " 'subaruimpreza',\n",
       " 'premeduk',\n",
       " 'virginityexchange',\n",
       " 'nottheonion',\n",
       " 'offbeat',\n",
       " 'mirrorsedge',\n",
       " 'notmyproudestupvote',\n",
       " 'bsrpcommunity',\n",
       " 'budgetfood',\n",
       " 'intp_irl',\n",
       " 'dirtywritingprompts',\n",
       " 'metalmemes',\n",
       " 'christians',\n",
       " 'oculusgo',\n",
       " 'theyshouldfixthat',\n",
       " 'bedroombands',\n",
       " 'artisanvideos',\n",
       " 'freedesign',\n",
       " 'bradyharanmemes',\n",
       " 'blazblue',\n",
       " 'spotify',\n",
       " 'hiphopcirclejerk',\n",
       " 'simcity',\n",
       " 'billsimmons',\n",
       " 'ffxivhousingmarket',\n",
       " 'castiron',\n",
       " 'thefighterandthekid',\n",
       " 'reiayanami',\n",
       " 'quantico',\n",
       " 'ladyladyboners',\n",
       " 'tubersimulator',\n",
       " 'ladybonersgonecuddly',\n",
       " 'actualfarcraftcircler',\n",
       " 'competitiveedh',\n",
       " 'entheogens',\n",
       " 'preppers',\n",
       " 'demisexuality',\n",
       " '90dayfiance',\n",
       " 'ufpica',\n",
       " 'reds',\n",
       " 'nonenglishmusic',\n",
       " 'formerpizzahuts',\n",
       " 'ubersicht',\n",
       " 'nycgaybros',\n",
       " 'plantsbeingjerks',\n",
       " 'ethtraderpro',\n",
       " 'gunpla',\n",
       " 'unexpectedtf2',\n",
       " 'team60sreb',\n",
       " 'askgaymen',\n",
       " 'mildyinteresting',\n",
       " 'marilyn_manson',\n",
       " 'spirituality',\n",
       " 'modlimit',\n",
       " 'ttc30',\n",
       " 'personality_tests',\n",
       " 'norwich',\n",
       " 'tomorrowworld',\n",
       " 'businesshub',\n",
       " 'sithit',\n",
       " 'mathpuzzles',\n",
       " 'powershell',\n",
       " 'papermario',\n",
       " 'allamericanroughneck',\n",
       " 'audiophile',\n",
       " 'battleborn',\n",
       " 'cormoran_strike',\n",
       " 'austrianormarxist',\n",
       " 'blackfellas',\n",
       " 'safeorscamvendors',\n",
       " 'jazz',\n",
       " 'datascience',\n",
       " 'creedthoughts',\n",
       " 'velomobile',\n",
       " 'unhealthyeatinghabits',\n",
       " 'myouimina',\n",
       " 'mhocnuppress',\n",
       " 'wrestlewiththeplot',\n",
       " 'texas',\n",
       " 'effectivealtruism',\n",
       " 'whitepeoplegifs',\n",
       " 'applehelp',\n",
       " 'delusionalcraigslist',\n",
       " 'dr_harper',\n",
       " 'iosthemes',\n",
       " 'hgtv',\n",
       " 'peggyvsshannon',\n",
       " 'republican',\n",
       " 'duolingomemes',\n",
       " 'economicdemocracy',\n",
       " 'dogsareliquid',\n",
       " 'dell',\n",
       " 'currentlytripping',\n",
       " 'dspd',\n",
       " 'careerguidance',\n",
       " 'invisiball',\n",
       " 'cowboybebop',\n",
       " 'tmux',\n",
       " 'agonygame',\n",
       " 'techjobs',\n",
       " 'chinesefood',\n",
       " 'infps_adulting',\n",
       " 'transcendentaltransit',\n",
       " 'pantyselling',\n",
       " 'sapphicbdsm',\n",
       " 'skincare_addiction',\n",
       " 'stylus',\n",
       " 'kittykatgaming',\n",
       " 'fatpeoplestories',\n",
       " 'steamgiveaway',\n",
       " 'infj',\n",
       " 'frederator',\n",
       " 'notlikeothergirls',\n",
       " 'cartography',\n",
       " 'nexus7',\n",
       " 'television',\n",
       " 'composer',\n",
       " 'searchforthesleeper',\n",
       " 'bf_hardline',\n",
       " 'pinball',\n",
       " 'thesecretworld',\n",
       " 'andrewjacksonjihad',\n",
       " 'cardano',\n",
       " 'rule34requests',\n",
       " 'redorchestra',\n",
       " 'cartoongangsters',\n",
       " 'widowmakermains',\n",
       " 'batteries',\n",
       " 'ohyou',\n",
       " 'dickbutt',\n",
       " 'redheadedgoddesses',\n",
       " 'celebjomaterial',\n",
       " 'itil',\n",
       " 'dragonsdogmaonline',\n",
       " 'usf',\n",
       " 'ck2gameofthrones',\n",
       " 'javahelp',\n",
       " 'garageshop',\n",
       " 'tmbr',\n",
       " 'nzparents',\n",
       " 'steel_division',\n",
       " 'thewhetstone',\n",
       " 'dank_gifs',\n",
       " 'kagw',\n",
       " 'losshurts',\n",
       " 'mildlydepressing',\n",
       " 'usgovsim',\n",
       " 'circloljerk',\n",
       " 'bigboobsgonewild',\n",
       " 'abilitydraft',\n",
       " 'futurewhatif',\n",
       " 'oshawa',\n",
       " 'banjo',\n",
       " 'hotpaper',\n",
       " 'lfservers',\n",
       " 'monsterprom',\n",
       " 'sevenkingdoms',\n",
       " 'extrafabulouscomics',\n",
       " 'modelbarclays',\n",
       " 'amersfoort',\n",
       " 'twitter',\n",
       " 'popcorntime',\n",
       " 'badpolitics',\n",
       " 'slp',\n",
       " 'fo76marketplace',\n",
       " 'jaguars',\n",
       " 'longboarding',\n",
       " 'nonprofitprojects',\n",
       " 'wholesomeyuri',\n",
       " 'winmyargument',\n",
       " 'hmusgovsouth',\n",
       " 'syracuse',\n",
       " 'ausproperty',\n",
       " 'singleparents',\n",
       " 'homegym',\n",
       " 'thanksobama',\n",
       " 'netrunner',\n",
       " 'full_news',\n",
       " 'cityandcolour',\n",
       " 'kanyeleaks',\n",
       " 'actualwomen',\n",
       " 'drawforme',\n",
       " 'shotglassbets',\n",
       " 'donkeykong',\n",
       " 'shitpostcrusaders',\n",
       " 'plant_progress',\n",
       " 'redpillwomen',\n",
       " 'apexlegends',\n",
       " 'kava',\n",
       " 'wheredoistart',\n",
       " 'virginspace',\n",
       " 'characterdrawing',\n",
       " 'dvamains',\n",
       " 'jessicanigri',\n",
       " 'austinrp',\n",
       " 'u_potbellyofficial',\n",
       " 'twotrumps',\n",
       " 'moemorphism',\n",
       " 'coopplay',\n",
       " 'cheatatmathhomework',\n",
       " 'malefashionmarket',\n",
       " 'editmyraw',\n",
       " 'bowieism',\n",
       " 'saiyanpeopletwitter',\n",
       " 'passionx',\n",
       " 'fbb_nsfw',\n",
       " 'tradfri',\n",
       " 'modelusgov',\n",
       " 'california',\n",
       " 'web_design',\n",
       " 'sellercirclestage',\n",
       " 'worldevents',\n",
       " 'carseathr',\n",
       " 'showerthoughts',\n",
       " 'mathriddles',\n",
       " 'gunnerkrigg',\n",
       " 'cattle',\n",
       " 'uppsala',\n",
       " 'paranatural',\n",
       " 'ultimateskyrim',\n",
       " 'fuckingphilosophy',\n",
       " 'forfashion',\n",
       " 'apink',\n",
       " 'drosteeffect',\n",
       " 'abbeycxxx',\n",
       " 'fleet_foxes',\n",
       " 'crackstatus',\n",
       " 'cuteguys',\n",
       " 'placecanada',\n",
       " 'celebs',\n",
       " 'a858de45f56d9bc9',\n",
       " 'modeljustice',\n",
       " 'trollxover30',\n",
       " 'outlast',\n",
       " 'dota2draft',\n",
       " 'warhammer30k',\n",
       " 'aberdeen',\n",
       " 'planetpokemonplus',\n",
       " 'hockey',\n",
       " 'wwiipics',\n",
       " 'civsaves',\n",
       " '18_19',\n",
       " 'ewwducational',\n",
       " 'conspiratard',\n",
       " 'nba',\n",
       " 'pickuptorrents',\n",
       " 'shareafap',\n",
       " 'healthymalesexuality',\n",
       " 'asshole',\n",
       " 'designthought',\n",
       " 'unity_tutorials',\n",
       " 'botanicalporn',\n",
       " 'modelindustrialworker',\n",
       " 'wurmunlimited',\n",
       " 'thef0rceawakens',\n",
       " 'gttod',\n",
       " 'askmrp',\n",
       " 'dishwashers',\n",
       " 'loliconsunite',\n",
       " 'civuncensored',\n",
       " 'corn_irl',\n",
       " 'godasshole',\n",
       " 'mmfb',\n",
       " 'magnetfishing',\n",
       " 'frenchforeignlegion',\n",
       " 'delete_the_donald',\n",
       " 'tf2lft',\n",
       " 'facedownassup',\n",
       " 'forza',\n",
       " 'clashdecks',\n",
       " 'boomerangfromcn',\n",
       " 'archerfx',\n",
       " 'shannaratv',\n",
       " 'skyrimrequiem',\n",
       " 'savate',\n",
       " 'outwardgame',\n",
       " 'asklgbt',\n",
       " 'numerical',\n",
       " 'modelthemasses',\n",
       " 'absurdism',\n",
       " 'canadaclicker',\n",
       " 'playboundless',\n",
       " 'trumpet',\n",
       " 'rupaulsdragrace',\n",
       " 'depressionregimens',\n",
       " 'chinesebookclub',\n",
       " 'materialdesign',\n",
       " 'socialcorporatism',\n",
       " 'smoobypost',\n",
       " 'pickling',\n",
       " 'gmc',\n",
       " 'chronicpain',\n",
       " '40korkscience',\n",
       " 'thickloads',\n",
       " 'guitarnet',\n",
       " 'saltlakecity',\n",
       " 'beansinthings',\n",
       " 'eaf',\n",
       " 'theloudhouse',\n",
       " 'altnewz',\n",
       " 'the100',\n",
       " 'brass',\n",
       " 'fitshionvsfatshion',\n",
       " 'spotted',\n",
       " 'dach_outdoor',\n",
       " 'avpd',\n",
       " 'composers',\n",
       " 'criticalthinking',\n",
       " 'reptime',\n",
       " 'asiangirlsbeingcute',\n",
       " 'lolmorbidreality',\n",
       " 'ecr_uk',\n",
       " 'galaxynote5',\n",
       " 'calvinandhobbes',\n",
       " 'help_with_math',\n",
       " 'mumbojumbofanserver',\n",
       " 'newjersey',\n",
       " 'darksoulspvp',\n",
       " 'crushcrush',\n",
       " 'unknownvideos',\n",
       " 'r4rasexual',\n",
       " 'ironfist',\n",
       " 'britishradio',\n",
       " 'debatepoliticalphil',\n",
       " 'apstudents',\n",
       " 'gendercynical',\n",
       " 'entpcirclejerk',\n",
       " 'technews',\n",
       " 'nibelheim',\n",
       " 'stormlight_archive',\n",
       " 'imagesofthe1990s',\n",
       " 'antimlm',\n",
       " 'googledrive',\n",
       " 'wholesome4chan',\n",
       " 'uspolitics',\n",
       " 'badukpolitics',\n",
       " 'halloween_costumes',\n",
       " 'guro',\n",
       " 'melodicdeathmetal',\n",
       " 'fedex',\n",
       " 'delco',\n",
       " 'thechapel',\n",
       " 'modellkafeterian',\n",
       " 'hentaiforcedorgasms',\n",
       " 'washu',\n",
       " 'iphonewallpapers',\n",
       " 'trucksim',\n",
       " 'lordoftherings',\n",
       " 'becomingtheiceman',\n",
       " 'snsd',\n",
       " 'foundonyoutube',\n",
       " 'olkb',\n",
       " 'meta_me_irl',\n",
       " 'computer_help',\n",
       " 'lovetowatchyouleave',\n",
       " 'vergecurrency',\n",
       " 'bannerlord',\n",
       " 'ihavesex',\n",
       " 'shitpostemblem',\n",
       " 'totalwarhammer',\n",
       " 'pantheonmmo',\n",
       " 'prolife',\n",
       " 'oxford',\n",
       " 'brse',\n",
       " 'fulfillmentbyamazon',\n",
       " 'rolereversal',\n",
       " 'mhocstormont',\n",
       " 'battlebots',\n",
       " 'gotham',\n",
       " 'amathenedit',\n",
       " 'fantanoforever',\n",
       " 'xperiaz5',\n",
       " 'verloreneredditoren',\n",
       " 'restaurant',\n",
       " 'blocknload',\n",
       " 'piratesofthecaribbean',\n",
       " 'vancouverjobs',\n",
       " 'u_tinylab',\n",
       " 'netherlands',\n",
       " 'mnzgov',\n",
       " 'medici_netflix',\n",
       " 'dnd5e',\n",
       " 'torchwood',\n",
       " 'iaido',\n",
       " 'nuclearthrone',\n",
       " 'isaacarthur',\n",
       " 'imagesofmontana',\n",
       " 'rabbits',\n",
       " 'keanubeingawesome',\n",
       " 'winnipegjets',\n",
       " 'webpack',\n",
       " 'jmu',\n",
       " 'catadvice',\n",
       " 'gallifrey',\n",
       " 'votethirdparty',\n",
       " 'dirtybomb',\n",
       " 'scriptswap',\n",
       " 'illusionporn',\n",
       " 'alll',\n",
       " 'fisting',\n",
       " 'mholyroodvote',\n",
       " 'americancrimestory',\n",
       " 'dykesgonemild',\n",
       " 'fifacoins',\n",
       " 'perth',\n",
       " 'yorickmains',\n",
       " 'powerbuilding',\n",
       " 'weinsteineffect',\n",
       " 'greeklife',\n",
       " 'jonwolf',\n",
       " 'jessicamshannon',\n",
       " 'comedybodyguards',\n",
       " 'theflash',\n",
       " 'lsa',\n",
       " 'applewhatshouldibuy',\n",
       " 'fixedgearbicycle',\n",
       " 'designjobs',\n",
       " 'dominos',\n",
       " 'alanwatts',\n",
       " 'nigelthornberry',\n",
       " 'bjj',\n",
       " 'nidus3',\n",
       " 'xenogears',\n",
       " 'psminecraft',\n",
       " 'intermittentfasting',\n",
       " 'exercise',\n",
       " 'breakingeggs',\n",
       " 'lordoftheringsrp',\n",
       " 'abop',\n",
       " 'rwbyamityarena',\n",
       " 'tall',\n",
       " 'oneirosophy',\n",
       " 'overwatchmemes',\n",
       " 'creepypms',\n",
       " 'thedavidpakmanshow',\n",
       " 'animegifs',\n",
       " 'pathology',\n",
       " 'wingchun',\n",
       " 'newarkde',\n",
       " 'nsfwcosplay',\n",
       " 'hilariouscringe',\n",
       " 'repsneakers',\n",
       " 'lifestartsnow',\n",
       " 'bodyperfection',\n",
       " 'mongodb',\n",
       " 'neutralpolitics',\n",
       " 'workgonewild',\n",
       " 'anticommiecringe',\n",
       " 'powerlifting',\n",
       " 'awfuleyebrows',\n",
       " 'nvidia',\n",
       " 'volcel',\n",
       " 'papertowns',\n",
       " 'spore',\n",
       " 'u_sandisc45',\n",
       " 'cellsatwork',\n",
       " 'geologyporn',\n",
       " 'penmanship',\n",
       " 'mathpics',\n",
       " 'gtagifs',\n",
       " 'laceychabert',\n",
       " 'documentaries',\n",
       " 'shitthefalsesay',\n",
       " 'slightlydamned',\n",
       " 'ygwinner',\n",
       " 'libraries',\n",
       " 'the_bogdanoff',\n",
       " 'pokemoninsurgence',\n",
       " 'pokemongoedmonton',\n",
       " 'fbhw',\n",
       " 'disneygifs',\n",
       " 'avoid5',\n",
       " 'dessert',\n",
       " 'stevenuniversensfw',\n",
       " 'settlethisforme',\n",
       " 'futurology',\n",
       " 'mineralporn',\n",
       " 'sprint',\n",
       " 'giveaways',\n",
       " 'autocockers101',\n",
       " 'ywam',\n",
       " 'cascadia',\n",
       " 'bittrex',\n",
       " 'playboicarti',\n",
       " 'oxygennotincluded',\n",
       " 'csbundestag',\n",
       " 'menaconflicts',\n",
       " 'planetcoaster',\n",
       " 'whatisthispainting',\n",
       " 'adultdepression',\n",
       " 'quarry',\n",
       " 'aliceinchains',\n",
       " 'fireemblemheroes',\n",
       " 'cerseiwinsthethrone',\n",
       " 'polaroid',\n",
       " 'covetfashion',\n",
       " 'animalcrossing',\n",
       " 'askpsychology',\n",
       " 'pokemontcg',\n",
       " 'graffiti',\n",
       " 'mixcraft_studio',\n",
       " 'psychadelics',\n",
       " 'piracy',\n",
       " 'konosuba',\n",
       " 'tarantino',\n",
       " 'distributism',\n",
       " 'memritvmemes',\n",
       " 'sushi',\n",
       " 'roastmyhistory',\n",
       " 'wacotexasmeetup',\n",
       " 'thatlookedexpensive',\n",
       " 'cheermeup',\n",
       " 'technologyprotips',\n",
       " 'mrrobot',\n",
       " 'fixedtattoos',\n",
       " 'teachers',\n",
       " 'wax_io',\n",
       " 'wardrobepurge',\n",
       " 'thesaurizethis',\n",
       " 'books',\n",
       " 'fullmoviesonvimeo',\n",
       " 'coaxedintoasnafu',\n",
       " 'philosophyself',\n",
       " 'androgyny',\n",
       " 'mytimeatportia',\n",
       " 'burninglibrary',\n",
       " 'atlantatv',\n",
       " 'flairwars',\n",
       " 'longhornnation',\n",
       " 'davidlynch',\n",
       " 'ausfemalefashion',\n",
       " 'mobalegends',\n",
       " 'musgovpress',\n",
       " 'indiemakeupandmore',\n",
       " 'postmalone',\n",
       " 'fbi_memes',\n",
       " 'dirtypenpals',\n",
       " 'postorgasm',\n",
       " 'doom',\n",
       " 'wholesomehentai',\n",
       " 'clashroyale',\n",
       " 'iamverysmart',\n",
       " 'vikingstv',\n",
       " 'johnmayer',\n",
       " 'shittyshowerthoughts',\n",
       " 'gameai',\n",
       " 'thanksgravity',\n",
       " 'competetive_overwatch',\n",
       " 'fpga',\n",
       " 'winehq',\n",
       " 'engineeringstudents',\n",
       " 'starwarsrebels',\n",
       " 'musicsuggestions',\n",
       " 'tradeorgift',\n",
       " 'isthislegal',\n",
       " 'dp_dr',\n",
       " 'kingstonontario',\n",
       " 'youonlifetime',\n",
       " 'huawei',\n",
       " 'reallifedoodles',\n",
       " 'geazy',\n",
       " 'thewestwing',\n",
       " 'dvzcirclejerk',\n",
       " 'radicalfeminism',\n",
       " 'hanselanime',\n",
       " 'wince',\n",
       " 'oneprotectrestattack',\n",
       " 'lootcratespoilers',\n",
       " 'amateurarchives',\n",
       " 'etikaworldnetwork',\n",
       " 'vice',\n",
       " 'shittygroupmembers',\n",
       " 'u_lightsilvermoon_',\n",
       " 'incremental_games',\n",
       " 'destinyitemmanager',\n",
       " 'castleclash',\n",
       " 'the_crew',\n",
       " 'flyinglotus',\n",
       " 'mariachi',\n",
       " 'herpetology',\n",
       " 'runningmusic',\n",
       " 'rs2vietnam',\n",
       " 'brisbane',\n",
       " 'flaggyflag',\n",
       " 'freeclams',\n",
       " 'adporn',\n",
       " 'robocraft',\n",
       " 'artbuddy',\n",
       " 'u_sparklyredlightsaber',\n",
       " 'trollmedia',\n",
       " 'humanfanclub',\n",
       " 'liluzivert',\n",
       " 'neutralnews',\n",
       " 'database',\n",
       " 'thewho',\n",
       " 'oracle',\n",
       " 'selfeducation',\n",
       " 'u_pikachuuuuu',\n",
       " 'onionlovers',\n",
       " 'bedbugs',\n",
       " 'cherokeexj',\n",
       " 'finalfantasytactics',\n",
       " 'veteranpacks',\n",
       " 'homedefense',\n",
       " 'modelpaypal',\n",
       " 'whenitgoesin',\n",
       " 'shameless',\n",
       " 'traderjoes',\n",
       " 'paforsanders',\n",
       " 'guildofdungeoneering',\n",
       " 'kinfoundation',\n",
       " 'torment',\n",
       " 'cogsci',\n",
       " 'drumpf',\n",
       " 'christinagrimmie',\n",
       " 'magikarpjump',\n",
       " 'morganamains',\n",
       " 'divorce',\n",
       " 'cannabisextracts',\n",
       " 'grammarnazi',\n",
       " 'frozenfriends',\n",
       " 'modelworldunsc',\n",
       " 'greendawn',\n",
       " 'modelontario',\n",
       " 'irishrugby',\n",
       " 'hugeboobs',\n",
       " 'lifeprotip',\n",
       " 'chihuahua',\n",
       " 'kingfallsam',\n",
       " 'paintball',\n",
       " 'awesomenauts',\n",
       " 'nsfw_gifs',\n",
       " 'jython',\n",
       " 'aspiememes',\n",
       " 'u_nathanisntreal',\n",
       " 'placede',\n",
       " 'stronglifts5x5',\n",
       " 'starcadian',\n",
       " 'explosion_gfys',\n",
       " 'paintballbst',\n",
       " 'crossdressing',\n",
       " 'stocksandboobs',\n",
       " 'mbbcnews',\n",
       " 'rifftrax',\n",
       " 'thehavenmh',\n",
       " 'predaddit',\n",
       " 'esfp',\n",
       " 'shitredditsays',\n",
       " 'britishsuccess',\n",
       " '911truth',\n",
       " 'timelapse',\n",
       " 'calligraffiti',\n",
       " 'anhedonia_recovery',\n",
       " 'seaofthieves',\n",
       " 'xboxonegamers',\n",
       " 'silvercasting',\n",
       " 'rockosmodernlife',\n",
       " 'modelduma',\n",
       " 'modeleuronews',\n",
       " 'bbcradiodrama',\n",
       " 'stardew_valley',\n",
       " 'unrealseries',\n",
       " '3dsmax',\n",
       " 'lightsalot',\n",
       " 'benfolds',\n",
       " 'leaves',\n",
       " 'fountainpens',\n",
       " 'invisibilia',\n",
       " 'biohackers',\n",
       " 'rachelbilson',\n",
       " 'aspergers_dating',\n",
       " 'wattpad',\n",
       " '4runner',\n",
       " 'fatpeoplehate',\n",
       " 'thebiblereloaded',\n",
       " 'newpmdt',\n",
       " 'fuakneg',\n",
       " 'whitewolfrpg',\n",
       " 'tampa',\n",
       " 'digimon',\n",
       " 'brasil',\n",
       " 'athiesm',\n",
       " 'theocs',\n",
       " 'streetartporn',\n",
       " 'northeastfootball',\n",
       " 'metallica',\n",
       " 'ukdocumentaries',\n",
       " 'habitrpg',\n",
       " 'anllelasagra',\n",
       " 'calgaryflames',\n",
       " 'hungarian',\n",
       " 'tryptonaut',\n",
       " 'puer',\n",
       " 'tractors',\n",
       " 'pso',\n",
       " 'nintendoswitchdeals',\n",
       " 'gmu',\n",
       " 'animalssmiling',\n",
       " 'gorillaz',\n",
       " 'polska',\n",
       " 'modelukrainianssr',\n",
       " 'videogamedunkey',\n",
       " 'shacomains',\n",
       " 'legaladvice',\n",
       " 'logic',\n",
       " 'ukipparty',\n",
       " 'coloradosprings',\n",
       " 'alignmentcharts',\n",
       " 'marioparty',\n",
       " 'vintagecomputing',\n",
       " 'europe',\n",
       " 'modelausadiscuss',\n",
       " 'mhocdisabilities',\n",
       " 'periwinkle',\n",
       " 'com98',\n",
       " 'azcardinals',\n",
       " 'shoebots',\n",
       " 'dramaticoverwatch',\n",
       " 'reits',\n",
       " 'thirdsentencehappy',\n",
       " 'transgenderuk',\n",
       " 'perfect_response',\n",
       " 'reasonablefantasy',\n",
       " 'treknobabble',\n",
       " 'ukrainianconflict',\n",
       " 'malegrooming',\n",
       " 'purplemusic',\n",
       " 'listige',\n",
       " 'chvrches',\n",
       " 'ldesurvival',\n",
       " 'rush',\n",
       " 'leftyincel',\n",
       " 'teamsesh',\n",
       " 'drumbuilding',\n",
       " 'modeluk',\n",
       " 'shutupandtakemymoney',\n",
       " 'messandnoise',\n",
       " 'ilustrado',\n",
       " 'madewithableton',\n",
       " 'singles',\n",
       " 'oscars',\n",
       " 'evedreddit',\n",
       " 'archlinux',\n",
       " 'psychiatry',\n",
       " 'unsentletters',\n",
       " 'lava',\n",
       " 'fiftyfifty',\n",
       " 'modelloveletters',\n",
       " 'ankylosingspondylitis',\n",
       " 'awkward',\n",
       " 'uslpro',\n",
       " 'awfuleverything',\n",
       " 'metalgearsolid',\n",
       " 'preacher',\n",
       " 'tmsfe',\n",
       " 'planetdolan',\n",
       " 'theexpanse',\n",
       " 'taylorswift',\n",
       " 'anarcho_capitalism',\n",
       " 'betweentheburiedandme',\n",
       " 'babyjail',\n",
       " 'dogshaming',\n",
       " 'rpdrcirclejerk',\n",
       " 'leicester',\n",
       " 'futuramasleepers',\n",
       " 'gcxrep',\n",
       " 'infpmusic',\n",
       " 'dogswithjobs',\n",
       " 'workonline',\n",
       " 'pokemonconspiracies',\n",
       " 'kustom',\n",
       " 'homekit',\n",
       " 'endgamespoilers',\n",
       " 'seniorkitties',\n",
       " 'paleolithicketogenic',\n",
       " 'fastingscience',\n",
       " 'churchofnaoto',\n",
       " 'ruinedorgasms',\n",
       " '1200isfineiguessugh',\n",
       " 'projecteternity',\n",
       " 'counterthreat',\n",
       " 'youtubecenter',\n",
       " 'gloucestershire',\n",
       " 'coolguides',\n",
       " 'intpcreations',\n",
       " 'spd',\n",
       " 'colony',\n",
       " 'futurism',\n",
       " 'sundressesgonewild',\n",
       " 'thestaircase',\n",
       " 'creatures_of_earth',\n",
       " 'loleventvods',\n",
       " 'dae',\n",
       " 'industrialengineering',\n",
       " 'samesexparents',\n",
       " 'formative',\n",
       " 'burningman',\n",
       " 'crackwatch',\n",
       " 'greenlattice',\n",
       " 'instantregret',\n",
       " 'programmerhumor',\n",
       " 'woahtunes',\n",
       " 'threekings',\n",
       " 'enneagram',\n",
       " 'mhoirseanadeireann',\n",
       " 'labouruk',\n",
       " 'isfpmusic',\n",
       " 'lildicky',\n",
       " 'lyon',\n",
       " 'urbanfarming',\n",
       " 'londinium',\n",
       " 'sociopaths',\n",
       " 'grmd',\n",
       " 'leavingneverland',\n",
       " 'mini14',\n",
       " 'reol',\n",
       " 'psychologytalk',\n",
       " 'wpdtalk',\n",
       " 'hoggit',\n",
       " 'infrastructurist',\n",
       " 'thenvidiashield',\n",
       " 'hotsamples',\n",
       " 'sportnl',\n",
       " 'handstyles',\n",
       " 'evernote',\n",
       " 'letsnotmeet',\n",
       " 'holdmyfries',\n",
       " 'theknick',\n",
       " 'verifyas',\n",
       " 'newyorkforsanders',\n",
       " 'mlclass',\n",
       " 'witchhouse',\n",
       " 'aceofangels8',\n",
       " 'freekarma4you',\n",
       " 'donthelpjustfilm',\n",
       " 'wordpress',\n",
       " 'proasheck',\n",
       " 'awwducational',\n",
       " 'quadrigacx2',\n",
       " 'ontariouniversities',\n",
       " 'printing',\n",
       " 'draven',\n",
       " 'oddlyspecific',\n",
       " 'life',\n",
       " 'madeofstyrofoam',\n",
       " 'hometheater',\n",
       " 'swgalaxyofheroes',\n",
       " 'leagueofmeta',\n",
       " 'excopticorthodox',\n",
       " 'amry',\n",
       " 'albiononline',\n",
       " 'magicdeckbuilding',\n",
       " 'alandislands',\n",
       " 'pdxchange',\n",
       " 'riotgrrrl',\n",
       " 'stonerprotips',\n",
       " 'overwatchrms',\n",
       " 'thedao',\n",
       " 'nomanshigh',\n",
       " 'paradoxplaats',\n",
       " 'insaneideas',\n",
       " 'cornsnake',\n",
       " 'doubleperspective',\n",
       " 'anarchoslacktivism',\n",
       " 'intel',\n",
       " 'teengamingnights',\n",
       " 'monkslookingatbeer',\n",
       " 'debunkthis',\n",
       " 'viktardforamerica',\n",
       " 'modernbaseball',\n",
       " 'beerandpizza',\n",
       " 'prey',\n",
       " 'relayforreddit',\n",
       " 'thelongestjourney',\n",
       " 'technologyporn',\n",
       " 'ibetyoulistento',\n",
       " 'medicalgore',\n",
       " 'rlpverify',\n",
       " 'yms',\n",
       " 'premed',\n",
       " 'shave_bazaar',\n",
       " 'alicegrove',\n",
       " 'wholesomenosleep',\n",
       " 'deformed',\n",
       " 'retrogaming',\n",
       " 'osheaga',\n",
       " 'sodogetip',\n",
       " 'nocontext_discussion',\n",
       " 'wigglebutts',\n",
       " 'bandnames',\n",
       " 'zootopia',\n",
       " 'requestnetwork',\n",
       " 'canadients',\n",
       " 'nickcave',\n",
       " 'dnmuk',\n",
       " 'physics',\n",
       " 'festivaltrees',\n",
       " 'aftereffects',\n",
       " 'animeitaly',\n",
       " 'novalauncher',\n",
       " 'italy',\n",
       " 'virginiatech',\n",
       " 'askle',\n",
       " '4kgamers',\n",
       " 'holdmyfeedingtube',\n",
       " 'offensivememes',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predf.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing for LDA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3d85dd7bc7428182f91ce75586d1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA with fifty topics: \n",
      "Start LDA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6d58a9a3154b5ca8a8bd969eb79f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA with onehundred topics: \n",
      "Start LDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-ef865b6b2c28>:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[name] = lst\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3139769a0344ad2bf5931bf9f8c8a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-32578:\n",
      "Process ForkPoolWorker-32584:\n",
      "Process ForkPoolWorker-32582:\n",
      "Process ForkPoolWorker-32586:\n",
      "Process ForkPoolWorker-32576:\n",
      "Process ForkPoolWorker-32577:\n",
      "Process ForkPoolWorker-32589:\n",
      "Process ForkPoolWorker-32573:\n",
      "Process ForkPoolWorker-32574:\n",
      "Process ForkPoolWorker-32588:\n",
      "Process ForkPoolWorker-32585:\n",
      "Process ForkPoolWorker-32579:\n",
      "Process ForkPoolWorker-32587:\n",
      "Process ForkPoolWorker-32575:\n",
      "Process ForkPoolWorker-32572:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "# User features\n",
    "\n",
    "# Preprocessing for LDA\n",
    "def preprocess_lda(df):\n",
    "    neglst = [\"no\", \"not\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nowhere\", \"never\", \"nay\"]\n",
    "    inputlst = []\n",
    "    for row in tqdm(df['tokens']):\n",
    "        rowlst = []\n",
    "        for comment in row:\n",
    "            rowlst.append([word for word in comment if (word not in neglst)])\n",
    "        inputlst.append(rowlst)\n",
    "    return inputlst\n",
    "# LDA for topics\n",
    "def apply_lda(df, inputlst, number, name):\n",
    "    print(\"Start LDA...\")\n",
    "    lst = []\n",
    "    for row in tqdm(inputlst):\n",
    "        if len(row) < 2:\n",
    "            lst.append(-1)\n",
    "        else:\n",
    "            dictionary = corpora.Dictionary(row)\n",
    "            corpus = [dictionary.doc2bow(text) for text in row]\n",
    "            ldamodel = gensim.models.LdaMulticore(corpus, num_topics=number, id2word = dictionary, passes=20, workers=15)\n",
    "            result = ldamodel.print_topics(num_topics=1, num_words=1)\n",
    "            res = list(result)\n",
    "            topic = [item[0] for item in res]\n",
    "            lst.append(topic[0])\n",
    "    df[name] = lst\n",
    "    return df\n",
    "\n",
    "# Wrapper\n",
    "def extract_userfeatures(df):\n",
    "    print(\"Preprocessing for LDA...\")\n",
    "    inputlst = preprocess_lda(df)\n",
    "    print(\"LDA with fifty topics: \")\n",
    "    df = apply_lda(df, inputlst, 50, \"ldafifty\")\n",
    "    print(\"LDA with onehundred topics: \")\n",
    "    df = apply_lda(df, inputlst, 100, \"ldahundred\")\n",
    "    return df\n",
    "\n",
    "# create df with user features\n",
    "# user_feat_df = extract_userfeatures(predf)\n",
    "\n",
    "# user_feat_df.to_pickle(\"user_feat_df_allcomments.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linguistic features (functions)\n",
    "\n",
    "# other features that are not mentioned in the paper\n",
    "def create_features(df):\n",
    "    # Total number of characters (including space)\n",
    "    df['char_count'] = df['complete_body'].str.len()\n",
    "    # Total number of stopwords\n",
    "    stopwordList = stopwords.words('english')\n",
    "    df['stopwords'] = df['complete_body'].apply(lambda x: len([x for x in x.split() if x in stopwordList]))\n",
    "    # Total number of punctuation or special characters\n",
    "    df['total_punc'] = df['complete_body'].apply(lambda x: len([x for x in x.split() for j in x if j in string.punctuation]))\n",
    "    # Total number of numerics\n",
    "    df['total_num'] = df['complete_body'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "    # Total number of uppercase words\n",
    "    df['total_uppercase'] = df['complete_body'].apply(lambda x: len([x for x in x.split() if x.isupper()]))    \n",
    "    return df\n",
    "\n",
    "# type token ratio\n",
    "def typetokenratio(df):\n",
    "    ratiolst = []\n",
    "    for comment in df['complete_body']:\n",
    "            lex = LexicalRichness(comment)\n",
    "            if lex.words == 0:\n",
    "                ratiolst.append(0)\n",
    "            else:\n",
    "                ratio = lex.ttr\n",
    "                ratiolst.append(ratio)\n",
    "    df['ttr'] = ratiolst\n",
    "    return df\n",
    "\n",
    "# words per sentence\n",
    "def wordcounter(df):\n",
    "    lengthscore = []\n",
    "    for row in df['senttokens']:\n",
    "        rowscore = []\n",
    "        for comment in row:\n",
    "            sentencescore = 0\n",
    "            for senttoken in comment:\n",
    "                length = len(senttoken.split())\n",
    "                sentencescore += length\n",
    "            sentencescore = sentencescore/len(comment)\n",
    "        lengthscore.append(sentencescore)\n",
    "        arr = np.array(lengthscore)\n",
    "    df['words_per_sent'] = lengthscore\n",
    "    return df\n",
    "\n",
    "# words longer than six characters\n",
    "def charcounter(df):\n",
    "    charscore = []\n",
    "    for row in df['tokens']:\n",
    "        for comment in row:\n",
    "            rowcharscore = 0\n",
    "            lencomment = len(comment)\n",
    "            if lencomment == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                number = 0\n",
    "                for token in comment:\n",
    "                    length = len(token)\n",
    "                    if length > 5:\n",
    "                        number+=1\n",
    "                score = number/lencomment\n",
    "            rowcharscore += score\n",
    "        rowcharscore = rowcharscore/len(row)\n",
    "        charscore.append(rowcharscore)\n",
    "    df['wordslongersix'] = charscore\n",
    "    return df\n",
    "\n",
    "# POS tagger\n",
    "def tagging(df):\n",
    "    past = [] #VPA\n",
    "    presence = [] #VPR\n",
    "    adverbs = [] #RB\n",
    "    prepositions = [] #PREP\n",
    "    pronouns = [] #PR\n",
    "    for comment in df['complete_body']:\n",
    "            text = comment.split()\n",
    "            tags = nltk.pos_tag(text)\n",
    "            counts = Counter(tag for word,tag in tags)\n",
    "            total = sum(counts.values())\n",
    "            pron = counts['PRP'] + counts['PRP$']\n",
    "            verbspr = counts['VB'] + counts['VBG'] + counts['VBP'] + counts['VBZ'] + counts['MD']\n",
    "            verbspa = counts['VBD'] + counts['VBN']\n",
    "            preps = counts['IN'] + counts['TO']\n",
    "            counts['PR'] = pron\n",
    "            counts['PREP'] = preps\n",
    "            counts['VPR'] = verbspr #present tense\n",
    "            counts['VPA'] = verbspa #past tense\n",
    "            if total == 0:\n",
    "                allcounts = dict((word, float(count)/1) for word,count in counts.items())\n",
    "            else:\n",
    "                allcounts = dict((word, float(count)/total) for word,count in counts.items())\n",
    "            try:\n",
    "                past.append(allcounts['VPA'])\n",
    "            except KeyError:\n",
    "                past.append(0)\n",
    "            try:\n",
    "                presence.append(allcounts['VPR'])\n",
    "            except KeyError:\n",
    "                presence.append(0)\n",
    "            try:\n",
    "                adverbs.append(allcounts['RB'])\n",
    "            except KeyError:\n",
    "                adverbs.append(0)\n",
    "            try:\n",
    "                prepositions.append(allcounts['PREP'])\n",
    "            except KeyError:\n",
    "                prepositions.append(0)\n",
    "            try:\n",
    "                pronouns.append(allcounts['PR'])\n",
    "            except KeyError:\n",
    "                pronouns.append(0)\n",
    "    df['pasttense'] = past\n",
    "    df['presencetense'] = presence\n",
    "    df['adverbs'] = adverbs\n",
    "    df['prepositions'] = prepositions\n",
    "    df['pronouns'] = pronouns\n",
    "    return df\n",
    "\n",
    "def ngrams(df, n_min, n_max, ngramtype):\n",
    "    # convert input from list to string\n",
    "    ngrams = []\n",
    "    inputtext = []\n",
    "    for row in df['tokens']:\n",
    "        for comment in row:\n",
    "            text = ' '.join(comment)\n",
    "        inputtext.append(text)\n",
    "    print(\"Length of inputtext: \", len(inputtext))\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n_min,n_max), analyzer=ngramtype)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    print(\"Get feature names...\")\n",
    "    names = vectorizer.get_feature_names()\n",
    "    print(\"Length of feature names: \", len(names))\n",
    "    print(\"Create df...\")\n",
    "    ngramdf = pd.DataFrame(denselist, columns=names)\n",
    "    return ngramdf\n",
    "\n",
    "def merge_dfs(df1, df2, df3):\n",
    "#     cwngramsdf = pd.merge(df1, df2, on='author', how='inner', suffixes= (None, \"_charngram\"))\n",
    "#     gramsdf = pd.merge(df3, cwngramsdf, on='author', how='inner', suffixes= (None, \"_ngram\"))\n",
    "    ngramsdf = df1.join(df2)\n",
    "    newdf = df3.join(ngramsdf)\n",
    "    return gramsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for linguistic features\n",
    "\n",
    "def extract_lin_features(df, create_ngrams):\n",
    "    print(\"Create additional features...\")\n",
    "    df = create_features(df)\n",
    "    print(\"Create ttr...\")\n",
    "    df = typetokenratio(df)\n",
    "    print(\"Count words per sentence...\")\n",
    "    df = wordcounter(df)\n",
    "    print(\"Count words with more than six letters...\")\n",
    "    df = charcounter(df)\n",
    "    print(\"POS-Tagger...\")\n",
    "    df = tagging(df)\n",
    "    print(\"number of rows df\", len(df))\n",
    "    \n",
    "    if create_ngrams == \"none\":\n",
    "        return df\n",
    "    \n",
    "    elif create_ngrams == \"all\":\n",
    "        print(\"Ngrams...\")\n",
    "        print(\"Create word ngrams...\")\n",
    "        wordngramsdf = ngrams(df, 1, 3, \"word\")\n",
    "        print(\"Create char ngrams...\")\n",
    "        charngramsdf = ngrams(df, 2, 3, \"char\")\n",
    "        print(\"Merge df...\")\n",
    "        gramsdf = merge_dfs(wordngramsdf, charngramsdf, df)\n",
    "        return gramsdf\n",
    "    \n",
    "    elif create_ngrams == \"word\":\n",
    "        wordngrams = ngrams(df, 1, 3, 'word')\n",
    "        wordngramsdf = pd.DataFrame(wordngrams)\n",
    "#         gramsdf = pd.merge(df, wordngramsdf, on='author', how='inner', suffixes=(None, \"_ngram\"))\n",
    "        gramsdf = df.join(wordngramsdf)\n",
    "        return gramsdf\n",
    "    \n",
    "# create dataframe with linguistic features\n",
    "\n",
    "# without ngrams\n",
    "# lin_feat_df = extract_lin_features(user_feat_df, \"none\")\n",
    "\n",
    "# with all ngrams\n",
    "# lin_ngrams_df = extract_lin_features(user_feat_df, \"all\")\n",
    "lin_ngrams_df = extract_lin_features(predf, \"all\")\n",
    "# wordngrams only\n",
    "# lin_wordngrams_df = extract_lin_features(user_feat_df, \"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordlists (functions)\n",
    "\n",
    "# Empath\n",
    "# create new categories with empath\n",
    "def new_cat():\n",
    "    empath = Empath()\n",
    "    social = empath.create_category(\"social\",[\"mate\",\"talk\",\"they\"])\n",
    "    humans = empath.create_category(\"humans\",[\"adult\",\"baby\",\"boy\"])\n",
    "    cognitive = empath.create_category(\"cognitive\",[\"cause\",\"know\",\"ought\"])\n",
    "    insight = empath.create_category(\"insight\",[\"think\",\"know\",\"consider\"])\n",
    "    causation = empath.create_category(\"causation\",[\"because\",\"effect\",\"hence\"])\n",
    "    discrepancy = empath.create_category(\"discrepancy\",[\"should\",\"would\",\"could\"])\n",
    "    tentative = empath.create_category(\"tentative\",[\"maybe\",\"perhaps\",\"guess\"])\n",
    "    certainty = empath.create_category(\"certainty\",[\"always\",\"never\", \"proof\"])\n",
    "    inhibition = empath.create_category(\"inhibition\",[\"block\",\"constrain\",\"stop\"])\n",
    "    inclusive = empath.create_category(\"inclusive\",[\"and\",\"with\",\"include\"])\n",
    "    exclusive = empath.create_category(\"exclusive\",[\"but\",\"without\",\"exclude\"])\n",
    "    perceptual = empath.create_category(\"perceptual\",[\"observing\",\"hear\",\"feeling\"])\n",
    "    see = empath.create_category(\"see\",[\"view\",\"saw\",\"seen\"])\n",
    "    feel = empath.create_category(\"feel\",[\"feels\",\"touch\",\"feeling\"])\n",
    "    biological = empath.create_category(\"biological\",[\"eat\",\"blood\",\"pain\"])\n",
    "    relativity = empath.create_category(\"relativity\",[\"area\",\"bend\",\"go\"])\n",
    "    space = empath.create_category(\"space\",[\"down\",\"in\",\"thin\"])\n",
    "    time = empath.create_category(\"time\",[\"end\",\"until\",\"season\"])\n",
    "    agreement = empath.create_category(\"agreement\", [\"agree\", \"ok\", \"yes\"])\n",
    "    fillers = empath.create_category(\"fillers\", [\"like\", \"Imean\", \"yaknow\"])\n",
    "    nonfluencies = empath.create_category(\"nonfluencies\", [\"umm\", \"hm\", \"er\"])\n",
    "    conjunctions = empath.create_category(\"conjunctions\", [\"and\", \"but\", \"whereas\"])\n",
    "    quantifiers = empath.create_category(\"quantifiers\", [\"few\", \"many\", \"much\"])\n",
    "    numbers = empath.create_category(\"numbers\", [\"two\", \"fourteen\", \"thousand\"])\n",
    "\n",
    "def apply_empath(df):\n",
    "    empath = Empath()\n",
    "    print(\"Create new empath categories...\")\n",
    "    new_cat()\n",
    "    print(\"Apply empath...\")\n",
    "    empathvalues = []\n",
    "    empathcategories = [\"swearing_terms\", \"social\", \"family\", \"friends\", \"humans\", \"emotional\", \"positive_emotion\", \"negative_emotion\", \"fear\", \"anger\", \"sadness\", \"cognitive\", \"insight\", \"causation\", \"discrepancy\", \"tentative\", \"certainty\", \"inhibition\", \"inclusive\", \"exclusive\", \"perceptual\", \"see\", \"hear\", \"feel\", \"biological\", \"body\", \"health\", \"sexual\", \"eat\", \"relativity\", \"space\", \"time\", \"work\", \"achievement\", \"leisure\", \"home\", \"money\", \"religion\", \"death\" ,\"agreement\", \"fillers\", \"nonfluencies\"]\n",
    "    for sentence in tqdm(df['complete_body']):\n",
    "        empathvalues.append(empath.analyze(sentence, categories=empathcategories, normalize=True))\n",
    "    empathdf = pd.DataFrame(empathvalues)\n",
    "    empathdf['author'] = df['author']\n",
    "\n",
    "    newdf = pd.merge(df, empathdf, on='author', how='inner', suffixes=(None, \"_wordlist\"))\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data for other wordlists\n",
    "concretenessdf = pd.read_csv('/home/sophia/ma_py/psych_lists/concreteness.csv')\n",
    "cdf = concretenessdf[['Conc.M']]\n",
    "cmatrix = cdf.to_numpy()\n",
    "concrete = concretenessdf['Word'].values.tolist()\n",
    "\n",
    "happinessdf = pd.read_csv('/home/sophia/ma_py/psych_lists/happiness_ratings.csv')\n",
    "hdf = happinessdf[['happiness_average']]\n",
    "hmatrix = hdf.to_numpy()\n",
    "happiness = happinessdf['word'].values.tolist()\n",
    "\n",
    "cursedf = pd.read_csv('/home/sophia/ma_py/psych_lists/mean_good_curse.csv')\n",
    "cudf = cursedf[['mean_good_curse']]\n",
    "cumatrix = cudf.to_numpy()\n",
    "curse = cursedf['word'].values.tolist()\n",
    "\n",
    "sensorydf = pd.read_csv('/home/sophia/ma_py/psych_lists/sensory_experience_ratings.csv')\n",
    "serdf = sensorydf[['Average SER']]\n",
    "sermatrix = serdf.to_numpy()\n",
    "ser = sensorydf['Word'].values.tolist()\n",
    "\n",
    "alldf = pd.read_csv('/home/sophia/ma_py/psych_lists/sensory_ratings_all.csv')\n",
    "newalldf = alldf[['Emotion', 'Polarity', 'Social', 'Moral', 'MotionSelf', 'Thought', 'Color', 'TasteSmell', 'Tactile', 'VisualForm', 'Auditory', 'Space', 'Quantity', 'Time', 'CNC', 'IMG', 'FAM']]\n",
    "allmatrix = newalldf.to_numpy()\n",
    "allsens = alldf['Word'].values.tolist()\n",
    "\n",
    "valarodomdf = pd.read_csv('/home/sophia/ma_py/psych_lists/valence_arousal_dominence.csv')\n",
    "vaddf = valarodomdf[['V.Mean.Sum', 'A.Mean.Sum', 'D.Mean.Sum']]\n",
    "vadmatrix = vaddf.to_numpy()\n",
    "vad = valarodomdf['Word'].values.tolist()\n",
    "\n",
    "mrcdf = pd.read_csv('/home/sophia/ma_py/psych_lists/mrclists_c_p.csv', sep='\\t', names=['word', 'cmean', 'pmean'])\n",
    "cpdf = mrcdf[['cmean', 'pmean']]\n",
    "cpmatrix = cpdf.to_numpy()\n",
    "mrc = mrcdf['word'].values.tolist()\n",
    "\n",
    "# function for other wordlists\n",
    "\n",
    "def counter(df, vocab):\n",
    "    inputtext = []\n",
    "    for row in df['complete_body']:\n",
    "        text = ' '.join(row)\n",
    "        inputtext.append(text)\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,1), vocabulary = vocab)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    v = vectors.toarray()\n",
    "    return v\n",
    "\n",
    "def multiply(matrix, ratings):\n",
    "    # matrix multiplication \n",
    "    result = np.matmul(matrix, ratings)\n",
    "    # divide each score with the number of words in the list to normalize\n",
    "    result = result/(len(ratings))\n",
    "    return result\n",
    "\n",
    "def aggregator(df, vocab, ratings, name):\n",
    "    count = counter(df, vocab)\n",
    "    result = multiply(count, ratings)\n",
    "    num_rows, num_cols = result.shape\n",
    "    \n",
    "    if num_cols ==1:\n",
    "        df[name] = result\n",
    "    else:\n",
    "        resultdf = pd.DataFrame(result)\n",
    "        for i in range(len(name)):\n",
    "            # first i is zero\n",
    "            column = name[i]\n",
    "            df[column] = resultdf[i]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordlists created manually\n",
    "\n",
    "negations = [\"no\", \"not\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nowhere\", \"never\", \"nay\"]\n",
    "articles = [\"a\", \"an\", \"the\"]\n",
    "future = [\"will\", \"gonna\"]\n",
    "\n",
    "def list_counter(df, vocab, name):\n",
    "    inputtext = []\n",
    "    total = []\n",
    "    for row in df['complete_body']:\n",
    "        total.append(len(row))\n",
    "        text = ' '.join(row)\n",
    "        inputtext.append(text)\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,1), vocabulary = vocab)\n",
    "    print(\"Vectorize...\")\n",
    "    vectors = vectorizer.fit_transform(tqdm(inputtext))\n",
    "    v = vectors.toarray()\n",
    "    averagev = v.sum(axis=1)\n",
    "    totalvector =  np.array(total)\n",
    "    score = np.divide(averagev, totalvector)\n",
    "    df[name] = score\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for wordlists\n",
    "\n",
    "def extract_wordlist_features(df):\n",
    "    print(\"Empath...\")\n",
    "    empdf = apply_empath(df)\n",
    "    # create scores for each word list and add them to df\n",
    "    print(\"Count Wordlist Concreteness: \\n\")\n",
    "    psychdf = aggregator(empdf, concrete, cmatrix, \"concreteness\")\n",
    "    print(\"Count Wordlist Happiness: \\n\")\n",
    "    psychdf = aggregator(empdf, happiness, hmatrix, \"happiness\")\n",
    "    print(\"Count Wordlist Good_Curse: \\n\")\n",
    "    psychdf = aggregator(empdf, curse, cumatrix, \"good_curse\")\n",
    "    print(\"Count 17 further wordlists: \\n\")\n",
    "    psychdf = aggregator(empdf, allsens, allmatrix, ['emotion', 'polarity', 'social', 'moral', 'motionself', 'thought', 'color', 'tastesmell', 'tactile', 'visualform', 'auditory', 'space', 'quantity', 'time', 'CNC', 'IMG', 'FAM'])\n",
    "    print(\"Count Wordlist SER: \\n\")\n",
    "    psychdf = aggregator(empdf, ser, sermatrix, \"SER\")\n",
    "    print(\"Count Wordlists Valence, Arousal, Dominance: \\n\")\n",
    "    psychdf = aggregator(empdf, vad, vadmatrix, ['valence', 'arousal', 'dominance'])\n",
    "    print(\"Count Wordlist Negation: \\n\")\n",
    "    psychdf = list_counter(empdf, negations, \"negations\")\n",
    "    print(\"Count Wordlist Articles: \\n\")\n",
    "    psychdf = list_counter(empdf, articles, \"articles\")\n",
    "    print(\"Count Wordlist Future: \\n\")\n",
    "    psychdf = list_counter(empdf, future, \"future\")\n",
    "    print(\"Count Wordlists from MRC (2): \\n\")\n",
    "    psychdf = aggregator(empdf, mrc, cpmatrix, [\"mrc_cmean\", \"mrc_pmean\"])\n",
    "    \n",
    "    return psychdf\n",
    "\n",
    "psychdf = extract_wordlist_features(lin_ngrams_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "# histogram of distribution of traits in dataset\n",
    "def all_hist_true(df):\n",
    "    plt.figure(figsize = (16, 8))\n",
    "#     plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(df['agreeableness'], bins = 20)\n",
    "    plt.title('Agreeableness')\n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.hist(df['openness'], bins = 20)\n",
    "    plt.title('Openness')\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.hist(df['conscientiousness'], bins = 20)\n",
    "    plt.title('Conscientiousness')\n",
    "    \n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.hist(df['extraversion'], bins = 20)\n",
    "    plt.title('Extraversion')\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.hist(df['neuroticism'], bins = 20)\n",
    "    plt.title('Neuroticism')\n",
    "    \n",
    "    plt.suptitle(\"Histograms of the true trait values\")\n",
    "    plt.subplots_adjust(left=0.1, \n",
    "                    bottom=0.1,  \n",
    "                    right=0.9,  \n",
    "                    top=0.9,  \n",
    "                    wspace=0.4,  \n",
    "                    hspace=0.4) \n",
    "    plt.show()\n",
    "\n",
    "#split dataset in features and target variable depending on which trait to focus on\n",
    "def trait(df, trait_name, startnumber):\n",
    "    featurelist = df.columns.tolist()\n",
    "    feature_cols = featurelist[startnumber:]\n",
    "    x = df[feature_cols] \n",
    "    \n",
    "    if trait_name == 'agree':\n",
    "        y = df.agree\n",
    "    elif trait_name == 'openn':\n",
    "        y = df.openn\n",
    "    elif trait_name == 'consc':\n",
    "        y = df.consc\n",
    "    elif trait_name == 'extra':\n",
    "        y = df.extra\n",
    "    elif trait_name == 'neuro':\n",
    "        y = df.neuro       \n",
    "    return x,y \n",
    "\n",
    "# create pipeline\n",
    "def create_pipeline(x_train, y_train ,classifier):\n",
    "    if classifier == \"log\":\n",
    "        pipeline = Pipeline([\n",
    "          ('variance_threshold', VarianceThreshold()),\n",
    "          ('feature_selection',  SelectKBest(f_classif, k=30)),\n",
    "          ('scaler', StandardScaler()),\n",
    "          ('classification',LogisticRegression(n_jobs=-1))\n",
    "        ])\n",
    "        \n",
    "    pipeline.fit(x_train, y_train)\n",
    "    return pipeline\n",
    "\n",
    "def get_names(x, pipeline):\n",
    "    features = pipeline.named_steps['feature_selection']\n",
    "    names = x.columns[features.get_support(indices=True)]\n",
    "    return names\n",
    "\n",
    "def get_pvalues(pipeline, x):\n",
    "    features = pipeline.named_steps['feature_selection']\n",
    "    pvalues = features.pvalues_\n",
    "    dfpvalues = pd.DataFrame(features.pvalues_)\n",
    "    dfscores = pd.DataFrame(features.scores_)\n",
    "    dfcolumns = pd.DataFrame(x.columns)\n",
    "    # concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores, dfpvalues],axis=1)\n",
    "    featureScores.columns = ['Specs','Score', 'P-Value']\n",
    "    # plot\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.hist(pvalues)\n",
    "    plt.show()\n",
    "    return featureScores\n",
    "\n",
    "def scores(y_test, y_pred, presentationtype):\n",
    "    if presentationtype == \"scores\":\n",
    "        accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "        precision=metrics.precision_score(y_test, y_pred)\n",
    "        recall=metrics.recall_score(y_test, y_pred)\n",
    "        f_one=metrics.f1_score(y_test, y_pred)\n",
    "        return accuracy, precision, recall, f_one\n",
    "    if presentationtype == \"report\":\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        return report\n",
    "    \n",
    "def score_plot(logreg, y_test, x_test):\n",
    "    lr_probs = logreg.predict_proba(x_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # predict class values\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "    # plot the precision-recall curves\n",
    "    no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return lr_precision, lr_recall\n",
    "\n",
    "def create_cnfmatrix(y_test, y_pred, plotting=True):\n",
    "    cnfpipe_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n",
    "    sumpositive = tp + fn\n",
    "    sumnegative = fp + tn\n",
    "    sumcorrect = tp + tn\n",
    "    sumwrong = fp + fn\n",
    "    sumall = tn+fp+fn+tp\n",
    "    print(\"TN, FP, FN, TP: \", tn, fp, fn, tp, \"\\nSum: \", sumall, \"\\nSum correct predictions: \", \n",
    "          sumcorrect, \"Percent: \", sumcorrect/sumall, \"\\nSum wrong predictions: \", sumwrong, \"\\tPercent: \",\n",
    "          sumwrong/sumall, \"\\nSum actual positives: \", sumpositive, \"\\tPercent: \", sumpositive/sumall,\n",
    "          \"\\nSum actual negatives: \", sumnegative, \"\\tPercent: \", sumnegative/sumall)\n",
    "    \n",
    "    if plotting:\n",
    "        %matplotlib inline\n",
    "        class_names=[0,1] # name  of classes\n",
    "        fig, ax = plt.subplots()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "        # create heatmap\n",
    "        sns.heatmap(pd.DataFrame(cnfpipe_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "        ax.xaxis.set_label_position(\"bottom\")\n",
    "        plt.tight_layout()\n",
    "        plt.title('Confusion matrix', y=1.1)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for classifier\n",
    "\n",
    "def classify(df, trait_name, startnumber, plotting=True):\n",
    "    print(\"Trait to predict: \", trait_name)\n",
    "    x,y = trait(df, trait_name, startnumber)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "    print(\"Number of authors in y_train: \", len(y_train))\n",
    "    print(\"Number of authors in y_test: \", len(y_test))\n",
    "    logpipe = create_pipeline(x_train, y_train, 'log')\n",
    "    y_pred=logpipe.predict(x_test)\n",
    "    print(\"Number of authors in y_pred: \", len(y_pred))\n",
    "    names = get_names(x, logpipe)\n",
    "    print(\"Names of the top\", len(names), \"features: \\n\", names, \"\\n\")\n",
    "    pvalues = get_pvalues(logpipe, x)\n",
    "    print(\"\\nP-Values: \")\n",
    "    print(pvalues.nsmallest(30,'P-Value'))\n",
    "    print(\"\\n\")\n",
    "    cnfmatrix = create_cnfmatrix(y_test, y_pred, plotting=True) \n",
    "#     accuracy, precision, recall, f_one = scores(y_test, y_pred, \"scores\")\n",
    "#     print(\"Scores:\\nAccuracy:\",accuracy, \"\\nPrecision:\",precision, \"\\nRecall:\",recall, \"\\nF1 score:\",f_one)\n",
    "    report = scores(y_test, y_pred, \"report\")\n",
    "    print(\"Classification report: \\n\", report)\n",
    "    lr_precision, lr_recall = score_plot(logpipe, y_test, x_test)\n",
    "    print(\"\\n \\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psychdf.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 18\n",
    "print (\"Number of authors: \", len(psychdf))\n",
    "\n",
    "# personality prediction on test set\n",
    "all_hist_true(psychdf)\n",
    "classify(psychdf, \"agree\", start, plotting=True)\n",
    "classify(psychdf, \"openn\", start, plotting=True)\n",
    "classify(psychdf, \"consc\", start, plotting=True)\n",
    "classify(psychdf, \"extra\", start, plotting=True)\n",
    "classify(psychdf, \"neuro\", start, plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for the train set\n",
    "\n",
    "def classify_trainset(df, trait_name, startnumber, plotting=True):\n",
    "    print(\"Trait to predict: \", trait_name)\n",
    "    x,y = trait(df, trait_name, startnumber)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n",
    "    logpipe = create_pipeline(x_train, y_train, 'log')\n",
    "    y_pred=logpipe.predict(x_train)\n",
    "    print(\"Number of authors in y_pred: \", len(y_pred))\n",
    "    names = get_names(x, logpipe)\n",
    "    print(\"Names of the top\", len(names), \"features: \\n\", names, \"\\n\")\n",
    "    pvalues = get_pvalues(logpipe, x)\n",
    "#     print(\"p-values of\", len(pvalues), \"features: \\n\", pvalues, \"\\n\")\n",
    "    print(\"\\nP-Values: \")\n",
    "    print(pvalues.nsmallest(30,'P-Value'))\n",
    "    print(\"\\n\")\n",
    "    cnfmatrix = create_cnfmatrix(y_train, y_pred, plotting=True) \n",
    "#     accuracy, precision, recall, f_one = scores(y_test, y_pred, \"scores\")\n",
    "#     print(\"Scores:\\nAccuracy:\",accuracy, \"\\nPrecision:\",precision, \"\\nRecall:\",recall, \"\\nF1 score:\",f_one)\n",
    "    report = scores(y_train, y_pred, \"report\")\n",
    "    print(\"Classification report: \\n\", report)\n",
    "    lr_precision, lr_recall = score_plot(logpipe, y_train, x_train)\n",
    "#     print(\"Scores:\\nLR_Precision:\",lr_precision, \"\\nLR_Recall:\",lr_recall)\n",
    "    plt.show()\n",
    "    print(\"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_trainset(psychdf, \"agree\", start, plotting=True)\n",
    "classify_trainset(psychdf, \"openn\", start, plotting=True)\n",
    "classify_trainset(psychdf, \"consc\", start, plotting=True)\n",
    "classify_trainset(psychdf, \"extra\", start, plotting=True)\n",
    "classify_trainset(psychdf, \"neuro\", start, plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
